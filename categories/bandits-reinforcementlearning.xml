<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Reinforcement Learning Notes (Posts about bandits reinforcementLearning)</title><link>https://necromuralist.github.io/reinforcement_learning/</link><description></description><atom:link href="https://necromuralist.github.io/reinforcement_learning/categories/bandits-reinforcementlearning.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Mon, 09 Jul 2018 00:15:19 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Epsilon Greedy with Normal-Distribution Payouts</title><link>https://necromuralist.github.io/reinforcement_learning/posts/Epsilon-Greedy-with-Normal-Distribution-Payouts/</link><dc:creator>Brunhilde</dc:creator><description>&lt;div&gt;&lt;p&gt;Since the Optimistic Initial Values agent can't use the Bernoulli Arm, I'm creating a version of the Epsilon Greedy Optimized that expects the &lt;tt class="docutils literal"&gt;arms&lt;/tt&gt; to be the population-mean for their payouts and the &lt;tt class="docutils literal"&gt;pull_arm&lt;/tt&gt; will return a set of normally-distributed around that mean.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-1"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;optimized&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;imports&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-2"&gt;&lt;/a&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-3"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;spec&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-5"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-6"&gt;&lt;/a&gt;&lt;span class="nd"&gt;@jitclass&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;spec&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-7"&gt;&lt;/a&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;EpsilonGreedyNormal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-8"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""The Epsilon Greedy Algorithm With Normal Arm&lt;/span&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-9"&gt;&lt;/a&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-10"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-11"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     epsilon (float): fraction of the time to explore&lt;/span&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-12"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     arms (list): collection of probabilities for bandit arm&lt;/span&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-13"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-14"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;optimized&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;constructor&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-15"&gt;&lt;/a&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-16"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;best&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-17"&gt;&lt;/a&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-18"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;select&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-19"&gt;&lt;/a&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-20"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;optimized&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;pull&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-21"&gt;&lt;/a&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-22"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;optimized&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-23"&gt;&lt;/a&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-24"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;optimized&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-25"&gt;&lt;/a&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-26"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;call&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;div class="section" id="optimized-imports"&gt;
&lt;h2&gt;1.1 Optimized Imports&lt;/h2&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_53cec2b26b3746f39a85b5613305ffec-1"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# pypi&lt;/span&gt;
&lt;a name="rest_code_53cec2b26b3746f39a85b5613305ffec-2"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numba&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
&lt;a name="rest_code_53cec2b26b3746f39a85b5613305ffec-3"&gt;&lt;/a&gt;    &lt;span class="n"&gt;jit&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_53cec2b26b3746f39a85b5613305ffec-4"&gt;&lt;/a&gt;    &lt;span class="n"&gt;jitclass&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_53cec2b26b3746f39a85b5613305ffec-5"&gt;&lt;/a&gt;    &lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_53cec2b26b3746f39a85b5613305ffec-6"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numba&lt;/span&gt;
&lt;a name="rest_code_53cec2b26b3746f39a85b5613305ffec-7"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="the-spec"&gt;
&lt;h2&gt;1.2 The Spec&lt;/h2&gt;
&lt;p&gt;This is how you tell numba what attributes the class will have.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_865ae0078fb540e9888ac1ea7ede48e9-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;spec&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
&lt;a name="rest_code_865ae0078fb540e9888ac1ea7ede48e9-2"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"epsilon"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numba&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;a name="rest_code_865ae0078fb540e9888ac1ea7ede48e9-3"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"arms"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numba&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double&lt;/span&gt;&lt;span class="p"&gt;[:]),&lt;/span&gt;
&lt;a name="rest_code_865ae0078fb540e9888ac1ea7ede48e9-4"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"counts"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numba&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double&lt;/span&gt;&lt;span class="p"&gt;[:]),&lt;/span&gt;
&lt;a name="rest_code_865ae0078fb540e9888ac1ea7ede48e9-5"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"rewards"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numba&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double&lt;/span&gt;&lt;span class="p"&gt;[:]),&lt;/span&gt;
&lt;a name="rest_code_865ae0078fb540e9888ac1ea7ede48e9-6"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"total_reward"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numba&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int64&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;a name="rest_code_865ae0078fb540e9888ac1ea7ede48e9-7"&gt;&lt;/a&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="the-constructor"&gt;
&lt;h2&gt;1.3 The Constructor&lt;/h2&gt;
&lt;p&gt;The constructor takes two arguments - &lt;em&gt;epsilon&lt;/em&gt; and &lt;em&gt;arms&lt;/em&gt;. The &lt;em&gt;arms&lt;/em&gt; list should contain the mean payout for each arm.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_b2a3580c86f144c7a666a75c6109bc95-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_b2a3580c86f144c7a666a75c6109bc95-2"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;
&lt;a name="rest_code_b2a3580c86f144c7a666a75c6109bc95-3"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;arms&lt;/span&gt;
&lt;a name="rest_code_b2a3580c86f144c7a666a75c6109bc95-4"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_b2a3580c86f144c7a666a75c6109bc95-5"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_b2a3580c86f144c7a666a75c6109bc95-6"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;a name="rest_code_b2a3580c86f144c7a666a75c6109bc95-7"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="reset"&gt;
&lt;h2&gt;1.4 Reset&lt;/h2&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_0926b1d2c024467eb3277782017d8fc0-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_0926b1d2c024467eb3277782017d8fc0-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""sets the counts, rewards, total_reward to 0s&lt;/span&gt;
&lt;a name="rest_code_0926b1d2c024467eb3277782017d8fc0-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_0926b1d2c024467eb3277782017d8fc0-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    This lets you re-used the EpsilonGreedy&lt;/span&gt;
&lt;a name="rest_code_0926b1d2c024467eb3277782017d8fc0-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_0926b1d2c024467eb3277782017d8fc0-6"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_0926b1d2c024467eb3277782017d8fc0-7"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_0926b1d2c024467eb3277782017d8fc0-8"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;a name="rest_code_0926b1d2c024467eb3277782017d8fc0-9"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="best-arm"&gt;
&lt;h2&gt;1.5 Best Arm&lt;/h2&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;best_arm&lt;/tt&gt; property returns the index of the arm that has the highest average reward so far. It returns the index instead of the arm itself because it's used to get the matching counts and rewards in the &lt;tt class="docutils literal"&gt;update&lt;/tt&gt; method. Since I'm using the &lt;tt class="docutils literal"&gt;jitclass&lt;/tt&gt; decorator I'm going to get rid of &lt;tt class="docutils literal"&gt;first_find&lt;/tt&gt;.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_04febdb970cc4fe5ba593e4d59282922-1"&gt;&lt;/a&gt;&lt;span class="nd"&gt;@property&lt;/span&gt;
&lt;a name="rest_code_04febdb970cc4fe5ba593e4d59282922-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;best_arm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_04febdb970cc4fe5ba593e4d59282922-3"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""Index of the arm with the most reward"""&lt;/span&gt;
&lt;a name="rest_code_04febdb970cc4fe5ba593e4d59282922-4"&gt;&lt;/a&gt;    &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_04febdb970cc4fe5ba593e4d59282922-5"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
&lt;a name="rest_code_04febdb970cc4fe5ba593e4d59282922-6"&gt;&lt;/a&gt;        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
&lt;a name="rest_code_04febdb970cc4fe5ba593e4d59282922-7"&gt;&lt;/a&gt;            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;
&lt;a name="rest_code_04febdb970cc4fe5ba593e4d59282922-8"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="select-arm"&gt;
&lt;h2&gt;1.6 Select Arm&lt;/h2&gt;
&lt;p&gt;This differs from the other Epsilon Greedy code only in that I'm using numpy instead of python for the random function.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_fd1eefc5c3c148d99501b0c43400b4e5-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;select_arm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_fd1eefc5c3c148d99501b0c43400b4e5-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""chooses the next arm to update&lt;/span&gt;
&lt;a name="rest_code_fd1eefc5c3c148d99501b0c43400b4e5-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_fd1eefc5c3c148d99501b0c43400b4e5-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;a name="rest_code_fd1eefc5c3c148d99501b0c43400b4e5-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     int: index of the next arm to pull&lt;/span&gt;
&lt;a name="rest_code_fd1eefc5c3c148d99501b0c43400b4e5-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_fd1eefc5c3c148d99501b0c43400b4e5-7"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_fd1eefc5c3c148d99501b0c43400b4e5-8"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_fd1eefc5c3c148d99501b0c43400b4e5-9"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_arm&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="pull-arm"&gt;
&lt;h2&gt;1.7 Pull Arm&lt;/h2&gt;
&lt;p&gt;Since we can't give user-defined objects as attributes of the class, this version will be both algorithm and bandit. This is what's different from the other Epsilon Greedy algorithms in that we're returning the arm's mean plus a random number from the normal distribution. If numba allowed us to pass in objects maybe we could have just switched out bandits. I need to look into how to make that work.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_3b0039b7e94e4abfad83f6d8bbcc1169-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;pull_arm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_3b0039b7e94e4abfad83f6d8bbcc1169-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""gets the reward&lt;/span&gt;
&lt;a name="rest_code_3b0039b7e94e4abfad83f6d8bbcc1169-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_3b0039b7e94e4abfad83f6d8bbcc1169-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a name="rest_code_3b0039b7e94e4abfad83f6d8bbcc1169-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     arm (int): index for the arm-probability array&lt;/span&gt;
&lt;a name="rest_code_3b0039b7e94e4abfad83f6d8bbcc1169-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;a name="rest_code_3b0039b7e94e4abfad83f6d8bbcc1169-7"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     float: reward&lt;/span&gt;
&lt;a name="rest_code_3b0039b7e94e4abfad83f6d8bbcc1169-8"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_3b0039b7e94e4abfad83f6d8bbcc1169-9"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="update"&gt;
&lt;h2&gt;1.8 Update&lt;/h2&gt;
&lt;p&gt;The update method pulls the arm whose index it is given and then updates the count and reward. Here we're calling the &lt;tt class="docutils literal"&gt;pull_arm&lt;/tt&gt; method instead of using a &lt;tt class="docutils literal"&gt;BernoulliArm&lt;/tt&gt; so we can't re-use the original method.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_19a62851d9f0423ebf43e5e21c8d5be3-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_19a62851d9f0423ebf43e5e21c8d5be3-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""pulls the arm and updates the value&lt;/span&gt;
&lt;a name="rest_code_19a62851d9f0423ebf43e5e21c8d5be3-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_19a62851d9f0423ebf43e5e21c8d5be3-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a name="rest_code_19a62851d9f0423ebf43e5e21c8d5be3-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     arm (int): index of the arm to pull&lt;/span&gt;
&lt;a name="rest_code_19a62851d9f0423ebf43e5e21c8d5be3-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_19a62851d9f0423ebf43e5e21c8d5be3-7"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;a name="rest_code_19a62851d9f0423ebf43e5e21c8d5be3-8"&gt;&lt;/a&gt;    &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_19a62851d9f0423ebf43e5e21c8d5be3-9"&gt;&lt;/a&gt;    &lt;span class="n"&gt;average_reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_19a62851d9f0423ebf43e5e21c8d5be3-10"&gt;&lt;/a&gt;    &lt;span class="n"&gt;reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pull_arm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_19a62851d9f0423ebf43e5e21c8d5be3-11"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;
&lt;a name="rest_code_19a62851d9f0423ebf43e5e21c8d5be3-12"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(((&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;average_reward&lt;/span&gt;
&lt;a name="rest_code_19a62851d9f0423ebf43e5e21c8d5be3-13"&gt;&lt;/a&gt;                        &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;a name="rest_code_19a62851d9f0423ebf43e5e21c8d5be3-14"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</description><category>bandits reinforcementLearning</category><guid>https://necromuralist.github.io/reinforcement_learning/posts/Epsilon-Greedy-with-Normal-Distribution-Payouts/</guid><pubDate>Wed, 02 Aug 2017 01:48:00 GMT</pubDate></item><item><title>Optimistic Initial Values</title><link>https://necromuralist.github.io/reinforcement_learning/posts/Optimistic-Initial-Values/</link><dc:creator>Brunhilde</dc:creator><description>&lt;div&gt;&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;1 Introduction&lt;/h2&gt;
&lt;p&gt;This is one possible to the n-armed bandit problem. It is similar to the &lt;em&gt;Epsilon Greedy&lt;/em&gt; algorithm except that instead of using a conditional to decide whether to explore or exploit, the algorithm sets the estimated (mean) payout for each arm to 1 (the theoretical maximum for our case) and then always exploits. As things proceed, the arms will settle down to their actual payoff-rates and those that haven't been explored will be chosen because they are still too high.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-tangle"&gt;
&lt;h2&gt;2 The Tangle&lt;/h2&gt;
&lt;p&gt;This is the no-web template to build the final file.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_3d43d62f55ec448887d380b3d4ade7eb-1"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;imports&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_3d43d62f55ec448887d380b3d4ade7eb-2"&gt;&lt;/a&gt;
&lt;a name="rest_code_3d43d62f55ec448887d380b3d4ade7eb-3"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;spec&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_3d43d62f55ec448887d380b3d4ade7eb-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_3d43d62f55ec448887d380b3d4ade7eb-5"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;class&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;declaration&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_3d43d62f55ec448887d380b3d4ade7eb-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_3d43d62f55ec448887d380b3d4ade7eb-7"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;constructor&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_3d43d62f55ec448887d380b3d4ade7eb-8"&gt;&lt;/a&gt;
&lt;a name="rest_code_3d43d62f55ec448887d380b3d4ade7eb-9"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;select&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_3d43d62f55ec448887d380b3d4ade7eb-10"&gt;&lt;/a&gt;
&lt;a name="rest_code_3d43d62f55ec448887d380b3d4ade7eb-11"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;pull&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_3d43d62f55ec448887d380b3d4ade7eb-12"&gt;&lt;/a&gt;
&lt;a name="rest_code_3d43d62f55ec448887d380b3d4ade7eb-13"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_3d43d62f55ec448887d380b3d4ade7eb-14"&gt;&lt;/a&gt;
&lt;a name="rest_code_3d43d62f55ec448887d380b3d4ade7eb-15"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="imports"&gt;
&lt;h2&gt;3 Imports&lt;/h2&gt;
&lt;p&gt;These are our external dependencies.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_094fe67a3f324db389a039d8ca5315c0-1"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numba&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;jitclass&lt;/span&gt;
&lt;a name="rest_code_094fe67a3f324db389a039d8ca5315c0-2"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numba&lt;/span&gt;
&lt;a name="rest_code_094fe67a3f324db389a039d8ca5315c0-3"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="the-spec"&gt;
&lt;h2&gt;4 The Spec&lt;/h2&gt;
&lt;p&gt;In order to use numba with the &lt;tt class="docutils literal"&gt;OptimisticInitialValues&lt;/tt&gt; class you have to create a 'spec' that tells numba what the data-types are for each of its fields.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_72676d90280e4990abdece090b95f2f3-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;SPEC&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
&lt;a name="rest_code_72676d90280e4990abdece090b95f2f3-2"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"arms"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numba&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double&lt;/span&gt;&lt;span class="p"&gt;[:]),&lt;/span&gt;
&lt;a name="rest_code_72676d90280e4990abdece090b95f2f3-3"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"counts"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numba&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double&lt;/span&gt;&lt;span class="p"&gt;[:]),&lt;/span&gt;
&lt;a name="rest_code_72676d90280e4990abdece090b95f2f3-4"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"rewards"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numba&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double&lt;/span&gt;&lt;span class="p"&gt;[:]),&lt;/span&gt;
&lt;a name="rest_code_72676d90280e4990abdece090b95f2f3-5"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"total_reward"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numba&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int64&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;a name="rest_code_72676d90280e4990abdece090b95f2f3-6"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"initial_reward"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numba&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;a name="rest_code_72676d90280e4990abdece090b95f2f3-7"&gt;&lt;/a&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="the-class-declaration"&gt;
&lt;h2&gt;5 The Class Declaration&lt;/h2&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_bb240b82d9af4491aaf2bb5eaa737bd6-1"&gt;&lt;/a&gt;&lt;span class="nd"&gt;@jitclass&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;SPEC&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_bb240b82d9af4491aaf2bb5eaa737bd6-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;OptimisticInitialValues&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_bb240b82d9af4491aaf2bb5eaa737bd6-3"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""Optimistic Initial Values greedy algorithm&lt;/span&gt;
&lt;a name="rest_code_bb240b82d9af4491aaf2bb5eaa737bd6-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_bb240b82d9af4491aaf2bb5eaa737bd6-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a name="rest_code_bb240b82d9af4491aaf2bb5eaa737bd6-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     numpy.array[float]: payout-probabilities for each arm&lt;/span&gt;
&lt;a name="rest_code_bb240b82d9af4491aaf2bb5eaa737bd6-7"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="the-constructor"&gt;
&lt;h2&gt;6 The Constructor&lt;/h2&gt;
&lt;p&gt;Here's our first change from the epsilon-greedy algorithm. We no longer have an &lt;tt class="docutils literal"&gt;epsilon&lt;/tt&gt; value and instead of initializing the &lt;tt class="docutils literal"&gt;rewards&lt;/tt&gt; as zeros we initialize them with an 'initial' reward. Also, although you can't see it here, the arms have to be a list of mean payout values (see the &lt;tt class="docutils literal"&gt;pull_arm&lt;/tt&gt; method below).&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_da54e378a7884e4d87bc46dbbc0559de-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;initial_reward&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_da54e378a7884e4d87bc46dbbc0559de-2"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;arms&lt;/span&gt;
&lt;a name="rest_code_da54e378a7884e4d87bc46dbbc0559de-3"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_da54e378a7884e4d87bc46dbbc0559de-4"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;initial_reward&lt;/span&gt;
&lt;a name="rest_code_da54e378a7884e4d87bc46dbbc0559de-5"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;a name="rest_code_da54e378a7884e4d87bc46dbbc0559de-6"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initial_reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;initial_reward&lt;/span&gt;
&lt;a name="rest_code_da54e378a7884e4d87bc46dbbc0559de-7"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="select-arm"&gt;
&lt;h2&gt;7 Select Arm&lt;/h2&gt;
&lt;p&gt;This chooses the next arm. Unlike the epsilon-greedy algorithm it will always pick the 'best' arm, choosing the first if there is a tie. Since the whole class is in the jit I'm also not using the external &lt;tt class="docutils literal"&gt;find_first&lt;/tt&gt; method.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_d34c28e22dfb4b86b528e64f5ba33e24-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;select_arm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_d34c28e22dfb4b86b528e64f5ba33e24-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""Index of the arm with the most reward&lt;/span&gt;
&lt;a name="rest_code_d34c28e22dfb4b86b528e64f5ba33e24-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_d34c28e22dfb4b86b528e64f5ba33e24-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;a name="rest_code_d34c28e22dfb4b86b528e64f5ba33e24-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     integer: index of arm with highest average reward&lt;/span&gt;
&lt;a name="rest_code_d34c28e22dfb4b86b528e64f5ba33e24-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_d34c28e22dfb4b86b528e64f5ba33e24-7"&gt;&lt;/a&gt;    &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_d34c28e22dfb4b86b528e64f5ba33e24-8"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
&lt;a name="rest_code_d34c28e22dfb4b86b528e64f5ba33e24-9"&gt;&lt;/a&gt;        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
&lt;a name="rest_code_d34c28e22dfb4b86b528e64f5ba33e24-10"&gt;&lt;/a&gt;            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="pull-arm"&gt;
&lt;h2&gt;8 Pull Arm&lt;/h2&gt;
&lt;p&gt;This gets the reward for the arm. with a Bernoulli arm, there's a chance that an arm will be set to 0 on its first pull, at which point you will never explore it (since there's no exploration), so even the best arm might get wiped out. To fix this you need a different scheme. This one uses a population mean (selected &lt;tt class="docutils literal"&gt;from self.arms&lt;/tt&gt;) which has noise added by selecting from the standard normal distribution.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_c9539a9322b14bd2ad669f1a6e083052-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;pull_arm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_c9539a9322b14bd2ad669f1a6e083052-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""gets the reward&lt;/span&gt;
&lt;a name="rest_code_c9539a9322b14bd2ad669f1a6e083052-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_c9539a9322b14bd2ad669f1a6e083052-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a name="rest_code_c9539a9322b14bd2ad669f1a6e083052-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     arm (int): index for the arm population-mean array&lt;/span&gt;
&lt;a name="rest_code_c9539a9322b14bd2ad669f1a6e083052-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;a name="rest_code_c9539a9322b14bd2ad669f1a6e083052-7"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     float: payout for the arm&lt;/span&gt;
&lt;a name="rest_code_c9539a9322b14bd2ad669f1a6e083052-8"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_c9539a9322b14bd2ad669f1a6e083052-9"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="update-arm"&gt;
&lt;h2&gt;9 Update Arm&lt;/h2&gt;
&lt;p&gt;This pulls the arm and updates the reward. This works the same as the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;epsilon-greedy&lt;/span&gt;&lt;/tt&gt; version does.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_89188c86760f4da1bf6f0f2ca889f8c9-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_89188c86760f4da1bf6f0f2ca889f8c9-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""pulls the arm and updates the average reward&lt;/span&gt;
&lt;a name="rest_code_89188c86760f4da1bf6f0f2ca889f8c9-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_89188c86760f4da1bf6f0f2ca889f8c9-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    also updates the total_reward the algorithm has earned so far&lt;/span&gt;
&lt;a name="rest_code_89188c86760f4da1bf6f0f2ca889f8c9-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_89188c86760f4da1bf6f0f2ca889f8c9-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a name="rest_code_89188c86760f4da1bf6f0f2ca889f8c9-7"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     arm (int): index of the arm to pull&lt;/span&gt;
&lt;a name="rest_code_89188c86760f4da1bf6f0f2ca889f8c9-8"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_89188c86760f4da1bf6f0f2ca889f8c9-9"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;a name="rest_code_89188c86760f4da1bf6f0f2ca889f8c9-10"&gt;&lt;/a&gt;    &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_89188c86760f4da1bf6f0f2ca889f8c9-11"&gt;&lt;/a&gt;    &lt;span class="n"&gt;average_reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_89188c86760f4da1bf6f0f2ca889f8c9-12"&gt;&lt;/a&gt;    &lt;span class="n"&gt;reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pull_arm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_89188c86760f4da1bf6f0f2ca889f8c9-13"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;
&lt;a name="rest_code_89188c86760f4da1bf6f0f2ca889f8c9-14"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(((&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;average_reward&lt;/span&gt;
&lt;a name="rest_code_89188c86760f4da1bf6f0f2ca889f8c9-15"&gt;&lt;/a&gt;                         &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;a name="rest_code_89188c86760f4da1bf6f0f2ca889f8c9-16"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="reset"&gt;
&lt;h2&gt;10 Reset&lt;/h2&gt;
&lt;p&gt;This resets the values so that you can re-use the algorithm. As with the constructor, it sets the &lt;tt class="docutils literal"&gt;rewards&lt;/tt&gt; to all ones instead of zeros as was the case with the epsilon-greedy algorithm.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_ce5660285bb149e4be8059b15828b78a-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_ce5660285bb149e4be8059b15828b78a-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""sets the counts, rewards, total_reward to 0s&lt;/span&gt;
&lt;a name="rest_code_ce5660285bb149e4be8059b15828b78a-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_ce5660285bb149e4be8059b15828b78a-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    This lets you re-used the EpsilonGreedy&lt;/span&gt;
&lt;a name="rest_code_ce5660285bb149e4be8059b15828b78a-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_ce5660285bb149e4be8059b15828b78a-6"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_ce5660285bb149e4be8059b15828b78a-7"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initial_reward&lt;/span&gt;
&lt;a name="rest_code_ce5660285bb149e4be8059b15828b78a-8"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;a name="rest_code_ce5660285bb149e4be8059b15828b78a-9"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</description><category>bandits reinforcementLearning</category><guid>https://necromuralist.github.io/reinforcement_learning/posts/Optimistic-Initial-Values/</guid><pubDate>Wed, 02 Aug 2017 01:47:00 GMT</pubDate></item><item><title>Assessing the Performance</title><link>https://necromuralist.github.io/reinforcement_learning/posts/assessing-the-performance/</link><dc:creator>hades</dc:creator><description>&lt;div&gt;&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;1 Introduction&lt;/h2&gt;
&lt;p&gt;As with the Epsilon-Greedy algorithm I'm going to use the Cumulative Reward as the metric. In this case we don't really have a parameter to tune.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="imports"&gt;
&lt;h2&gt;2 Imports&lt;/h2&gt;
&lt;p&gt;The dependencies.&lt;/p&gt;
&lt;pre class="code ipython"&gt;&lt;a name="rest_code_470478eeeb404670a80ca4a9aa8abf24-1"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# python standard library&lt;/span&gt;
&lt;a name="rest_code_470478eeeb404670a80ca4a9aa8abf24-2"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;datetime&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;
&lt;a name="rest_code_470478eeeb404670a80ca4a9aa8abf24-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_470478eeeb404670a80ca4a9aa8abf24-4"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# pypi&lt;/span&gt;
&lt;a name="rest_code_470478eeeb404670a80ca4a9aa8abf24-5"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numba&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;jit&lt;/span&gt;
&lt;a name="rest_code_470478eeeb404670a80ca4a9aa8abf24-6"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt;
&lt;a name="rest_code_470478eeeb404670a80ca4a9aa8abf24-7"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt;
&lt;a name="rest_code_470478eeeb404670a80ca4a9aa8abf24-8"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plot&lt;/span&gt;
&lt;a name="rest_code_470478eeeb404670a80ca4a9aa8abf24-9"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;seaborn&lt;/span&gt;
&lt;a name="rest_code_470478eeeb404670a80ca4a9aa8abf24-10"&gt;&lt;/a&gt;
&lt;a name="rest_code_470478eeeb404670a80ca4a9aa8abf24-11"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# this project&lt;/span&gt;
&lt;a name="rest_code_470478eeeb404670a80ca4a9aa8abf24-12"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;optimistic_initial_values&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;OptimisticInitialValues&lt;/span&gt;
&lt;a name="rest_code_470478eeeb404670a80ca4a9aa8abf24-13"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;epsilon_greedy_normal&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;EpsilonGreedyNormal&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="set-up-the-plotting"&gt;
&lt;h2&gt;3 Set-up the Plotting&lt;/h2&gt;
&lt;p&gt;This will enable the plotting and set the style.&lt;/p&gt;
&lt;pre class="code ipython"&gt;&lt;a name="rest_code_36fb7ba1399144aab2f0f2f574dde833-1"&gt;&lt;/a&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="k"&gt;matplotlib&lt;/span&gt; inline
&lt;a name="rest_code_36fb7ba1399144aab2f0f2f574dde833-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;seaborn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_style&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"whitegrid"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="running-the-assessment"&gt;
&lt;h2&gt;4 Running the Assessment&lt;/h2&gt;
&lt;pre class="code ipython"&gt;&lt;a name="rest_code_5088e976d842497498ee0088acf31011-1"&gt;&lt;/a&gt;&lt;span class="nd"&gt;@jit&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;cumulative_reward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trials&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;400&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-3"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""this generates the cumulative reward as the agent pulls the arms&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     agent: implementation that selects and updates the arms&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-7"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     trials (int): number of times to train the agent&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-8"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     times (int): length of time to train the agent&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-9"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-10"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     numpy.array: average cumulative rewards over time&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-11"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-12"&gt;&lt;/a&gt;    &lt;span class="n"&gt;cumulative_rewards&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-13"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;trial&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trials&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-14"&gt;&lt;/a&gt;        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-15"&gt;&lt;/a&gt;            &lt;span class="n"&gt;arm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select_arm&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-16"&gt;&lt;/a&gt;            &lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-17"&gt;&lt;/a&gt;            &lt;span class="n"&gt;cumulative_rewards&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-18"&gt;&lt;/a&gt;        &lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-19"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;cumulative_rewards&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;trials&lt;/span&gt;
&lt;/pre&gt;&lt;pre class="code ipython"&gt;&lt;a name="rest_code_e91f155c7ffd4c9b90fc5a183ba36291-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;plot_cumulative&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cumulative&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_e91f155c7ffd4c9b90fc5a183ba36291-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""generates and plots cumulative average&lt;/span&gt;
&lt;a name="rest_code_e91f155c7ffd4c9b90fc5a183ba36291-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_e91f155c7ffd4c9b90fc5a183ba36291-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a name="rest_code_e91f155c7ffd4c9b90fc5a183ba36291-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     cumulative (pandas.DataFrame): data to plot&lt;/span&gt;
&lt;a name="rest_code_e91f155c7ffd4c9b90fc5a183ba36291-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_e91f155c7ffd4c9b90fc5a183ba36291-7"&gt;&lt;/a&gt;    &lt;span class="n"&gt;figure&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_e91f155c7ffd4c9b90fc5a183ba36291-8"&gt;&lt;/a&gt;    &lt;span class="n"&gt;axe&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gca&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_e91f155c7ffd4c9b90fc5a183ba36291-9"&gt;&lt;/a&gt;    &lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Cumulative Reward of the Optimistic Initial Values Algorithm ({} trials)"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TRIALS&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_e91f155c7ffd4c9b90fc5a183ba36291-10"&gt;&lt;/a&gt;    &lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Time (number of pulls on the arm)"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_e91f155c7ffd4c9b90fc5a183ba36291-11"&gt;&lt;/a&gt;    &lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Cumulative Reward"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_e91f155c7ffd4c9b90fc5a183ba36291-12"&gt;&lt;/a&gt;    &lt;span class="n"&gt;cumulative&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_e91f155c7ffd4c9b90fc5a183ba36291-13"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;pre class="code ipython"&gt;&lt;a name="rest_code_fbe49364bad2422a95c0fd932b8b9e62-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;TRIALS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5000&lt;/span&gt;
&lt;a name="rest_code_fbe49364bad2422a95c0fd932b8b9e62-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;TIMES&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;400&lt;/span&gt;
&lt;a name="rest_code_fbe49364bad2422a95c0fd932b8b9e62-3"&gt;&lt;/a&gt;&lt;span class="n"&gt;similar_payout_rates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;6.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_fbe49364bad2422a95c0fd932b8b9e62-4"&gt;&lt;/a&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;similar_payout_rates&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_fbe49364bad2422a95c0fd932b8b9e62-5"&gt;&lt;/a&gt;&lt;span class="n"&gt;one_good_arm_rates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;9.0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;a name="rest_code_fbe49364bad2422a95c0fd932b8b9e62-6"&gt;&lt;/a&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;one_good_arm_rates&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;div class="section" id="similar-arms"&gt;
&lt;h3&gt;4.1 Similar Arms&lt;/h3&gt;
&lt;p&gt;This will create a range where each arm only differs by 0.1&lt;/p&gt;
&lt;pre class="code ipython"&gt;&lt;a name="rest_code_e4202bf9f9db4ba3b03001746dbe1faa-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;optimistic_agent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;OptimisticInitialValues&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;similar_payout_rates&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;10.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_e4202bf9f9db4ba3b03001746dbe1faa-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
&lt;a name="rest_code_e4202bf9f9db4ba3b03001746dbe1faa-3"&gt;&lt;/a&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Optimistic Initial Values"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cumulative_reward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimistic_agent&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trials&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TRIALS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TIMES&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_e4202bf9f9db4ba3b03001746dbe1faa-4"&gt;&lt;/a&gt;&lt;span class="n"&gt;epsilon_agent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;EpsilonGreedyNormal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;similar_payout_rates&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_e4202bf9f9db4ba3b03001746dbe1faa-5"&gt;&lt;/a&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Epsilon Greedy (0.1)"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cumulative_reward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epsilon_agent&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trials&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TRIALS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TIMES&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_e4202bf9f9db4ba3b03001746dbe1faa-6"&gt;&lt;/a&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_e4202bf9f9db4ba3b03001746dbe1faa-7"&gt;&lt;/a&gt;&lt;span class="n"&gt;plot_cumulative&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;img alt="optimistic_similar_cumulative.png" src="https://necromuralist.github.io/reinforcement_learning/posts/assessing-the-performance/optimistic_similar_cumulative.png"&gt;
&lt;p&gt;The Optimistic Initial Values agent does better than the Epsilon Greedy, as you would expect (since it eventually stops exploring). But it looks suspisciously linear.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="one-good-arm"&gt;
&lt;h3&gt;4.2 One Good Arm&lt;/h3&gt;
&lt;p&gt;Lets see how it goes when one arm dominates the payouts.&lt;/p&gt;
&lt;pre class="code ipython"&gt;&lt;a name="rest_code_f700a5d944f94d90bfb9dbdffa071196-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;one_good_arm_rates&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_f700a5d944f94d90bfb9dbdffa071196-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;optimistic_agent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;OptimisticInitialValues&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;one_good_arm_rates&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;10.&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_f700a5d944f94d90bfb9dbdffa071196-3"&gt;&lt;/a&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
&lt;a name="rest_code_f700a5d944f94d90bfb9dbdffa071196-4"&gt;&lt;/a&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Optimistic Initial Values"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cumulative_reward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimistic_agent&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trials&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TRIALS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TIMES&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_f700a5d944f94d90bfb9dbdffa071196-5"&gt;&lt;/a&gt;&lt;span class="n"&gt;epsilon_agent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;EpsilonGreedyNormal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;one_good_arm_rates&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_f700a5d944f94d90bfb9dbdffa071196-6"&gt;&lt;/a&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Epsilon Greedy (0.1)"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cumulative_reward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epsilon_agent&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trials&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TRIALS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TIMES&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_f700a5d944f94d90bfb9dbdffa071196-7"&gt;&lt;/a&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_f700a5d944f94d90bfb9dbdffa071196-8"&gt;&lt;/a&gt;&lt;span class="n"&gt;plot_cumulative&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;img alt="optimistic_cumulative_one_good_arm.png" src="https://necromuralist.github.io/reinforcement_learning/posts/assessing-the-performance/optimistic_cumulative_one_good_arm.png"&gt;
&lt;p&gt;It looks like the optimistic agent does even better with one dominant arm. Likely because it found it quick enough that always exploiting it gives it a huge advantage over the epsilon greedy, which never stops exploring.&lt;/p&gt;
&lt;pre class="code ipython"&gt;&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-1"&gt;&lt;/a&gt;&lt;span class="nd"&gt;@jit&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;average_reward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trials&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-3"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""this generates the average reward for the trials over time&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     trials (int): number of times to train the agent&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-7"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     times (int): length of time to train the agent&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-8"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-9"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     numpy.array: the average reward&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-10"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-11"&gt;&lt;/a&gt;    &lt;span class="n"&gt;average_rewards&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-12"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;trial&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trials&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-13"&gt;&lt;/a&gt;        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-14"&gt;&lt;/a&gt;            &lt;span class="n"&gt;arm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select_arm&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-15"&gt;&lt;/a&gt;            &lt;span class="n"&gt;old_reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-16"&gt;&lt;/a&gt;            &lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-17"&gt;&lt;/a&gt;            &lt;span class="n"&gt;average_rewards&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;old_reward&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-18"&gt;&lt;/a&gt;        &lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-19"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;average_rewards&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;trials&lt;/span&gt;
&lt;/pre&gt;&lt;pre class="code ipython"&gt;&lt;a name="rest_code_493ab2bb18e64c8f8dd038db6c1198df-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
&lt;a name="rest_code_493ab2bb18e64c8f8dd038db6c1198df-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Optimistic Initial Values"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;average_reward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimistic_agent&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;TIMES&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;TRIALS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_493ab2bb18e64c8f8dd038db6c1198df-3"&gt;&lt;/a&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Epsilon 0.1"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;average_reward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epsilon_agent&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;TIMES&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;TRIALS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;pre class="code ipython"&gt;&lt;a name="rest_code_0d7788727eae4a418a37b68eaf63dd8a-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;averages&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;pre class="code ipython"&gt;&lt;a name="rest_code_ae6c43cabdb7467fad6f3ec6c15697b0-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;figure&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_ae6c43cabdb7467fad6f3ec6c15697b0-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;axe&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gca&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_ae6c43cabdb7467fad6f3ec6c15697b0-3"&gt;&lt;/a&gt;&lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Average Reward (One Dominant Arm)"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_ae6c43cabdb7467fad6f3ec6c15697b0-4"&gt;&lt;/a&gt;&lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Time (number of pulls on the arm)"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_ae6c43cabdb7467fad6f3ec6c15697b0-5"&gt;&lt;/a&gt;&lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Average Reward"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_ae6c43cabdb7467fad6f3ec6c15697b0-6"&gt;&lt;/a&gt;&lt;span class="n"&gt;averages&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;marker&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'.'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;linestyle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"None"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;img alt="optimistic_averages.png" src="https://necromuralist.github.io/reinforcement_learning/posts/assessing-the-performance/optimistic_averages.png"&gt;
&lt;p&gt;It looks like there was a brief period where the Epsilon Greedy did better, but the Optimistic agent settled in fairly quickly.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>bandits reinforcementLearning</category><guid>https://necromuralist.github.io/reinforcement_learning/posts/assessing-the-performance/</guid><pubDate>Wed, 02 Aug 2017 01:46:00 GMT</pubDate></item></channel></rss>