<div class="contents topic" id="contents">
<p class="topic-title first">Contents</p>
<ul class="simple">
<li><a class="reference internal" href="#introduction" id="id1">1 Introduction</a></li>
<li><a class="reference internal" href="#imports" id="id2">2 Imports</a></li>
<li><a class="reference internal" href="#set-up-the-plotting" id="id3">3 Set-up the Plotting</a></li>
<li><a class="reference internal" href="#the-probabilities" id="id4">4 The Probabilities</a><ul>
<li><a class="reference internal" href="#generate-the-probabilities" id="id5">4.1 Generate the Probabilities</a></li>
<li><a class="reference internal" href="#inspecting-the-outcome" id="id6">4.2 Inspecting the Outcome</a></li>
</ul>
</li>
<li><a class="reference internal" href="#average-reward" id="id7">5 Average Reward</a></li>
<li><a class="reference internal" href="#cumulative-reward" id="id8">6 Cumulative Reward</a><ul>
<li><a class="reference internal" href="#imbalanced-case" id="id9">6.1 Imbalanced Case</a></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="introduction">
<h1><a class="toc-backref" href="#id1">1 Introduction</a></h1>
<p>This is one of the ways to characterize the performance of the <em>Epsilon Greedy</em> agent using our <em>Bernoulli Arm</em>. We are going to look at three ways to evaluate how well the algorithm does.</p>
<ul class="simple">
<li>probability of using the best arm</li>
<li>average reward</li>
<li>cumulative reward</li>
</ul>
</div>
<div class="section" id="imports">
<h1><a class="toc-backref" href="#id2">2 Imports</a></h1>
<pre class="code ipython"><a name="rest_code_834438fbb48646d296e7c47619f53dba-1"></a><span class="c1"># python standard library</span>
<a name="rest_code_834438fbb48646d296e7c47619f53dba-2"></a><span class="kn">import</span> <span class="nn">random</span>
<a name="rest_code_834438fbb48646d296e7c47619f53dba-3"></a><span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
<a name="rest_code_834438fbb48646d296e7c47619f53dba-4"></a>
<a name="rest_code_834438fbb48646d296e7c47619f53dba-5"></a><span class="c1"># pypi</span>
<a name="rest_code_834438fbb48646d296e7c47619f53dba-6"></a><span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">jit</span>
<a name="rest_code_834438fbb48646d296e7c47619f53dba-7"></a><span class="kn">import</span> <span class="nn">numpy</span>
<a name="rest_code_834438fbb48646d296e7c47619f53dba-8"></a><span class="kn">import</span> <span class="nn">pandas</span>
<a name="rest_code_834438fbb48646d296e7c47619f53dba-9"></a><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plot</span>
<a name="rest_code_834438fbb48646d296e7c47619f53dba-10"></a><span class="kn">import</span> <span class="nn">seaborn</span>
<a name="rest_code_834438fbb48646d296e7c47619f53dba-11"></a>
<a name="rest_code_834438fbb48646d296e7c47619f53dba-12"></a><span class="c1"># this project</span>
<a name="rest_code_834438fbb48646d296e7c47619f53dba-13"></a><span class="kn">from</span> <span class="nn">epsilon_greedy</span> <span class="kn">import</span> <span class="p">(</span>
<a name="rest_code_834438fbb48646d296e7c47619f53dba-14"></a>    <span class="n">EpsilonGreedy</span><span class="p">,</span>
<a name="rest_code_834438fbb48646d296e7c47619f53dba-15"></a>    <span class="n">find_first</span>
<a name="rest_code_834438fbb48646d296e7c47619f53dba-16"></a><span class="p">)</span>
<a name="rest_code_834438fbb48646d296e7c47619f53dba-17"></a><span class="kn">from</span> <span class="nn">epsilon_greedy_optimized</span> <span class="kn">import</span> <span class="n">EpsilonGreedyOptimized</span>
<a name="rest_code_834438fbb48646d296e7c47619f53dba-18"></a><span class="kn">from</span> <span class="nn">bernoulli_arm</span> <span class="kn">import</span> <span class="n">BernoulliArm</span>
</pre></div>
<div class="section" id="set-up-the-plotting">
<h1><a class="toc-backref" href="#id3">3 Set-up the Plotting</a></h1>
<p>This will enable the plotting and set the style.</p>
<pre class="code ipython"><a name="rest_code_1759baf9b35349d098682ef85ec70976-1"></a><span class="o">%</span><span class="k">matplotlib</span> inline
<a name="rest_code_1759baf9b35349d098682ef85ec70976-2"></a><span class="n">seaborn</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">&quot;whitegrid&quot;</span><span class="p">)</span>
</pre></div>
<div class="section" id="the-probabilities">
<h1><a class="toc-backref" href="#id4">4 The Probabilities</a></h1>
<div class="section" id="generate-the-probabilities">
<h2><a class="toc-backref" href="#id5">4.1 Generate the Probabilities</a></h2>
<p>This code will run generate the probabilities. Although I made it so that using the <tt class="docutils literal">EpsilonGreedy</tt> call method would both choose the arm and update the reward, in this case we need to know which arm was selected so I'm going to do the steps individually.</p>
<pre class="code ipython"><a name="rest_code_b1c41e5f96ea4a6eb2d5a4b725146a91-1"></a><span class="k">def</span> <span class="nf">generate_probabilities</span><span class="p">(</span><span class="n">times</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">trials</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
<a name="rest_code_b1c41e5f96ea4a6eb2d5a4b725146a91-2"></a>    <span class="sd">&quot;&quot;&quot;this generates the probabilites for finding the best arm</span>
<a name="rest_code_b1c41e5f96ea4a6eb2d5a4b725146a91-3"></a>
<a name="rest_code_b1c41e5f96ea4a6eb2d5a4b725146a91-4"></a><span class="sd">    Args:</span>
<a name="rest_code_b1c41e5f96ea4a6eb2d5a4b725146a91-5"></a><span class="sd">     trials (int): number of times to train the agent</span>
<a name="rest_code_b1c41e5f96ea4a6eb2d5a4b725146a91-6"></a><span class="sd">     times (int): length of time to train the agent</span>
<a name="rest_code_b1c41e5f96ea4a6eb2d5a4b725146a91-7"></a><span class="sd">    Returns:</span>
<a name="rest_code_b1c41e5f96ea4a6eb2d5a4b725146a91-8"></a><span class="sd">     Dict: the probabilites for each epsilon over time</span>
<a name="rest_code_b1c41e5f96ea4a6eb2d5a4b725146a91-9"></a><span class="sd">     &quot;&quot;&quot;</span>
<a name="rest_code_b1c41e5f96ea4a6eb2d5a4b725146a91-10"></a>    <span class="n">arm_probabilities</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]</span>
<a name="rest_code_b1c41e5f96ea4a6eb2d5a4b725146a91-11"></a>    <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">arm_probabilities</span><span class="p">)</span>
<a name="rest_code_b1c41e5f96ea4a6eb2d5a4b725146a91-12"></a>    <span class="n">best_arm</span> <span class="o">=</span> <span class="n">arm_probabilities</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">arm_probabilities</span><span class="p">))</span>
<a name="rest_code_b1c41e5f96ea4a6eb2d5a4b725146a91-13"></a>    <span class="n">arms</span> <span class="o">=</span> <span class="p">[</span><span class="n">BernoulliArm</span><span class="p">(</span><span class="n">probability</span><span class="p">)</span> <span class="k">for</span> <span class="n">probability</span> <span class="ow">in</span> <span class="n">arm_probabilities</span><span class="p">]</span>
<a name="rest_code_b1c41e5f96ea4a6eb2d5a4b725146a91-14"></a>    <span class="n">epsilons</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
<a name="rest_code_b1c41e5f96ea4a6eb2d5a4b725146a91-15"></a>
<a name="rest_code_b1c41e5f96ea4a6eb2d5a4b725146a91-16"></a>    <span class="n">outcomes</span> <span class="o">=</span> <span class="p">{}</span>
<a name="rest_code_b1c41e5f96ea4a6eb2d5a4b725146a91-17"></a>    <span class="k">for</span> <span class="n">epsilon</span> <span class="ow">in</span> <span class="n">epsilons</span><span class="p">:</span>
<a name="rest_code_b1c41e5f96ea4a6eb2d5a4b725146a91-18"></a>        <span class="n">agent</span> <span class="o">=</span> <span class="n">EpsilonGreedy</span><span class="p">(</span><span class="n">epsilon</span><span class="p">,</span> <span class="n">arms</span><span class="p">)</span>
<a name="rest_code_b1c41e5f96ea4a6eb2d5a4b725146a91-19"></a>        <span class="n">probabilities</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">times</span><span class="p">)</span>
<a name="rest_code_b1c41e5f96ea4a6eb2d5a4b725146a91-20"></a>        <span class="k">for</span> <span class="n">trial</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">trials</span><span class="p">):</span>
<a name="rest_code_b1c41e5f96ea4a6eb2d5a4b725146a91-21"></a>            <span class="k">for</span> <span class="n">time</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">times</span><span class="p">):</span>
<a name="rest_code_b1c41e5f96ea4a6eb2d5a4b725146a91-22"></a>                <span class="n">arm</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">select_arm</span><span class="p">()</span>
<a name="rest_code_b1c41e5f96ea4a6eb2d5a4b725146a91-23"></a>                <span class="n">agent</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">arm</span><span class="p">)</span>
<a name="rest_code_b1c41e5f96ea4a6eb2d5a4b725146a91-24"></a>                <span class="k">if</span> <span class="n">arm</span> <span class="o">==</span> <span class="n">best_arm</span><span class="p">:</span>
<a name="rest_code_b1c41e5f96ea4a6eb2d5a4b725146a91-25"></a>                    <span class="n">probabilities</span><span class="p">[</span><span class="n">time</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
<a name="rest_code_b1c41e5f96ea4a6eb2d5a4b725146a91-26"></a>            <span class="n">agent</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<a name="rest_code_b1c41e5f96ea4a6eb2d5a4b725146a91-27"></a>        <span class="n">outcomes</span><span class="p">[</span><span class="s2">&quot;Epsilon {:.02f}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epsilon</span><span class="p">)]</span> <span class="o">=</span> <span class="n">probabilities</span><span class="o">/</span><span class="n">times</span>
<a name="rest_code_b1c41e5f96ea4a6eb2d5a4b725146a91-28"></a>    <span class="k">return</span> <span class="n">outcomes</span>
</pre></div>
<div class="section" id="inspecting-the-outcome">
<h2><a class="toc-backref" href="#id6">4.2 Inspecting the Outcome</a></h2>
<pre class="code ipython"><a name="rest_code_9c42454effd5484bb681c62a3ff7b48a-1"></a><span class="n">TRIALS</span> <span class="o">=</span> <span class="mi">5000</span>
<a name="rest_code_9c42454effd5484bb681c62a3ff7b48a-2"></a><span class="n">TIMES</span> <span class="o">=</span> <span class="mi">400</span>
</pre><pre class="code ipython"><a name="rest_code_591a4aa7329e4f6292c43e76f9837605-1"></a><span class="n">start</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>
<a name="rest_code_591a4aa7329e4f6292c43e76f9837605-2"></a><span class="n">probabilities</span> <span class="o">=</span> <span class="n">generate_probabilities</span><span class="p">(</span><span class="n">trials</span><span class="o">=</span><span class="n">TRIALS</span><span class="p">,</span> <span class="n">times</span><span class="o">=</span><span class="n">TIMES</span><span class="p">)</span>
<a name="rest_code_591a4aa7329e4f6292c43e76f9837605-3"></a><span class="k">print</span><span class="p">(</span><span class="s2">&quot;Run Time: {0}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>
<a name="rest_code_591a4aa7329e4f6292c43e76f9837605-4"></a><span class="n">probabilities</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">probabilities</span><span class="p">)</span>
<a name="rest_code_591a4aa7329e4f6292c43e76f9837605-5"></a><span class="n">probabilities</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre><pre class="literal-block">
       Epsilon 0.05  Epsilon 0.10  Epsilon 0.20  Epsilon 0.30  Epsilon 0.40  \
count    400.000000    400.000000    400.000000    400.000000    400.000000
mean       8.784956      9.971200      9.801344      9.084844      8.235194
std        3.064916      2.523568      1.775677      1.319447      0.998305
min        0.120000      0.275000      0.495000      0.757500      1.050000
25%        7.141875      9.740000     10.264375      9.390000      8.409375
50%        9.937500     11.187500     10.456250      9.475000      8.480000
75%       11.225000     11.457500     10.515000      9.530625      8.545000
max       11.720000     11.605000     10.727500      9.665000      8.712500

       Epsilon 0.50
count    400.000000
mean       7.310087
std        0.763259
min        1.190000
25%        7.406875
50%        7.473750
75%        7.535625
max        7.725000
</pre>
<pre class="code ipython"><a name="rest_code_731d93184c2743c1a434058f26e10903-1"></a><span class="n">probabilities</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s2">&quot;epsilon_greedy_accuracy.csv&quot;</span><span class="p">)</span>
</pre><pre class="code ipython"><a name="rest_code_607a48d6c095429381803452084250ef-1"></a><span class="n">figure</span> <span class="o">=</span> <span class="n">plot</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<a name="rest_code_607a48d6c095429381803452084250ef-2"></a><span class="n">axe</span> <span class="o">=</span> <span class="n">figure</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
<a name="rest_code_607a48d6c095429381803452084250ef-3"></a><span class="n">axe</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Accuracy of the Epsilon Greedy Algorithm ({} trials)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">TRIALS</span><span class="p">))</span>
<a name="rest_code_607a48d6c095429381803452084250ef-4"></a><span class="n">axe</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Time (number of pulls on the arm)&quot;</span><span class="p">)</span>
<a name="rest_code_607a48d6c095429381803452084250ef-5"></a><span class="n">axe</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Probability of retrieving the best arm&quot;</span><span class="p">)</span>
<a name="rest_code_607a48d6c095429381803452084250ef-6"></a><span class="n">probabilities</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axe</span><span class="p">)</span>
</pre><img alt="epsilon_greedy_probablilities.png" src="epsilon_greedy_probablilities.png" />
<p>Looking at the plots, it appears that the epsilons greater than 0.05 converge faster that 0.05 (their curves are steeper at the beginning), as you would expect, but they also don't do as well in the long run, as you might also expect, since they're doing more exploration. In the long run, the more exploitation, the better the profit, but I suppose it depends on the window you have to work with, if you have a short one, then the more aggresive explorers might be better. Anything less than 350 would do better with 0.1 rather than 0.05, for instance.</p>
</div>
</div>
<div class="section" id="average-reward">
<h1><a class="toc-backref" href="#id7">5 Average Reward</a></h1>
<p>One of the things to note about the previous trials is that there was one arm that did notably better than all the others. When they are more uniform using the probability of retrieving the best arm might not be as revealing. Instead, using the average reward so far would give us more information.</p>
<pre class="code ipython"><a name="rest_code_6b2a3a4e9248457c848dbf70c8075db7-1"></a><span class="nd">@jit</span>
<a name="rest_code_6b2a3a4e9248457c848dbf70c8075db7-2"></a><span class="k">def</span> <span class="nf">average_reward</span><span class="p">(</span><span class="n">times</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">trials</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
<a name="rest_code_6b2a3a4e9248457c848dbf70c8075db7-3"></a>    <span class="sd">&quot;&quot;&quot;this generates the probabilites for finding the best arm</span>
<a name="rest_code_6b2a3a4e9248457c848dbf70c8075db7-4"></a>
<a name="rest_code_6b2a3a4e9248457c848dbf70c8075db7-5"></a><span class="sd">    Args:</span>
<a name="rest_code_6b2a3a4e9248457c848dbf70c8075db7-6"></a><span class="sd">     trials (int): number of times to train the agent</span>
<a name="rest_code_6b2a3a4e9248457c848dbf70c8075db7-7"></a><span class="sd">     times (int): length of time to train the agent</span>
<a name="rest_code_6b2a3a4e9248457c848dbf70c8075db7-8"></a><span class="sd">    Returns:</span>
<a name="rest_code_6b2a3a4e9248457c848dbf70c8075db7-9"></a><span class="sd">     Dict: the probabilites for each epsilon over time</span>
<a name="rest_code_6b2a3a4e9248457c848dbf70c8075db7-10"></a><span class="sd">    &quot;&quot;&quot;</span>
<a name="rest_code_6b2a3a4e9248457c848dbf70c8075db7-11"></a>    <span class="n">arm_probabilities</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
<a name="rest_code_6b2a3a4e9248457c848dbf70c8075db7-12"></a>    <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">arm_probabilities</span><span class="p">)</span>
<a name="rest_code_6b2a3a4e9248457c848dbf70c8075db7-13"></a>    <span class="c1"># arms = [BernoulliArm(probability) for probability in arm_probabilities]</span>
<a name="rest_code_6b2a3a4e9248457c848dbf70c8075db7-14"></a>    <span class="n">epsilons</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
<a name="rest_code_6b2a3a4e9248457c848dbf70c8075db7-15"></a>
<a name="rest_code_6b2a3a4e9248457c848dbf70c8075db7-16"></a>    <span class="n">outcomes</span> <span class="o">=</span> <span class="p">{}</span>
<a name="rest_code_6b2a3a4e9248457c848dbf70c8075db7-17"></a>    <span class="k">for</span> <span class="n">epsilon</span> <span class="ow">in</span> <span class="n">epsilons</span><span class="p">:</span>
<a name="rest_code_6b2a3a4e9248457c848dbf70c8075db7-18"></a>        <span class="n">agent</span> <span class="o">=</span> <span class="n">EpsilonGreedyOptimized</span><span class="p">(</span><span class="n">epsilon</span><span class="p">,</span> <span class="n">arm_probabilities</span><span class="p">)</span>
<a name="rest_code_6b2a3a4e9248457c848dbf70c8075db7-19"></a>        <span class="n">average_rewards</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">times</span><span class="p">)</span>
<a name="rest_code_6b2a3a4e9248457c848dbf70c8075db7-20"></a>        <span class="k">for</span> <span class="n">trial</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">trials</span><span class="p">):</span>
<a name="rest_code_6b2a3a4e9248457c848dbf70c8075db7-21"></a>            <span class="k">for</span> <span class="n">time</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">times</span><span class="p">):</span>
<a name="rest_code_6b2a3a4e9248457c848dbf70c8075db7-22"></a>                <span class="n">arm</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">select_arm</span><span class="p">()</span>
<a name="rest_code_6b2a3a4e9248457c848dbf70c8075db7-23"></a>                <span class="n">old_reward</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">total_reward</span>
<a name="rest_code_6b2a3a4e9248457c848dbf70c8075db7-24"></a>                <span class="n">agent</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">arm</span><span class="p">)</span>
<a name="rest_code_6b2a3a4e9248457c848dbf70c8075db7-25"></a>                <span class="n">average_rewards</span><span class="p">[</span><span class="n">time</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">total_reward</span> <span class="o">-</span> <span class="n">old_reward</span><span class="p">)</span>
<a name="rest_code_6b2a3a4e9248457c848dbf70c8075db7-26"></a>            <span class="n">agent</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<a name="rest_code_6b2a3a4e9248457c848dbf70c8075db7-27"></a>        <span class="n">outcomes</span><span class="p">[</span><span class="s2">&quot;Epsilon {0:.02f}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epsilon</span><span class="p">)]</span> <span class="o">=</span> <span class="n">average_rewards</span><span class="o">/</span><span class="n">trials</span>
<a name="rest_code_6b2a3a4e9248457c848dbf70c8075db7-28"></a>    <span class="k">return</span> <span class="n">outcomes</span>
</pre><pre class="code ipython"><a name="rest_code_a90083430f474f8b8ceed9b8fd4e2f9a-1"></a><span class="n">start</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>
<a name="rest_code_a90083430f474f8b8ceed9b8fd4e2f9a-2"></a><span class="n">averages</span> <span class="o">=</span> <span class="n">average_reward</span><span class="p">(</span><span class="n">TIMES</span><span class="p">,</span> <span class="n">TRIALS</span><span class="p">)</span>
<a name="rest_code_a90083430f474f8b8ceed9b8fd4e2f9a-3"></a><span class="k">print</span><span class="p">(</span><span class="s2">&quot;Run Time: {0}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>
<a name="rest_code_a90083430f474f8b8ceed9b8fd4e2f9a-4"></a><span class="n">averages</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">averages</span><span class="p">)</span>
</pre><pre class="literal-block">
Run Time: 0:01:08.727723
</pre>
<pre class="code ipython"><a name="rest_code_8775a245b6d44460b330c49f643f6738-1"></a><span class="n">figure</span> <span class="o">=</span> <span class="n">plot</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<a name="rest_code_8775a245b6d44460b330c49f643f6738-2"></a><span class="n">axe</span> <span class="o">=</span> <span class="n">figure</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
<a name="rest_code_8775a245b6d44460b330c49f643f6738-3"></a><span class="n">axe</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Reward of the Epsilon Greedy Algorithm ({} trials)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">TRIALS</span><span class="p">))</span>
<a name="rest_code_8775a245b6d44460b330c49f643f6738-4"></a><span class="n">axe</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Time (number of pulls on the arm)&quot;</span><span class="p">)</span>
<a name="rest_code_8775a245b6d44460b330c49f643f6738-5"></a><span class="n">axe</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Average Reward&quot;</span><span class="p">)</span>
<a name="rest_code_8775a245b6d44460b330c49f643f6738-6"></a><span class="n">averages</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axe</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;None&quot;</span><span class="p">)</span>
</pre><img alt="epsilon_averages.png" src="epsilon_averages.png" />
<p>There's much more variablity and overlap here, as you might expect since I made the probabilities closer. Interestingly, the strongly exploratory agents seem to do worse, even from the beginning, while the more exploitative ones do better.  Although it looks like 0.2 might be doing as well or better than 0.1 once you get over 100.</p>
</div>
<div class="section" id="cumulative-reward">
<h1><a class="toc-backref" href="#id8">6 Cumulative Reward</a></h1>
<p>The previous two metrics turn out to be useful, but somewhat unfair to the aggresively exploring models, which we know won't ultimately do as well, but do have an advantage in the initial phase. To better qualify the overall effect of exploration versus exploitation, it's better to use a cumulative sum of the rewards.</p>
<pre class="code ipython"><a name="rest_code_196c599295dc4fd0b5c35740656c0e36-1"></a><span class="nd">@jit</span>
<a name="rest_code_196c599295dc4fd0b5c35740656c0e36-2"></a><span class="k">def</span> <span class="nf">cumulative_reward</span><span class="p">(</span><span class="n">arms</span><span class="p">,</span> <span class="n">times</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">trials</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
<a name="rest_code_196c599295dc4fd0b5c35740656c0e36-3"></a>    <span class="sd">&quot;&quot;&quot;this generates the cumulative reward as the agent pulls the arms</span>
<a name="rest_code_196c599295dc4fd0b5c35740656c0e36-4"></a>
<a name="rest_code_196c599295dc4fd0b5c35740656c0e36-5"></a><span class="sd">    Args:</span>
<a name="rest_code_196c599295dc4fd0b5c35740656c0e36-6"></a><span class="sd">     arms (numpy.array): array of probabilities that the arm will pay-off</span>
<a name="rest_code_196c599295dc4fd0b5c35740656c0e36-7"></a><span class="sd">     trials (int): number of times to train the agent</span>
<a name="rest_code_196c599295dc4fd0b5c35740656c0e36-8"></a><span class="sd">     times (int): length of time to train the agent</span>
<a name="rest_code_196c599295dc4fd0b5c35740656c0e36-9"></a><span class="sd">    Returns:</span>
<a name="rest_code_196c599295dc4fd0b5c35740656c0e36-10"></a><span class="sd">     Dict: the probabilites for each epsilon over time</span>
<a name="rest_code_196c599295dc4fd0b5c35740656c0e36-11"></a><span class="sd">    &quot;&quot;&quot;</span>
<a name="rest_code_196c599295dc4fd0b5c35740656c0e36-12"></a>    <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">arms</span><span class="p">)</span>
<a name="rest_code_196c599295dc4fd0b5c35740656c0e36-13"></a>    <span class="n">epsilons</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
<a name="rest_code_196c599295dc4fd0b5c35740656c0e36-14"></a>
<a name="rest_code_196c599295dc4fd0b5c35740656c0e36-15"></a>    <span class="n">outcomes</span> <span class="o">=</span> <span class="p">{}</span>
<a name="rest_code_196c599295dc4fd0b5c35740656c0e36-16"></a>    <span class="k">for</span> <span class="n">epsilon</span> <span class="ow">in</span> <span class="n">epsilons</span><span class="p">:</span>
<a name="rest_code_196c599295dc4fd0b5c35740656c0e36-17"></a>        <span class="n">agent</span> <span class="o">=</span> <span class="n">EpsilonGreedyOptimized</span><span class="p">(</span><span class="n">epsilon</span><span class="p">,</span> <span class="n">arms</span><span class="p">)</span>
<a name="rest_code_196c599295dc4fd0b5c35740656c0e36-18"></a>        <span class="n">cumulative_rewards</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">times</span><span class="p">)</span>
<a name="rest_code_196c599295dc4fd0b5c35740656c0e36-19"></a>        <span class="k">for</span> <span class="n">trial</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">trials</span><span class="p">):</span>
<a name="rest_code_196c599295dc4fd0b5c35740656c0e36-20"></a>            <span class="k">for</span> <span class="n">time</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">times</span><span class="p">):</span>
<a name="rest_code_196c599295dc4fd0b5c35740656c0e36-21"></a>                <span class="n">arm</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">select_arm</span><span class="p">()</span>
<a name="rest_code_196c599295dc4fd0b5c35740656c0e36-22"></a>                <span class="n">agent</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">arm</span><span class="p">)</span>
<a name="rest_code_196c599295dc4fd0b5c35740656c0e36-23"></a>                <span class="n">cumulative_rewards</span><span class="p">[</span><span class="n">time</span><span class="p">]</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">total_reward</span>
<a name="rest_code_196c599295dc4fd0b5c35740656c0e36-24"></a>            <span class="n">agent</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<a name="rest_code_196c599295dc4fd0b5c35740656c0e36-25"></a>        <span class="n">outcomes</span><span class="p">[</span><span class="s2">&quot;Epsilon {:.02f}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epsilon</span><span class="p">)]</span> <span class="o">=</span> <span class="n">cumulative_rewards</span><span class="o">/</span><span class="n">trials</span>
<a name="rest_code_196c599295dc4fd0b5c35740656c0e36-26"></a>    <span class="k">return</span> <span class="n">outcomes</span>
</pre><pre class="code ipython"><a name="rest_code_0ae492ba1c024af9965e5479b989c3b4-1"></a><span class="k">def</span> <span class="nf">generate_cumulative</span><span class="p">(</span><span class="n">arms</span><span class="p">):</span>
<a name="rest_code_0ae492ba1c024af9965e5479b989c3b4-2"></a>    <span class="sd">&quot;&quot;&quot;runs the cumulative output function</span>
<a name="rest_code_0ae492ba1c024af9965e5479b989c3b4-3"></a>
<a name="rest_code_0ae492ba1c024af9965e5479b989c3b4-4"></a><span class="sd">    Args:</span>
<a name="rest_code_0ae492ba1c024af9965e5479b989c3b4-5"></a><span class="sd">     arms (numpy.array): probabilities that arms will pay out</span>
<a name="rest_code_0ae492ba1c024af9965e5479b989c3b4-6"></a>
<a name="rest_code_0ae492ba1c024af9965e5479b989c3b4-7"></a><span class="sd">    Returns:</span>
<a name="rest_code_0ae492ba1c024af9965e5479b989c3b4-8"></a><span class="sd">     pandas.DataFrame: the average cumulative rewards</span>
<a name="rest_code_0ae492ba1c024af9965e5479b989c3b4-9"></a><span class="sd">    &quot;&quot;&quot;</span>
<a name="rest_code_0ae492ba1c024af9965e5479b989c3b4-10"></a>    <span class="n">start</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>
<a name="rest_code_0ae492ba1c024af9965e5479b989c3b4-11"></a>    <span class="n">cumulative</span> <span class="o">=</span> <span class="n">cumulative_reward</span><span class="p">(</span><span class="n">arms</span><span class="p">,</span> <span class="n">times</span><span class="o">=</span><span class="n">TIMES</span><span class="p">,</span> <span class="n">trials</span><span class="o">=</span><span class="n">TRIALS</span><span class="p">)</span>
<a name="rest_code_0ae492ba1c024af9965e5479b989c3b4-12"></a>    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Run Time: {0}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>
<a name="rest_code_0ae492ba1c024af9965e5479b989c3b4-13"></a>    <span class="k">return</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">cumulative</span><span class="p">)</span>
</pre><pre class="code ipython"><a name="rest_code_aec74ae60b574f8d9665aea8cceb6de3-1"></a><span class="k">def</span> <span class="nf">plot_cumulative</span><span class="p">(</span><span class="n">cumulative</span><span class="p">):</span>
<a name="rest_code_aec74ae60b574f8d9665aea8cceb6de3-2"></a>    <span class="sd">&quot;&quot;&quot;generates and plots cumulative average</span>
<a name="rest_code_aec74ae60b574f8d9665aea8cceb6de3-3"></a>
<a name="rest_code_aec74ae60b574f8d9665aea8cceb6de3-4"></a><span class="sd">    Args:</span>
<a name="rest_code_aec74ae60b574f8d9665aea8cceb6de3-5"></a><span class="sd">     cumulative (pandas.DataFrame): data to plot</span>
<a name="rest_code_aec74ae60b574f8d9665aea8cceb6de3-6"></a><span class="sd">    &quot;&quot;&quot;</span>
<a name="rest_code_aec74ae60b574f8d9665aea8cceb6de3-7"></a>    <span class="n">figure</span> <span class="o">=</span> <span class="n">plot</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<a name="rest_code_aec74ae60b574f8d9665aea8cceb6de3-8"></a>    <span class="n">axe</span> <span class="o">=</span> <span class="n">figure</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
<a name="rest_code_aec74ae60b574f8d9665aea8cceb6de3-9"></a>    <span class="n">axe</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Cumulative Reward of the Epsilon Greedy Algorithm ({} trials)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">TRIALS</span><span class="p">))</span>
<a name="rest_code_aec74ae60b574f8d9665aea8cceb6de3-10"></a>    <span class="n">axe</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Time (number of pulls on the arm)&quot;</span><span class="p">)</span>
<a name="rest_code_aec74ae60b574f8d9665aea8cceb6de3-11"></a>    <span class="n">axe</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Cumulative Reward&quot;</span><span class="p">)</span>
<a name="rest_code_aec74ae60b574f8d9665aea8cceb6de3-12"></a>    <span class="n">cumulative</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axe</span><span class="p">)</span>
<a name="rest_code_aec74ae60b574f8d9665aea8cceb6de3-13"></a>    <span class="k">return</span>
</pre><pre class="code ipython"><a name="rest_code_429a7b695e434d17a3dce7c2750550e6-1"></a><span class="n">cumulative</span> <span class="o">=</span> <span class="n">generate_cumulative</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">))</span>
<a name="rest_code_429a7b695e434d17a3dce7c2750550e6-2"></a><span class="n">plot_cumulative</span><span class="p">(</span><span class="n">cumulative</span><span class="p">)</span>
</pre><img alt="epsilon_greedy_cumulative.png" src="epsilon_greedy_cumulative.png" />
<p>Because of the randomness this will change everytime you run it, but we can see that in this case, the average cumulative reward was better for the 0.3 and 0.5 epsilon values that the more conservative values up until around 275, and the second most conservative case (0.2) actually did worse on average than the more exploratory cases did.</p>
<div class="section" id="imbalanced-case">
<h2><a class="toc-backref" href="#id9">6.1 Imbalanced Case</a></h2>
<p>I'll re-run this again with more arms and a only one clear good arm to see if this changes things.</p>
<pre class="code ipython"><a name="rest_code_318a4e5caa8142439bdd9fdbd9d9a555-1"></a><span class="n">plot_cumulative</span><span class="p">(</span><span class="n">generate_cumulative</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">10</span> <span class="o">+</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">])))</span>
</pre><img alt="epsilon_cumulative_2.png" src="epsilon_cumulative_2.png" />
<p>In this case, the most exploitive agent did much worse than the other agents. It looks like it didn't find the best arm until around the 240th pull. In this case, when most arms pay off poorly and one arm pays off much better, the exploratory arms accumulate more reward within our time frame. I'm guessing that the 0.10 epsilon would, given enough time, pull ahead, and you can in fact see that the most exploratory agent has already been surpassed by the 0.2 agent, so eventually exploration would probably take a back seat to exploitation, but not in this case. It's important to note, however, that if the most exploitive agent had happened to find the best arm at the start, he would likely have ended up the best, it's just the nature of randomization that you aren't guaranteed that this would be the case.</p>
</div>
</div>
