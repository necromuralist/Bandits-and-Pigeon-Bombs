<div class="section" id="introduction">
<h1>1 Introduction</h1>
<p>As with the Epsilon-Greedy algorithm I'm going to use the Cumulative Reward as the metric. In this case we don't really have a parameter to tune.</p>
</div>
<div class="section" id="imports">
<h1>2 Imports</h1>
<p>The dependencies.</p>
<pre class="code ipython"><a name="rest_code_3358eeed38864d27ae51e17c5af95efc-1"></a><span class="c1"># python standard library</span>
<a name="rest_code_3358eeed38864d27ae51e17c5af95efc-2"></a><span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
<a name="rest_code_3358eeed38864d27ae51e17c5af95efc-3"></a>
<a name="rest_code_3358eeed38864d27ae51e17c5af95efc-4"></a><span class="c1"># pypi</span>
<a name="rest_code_3358eeed38864d27ae51e17c5af95efc-5"></a><span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">jit</span>
<a name="rest_code_3358eeed38864d27ae51e17c5af95efc-6"></a><span class="kn">import</span> <span class="nn">numpy</span>
<a name="rest_code_3358eeed38864d27ae51e17c5af95efc-7"></a><span class="kn">import</span> <span class="nn">pandas</span>
<a name="rest_code_3358eeed38864d27ae51e17c5af95efc-8"></a><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plot</span>
<a name="rest_code_3358eeed38864d27ae51e17c5af95efc-9"></a><span class="kn">import</span> <span class="nn">seaborn</span>
<a name="rest_code_3358eeed38864d27ae51e17c5af95efc-10"></a>
<a name="rest_code_3358eeed38864d27ae51e17c5af95efc-11"></a><span class="c1"># this project</span>
<a name="rest_code_3358eeed38864d27ae51e17c5af95efc-12"></a><span class="kn">from</span> <span class="nn">optimistic_initial_values</span> <span class="kn">import</span> <span class="n">OptimisticInitialValues</span>
<a name="rest_code_3358eeed38864d27ae51e17c5af95efc-13"></a><span class="kn">from</span> <span class="nn">epsilon_greedy_normal</span> <span class="kn">import</span> <span class="n">EpsilonGreedyNormal</span>
</pre></div>
<div class="section" id="set-up-the-plotting">
<h1>3 Set-up the Plotting</h1>
<p>This will enable the plotting and set the style.</p>
<pre class="code ipython"><a name="rest_code_484c0471db0c4eb2a4073d45f26c714b-1"></a><span class="o">%</span><span class="k">matplotlib</span> inline
<a name="rest_code_484c0471db0c4eb2a4073d45f26c714b-2"></a><span class="n">seaborn</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">&quot;whitegrid&quot;</span><span class="p">)</span>
</pre></div>
<div class="section" id="running-the-assessment">
<h1>4 Running the Assessment</h1>
<pre class="code ipython"><a name="rest_code_27d82a5f91b743d3984a55150a3de70d-1"></a><span class="nd">@jit</span>
<a name="rest_code_27d82a5f91b743d3984a55150a3de70d-2"></a><span class="k">def</span> <span class="nf">cumulative_reward</span><span class="p">(</span><span class="n">agent</span><span class="p">,</span> <span class="n">times</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">trials</span><span class="o">=</span><span class="mi">400</span><span class="p">):</span>
<a name="rest_code_27d82a5f91b743d3984a55150a3de70d-3"></a>    <span class="sd">&quot;&quot;&quot;this generates the cumulative reward as the agent pulls the arms</span>
<a name="rest_code_27d82a5f91b743d3984a55150a3de70d-4"></a>
<a name="rest_code_27d82a5f91b743d3984a55150a3de70d-5"></a><span class="sd">    Args:</span>
<a name="rest_code_27d82a5f91b743d3984a55150a3de70d-6"></a><span class="sd">     agent: implementation that selects and updates the arms</span>
<a name="rest_code_27d82a5f91b743d3984a55150a3de70d-7"></a><span class="sd">     trials (int): number of times to train the agent</span>
<a name="rest_code_27d82a5f91b743d3984a55150a3de70d-8"></a><span class="sd">     times (int): length of time to train the agent</span>
<a name="rest_code_27d82a5f91b743d3984a55150a3de70d-9"></a><span class="sd">    Returns:</span>
<a name="rest_code_27d82a5f91b743d3984a55150a3de70d-10"></a><span class="sd">     numpy.array: average cumulative rewards over time</span>
<a name="rest_code_27d82a5f91b743d3984a55150a3de70d-11"></a><span class="sd">    &quot;&quot;&quot;</span>
<a name="rest_code_27d82a5f91b743d3984a55150a3de70d-12"></a>    <span class="n">cumulative_rewards</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">times</span><span class="p">)</span>
<a name="rest_code_27d82a5f91b743d3984a55150a3de70d-13"></a>    <span class="k">for</span> <span class="n">trial</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">trials</span><span class="p">):</span>
<a name="rest_code_27d82a5f91b743d3984a55150a3de70d-14"></a>        <span class="k">for</span> <span class="n">time</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">times</span><span class="p">):</span>
<a name="rest_code_27d82a5f91b743d3984a55150a3de70d-15"></a>            <span class="n">arm</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">select_arm</span><span class="p">()</span>
<a name="rest_code_27d82a5f91b743d3984a55150a3de70d-16"></a>            <span class="n">agent</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">arm</span><span class="p">)</span>
<a name="rest_code_27d82a5f91b743d3984a55150a3de70d-17"></a>            <span class="n">cumulative_rewards</span><span class="p">[</span><span class="n">time</span><span class="p">]</span> <span class="o">+=</span> <span class="n">agent</span><span class="o">.</span><span class="n">total_reward</span>
<a name="rest_code_27d82a5f91b743d3984a55150a3de70d-18"></a>        <span class="n">agent</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<a name="rest_code_27d82a5f91b743d3984a55150a3de70d-19"></a>    <span class="k">return</span> <span class="n">cumulative_rewards</span><span class="o">/</span><span class="n">trials</span>
</pre><pre class="code ipython"><a name="rest_code_c0c6d04e9d814ee2a6199e455575c855-1"></a><span class="k">def</span> <span class="nf">plot_cumulative</span><span class="p">(</span><span class="n">cumulative</span><span class="p">):</span>
<a name="rest_code_c0c6d04e9d814ee2a6199e455575c855-2"></a>    <span class="sd">&quot;&quot;&quot;generates and plots cumulative average</span>
<a name="rest_code_c0c6d04e9d814ee2a6199e455575c855-3"></a>
<a name="rest_code_c0c6d04e9d814ee2a6199e455575c855-4"></a><span class="sd">    Args:</span>
<a name="rest_code_c0c6d04e9d814ee2a6199e455575c855-5"></a><span class="sd">     cumulative (pandas.DataFrame): data to plot</span>
<a name="rest_code_c0c6d04e9d814ee2a6199e455575c855-6"></a><span class="sd">    &quot;&quot;&quot;</span>
<a name="rest_code_c0c6d04e9d814ee2a6199e455575c855-7"></a>    <span class="n">figure</span> <span class="o">=</span> <span class="n">plot</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<a name="rest_code_c0c6d04e9d814ee2a6199e455575c855-8"></a>    <span class="n">axe</span> <span class="o">=</span> <span class="n">figure</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
<a name="rest_code_c0c6d04e9d814ee2a6199e455575c855-9"></a>    <span class="n">axe</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Cumulative Reward of the Optimistic Initial Values Algorithm ({} trials)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">TRIALS</span><span class="p">))</span>
<a name="rest_code_c0c6d04e9d814ee2a6199e455575c855-10"></a>    <span class="n">axe</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Time (number of pulls on the arm)&quot;</span><span class="p">)</span>
<a name="rest_code_c0c6d04e9d814ee2a6199e455575c855-11"></a>    <span class="n">axe</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Cumulative Reward&quot;</span><span class="p">)</span>
<a name="rest_code_c0c6d04e9d814ee2a6199e455575c855-12"></a>    <span class="n">cumulative</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axe</span><span class="p">)</span>
<a name="rest_code_c0c6d04e9d814ee2a6199e455575c855-13"></a>    <span class="k">return</span>
</pre><pre class="code ipython"><a name="rest_code_5b26abc25fa940a9ab13c22b88ad1d77-1"></a><span class="n">TRIALS</span> <span class="o">=</span> <span class="mi">5000</span>
<a name="rest_code_5b26abc25fa940a9ab13c22b88ad1d77-2"></a><span class="n">TIMES</span> <span class="o">=</span> <span class="mi">400</span>
<a name="rest_code_5b26abc25fa940a9ab13c22b88ad1d77-3"></a><span class="n">similar_payout_rates</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">)</span>
<a name="rest_code_5b26abc25fa940a9ab13c22b88ad1d77-4"></a><span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">similar_payout_rates</span><span class="p">)</span>
<a name="rest_code_5b26abc25fa940a9ab13c22b88ad1d77-5"></a><span class="n">one_good_arm_rates</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">10</span> <span class="o">+</span> <span class="p">[</span><span class="mf">9.0</span><span class="p">])</span>
<a name="rest_code_5b26abc25fa940a9ab13c22b88ad1d77-6"></a><span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">one_good_arm_rates</span><span class="p">)</span>
</pre><div class="section" id="similar-arms">
<h2>4.1 Similar Arms</h2>
<p>This will create a range where each arm only differs by 0.1</p>
<pre class="code ipython"><a name="rest_code_5ec96ee2641b4534aab8336d29b5511e-1"></a><span class="n">optimistic_agent</span> <span class="o">=</span> <span class="n">OptimisticInitialValues</span><span class="p">(</span><span class="n">similar_payout_rates</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)</span>
<a name="rest_code_5ec96ee2641b4534aab8336d29b5511e-2"></a><span class="n">data</span> <span class="o">=</span> <span class="p">{}</span>
<a name="rest_code_5ec96ee2641b4534aab8336d29b5511e-3"></a><span class="n">data</span><span class="p">[</span><span class="s2">&quot;Optimistic Initial Values&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">cumulative_reward</span><span class="p">(</span><span class="n">optimistic_agent</span><span class="p">,</span> <span class="n">trials</span><span class="o">=</span><span class="n">TRIALS</span><span class="p">,</span> <span class="n">times</span><span class="o">=</span><span class="n">TIMES</span><span class="p">)</span>
<a name="rest_code_5ec96ee2641b4534aab8336d29b5511e-4"></a><span class="n">epsilon_agent</span> <span class="o">=</span> <span class="n">EpsilonGreedyNormal</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">similar_payout_rates</span><span class="p">)</span>
<a name="rest_code_5ec96ee2641b4534aab8336d29b5511e-5"></a><span class="n">data</span><span class="p">[</span><span class="s2">&quot;Epsilon Greedy (0.1)&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">cumulative_reward</span><span class="p">(</span><span class="n">epsilon_agent</span><span class="p">,</span> <span class="n">trials</span><span class="o">=</span><span class="n">TRIALS</span><span class="p">,</span> <span class="n">times</span><span class="o">=</span><span class="n">TIMES</span><span class="p">)</span>
<a name="rest_code_5ec96ee2641b4534aab8336d29b5511e-6"></a><span class="n">data</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<a name="rest_code_5ec96ee2641b4534aab8336d29b5511e-7"></a><span class="n">plot_cumulative</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre><img alt="optimistic_similar_cumulative.png" src="optimistic_similar_cumulative.png" />
<p>The Optimistic Initial Values agent does better than the Epsilon Greedy, as you would expect (since it eventually stops exploring). But it looks suspisciously linear.</p>
</div>
<div class="section" id="one-good-arm">
<h2>4.2 One Good Arm</h2>
<p>Lets see how it goes when one arm dominates the payouts.</p>
<pre class="code ipython"><a name="rest_code_8b4d7bd0144f4bc1acccce208f3db5ec-1"></a><span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">one_good_arm_rates</span><span class="p">)</span>
<a name="rest_code_8b4d7bd0144f4bc1acccce208f3db5ec-2"></a><span class="n">optimistic_agent</span> <span class="o">=</span> <span class="n">OptimisticInitialValues</span><span class="p">(</span><span class="n">one_good_arm_rates</span><span class="p">,</span> <span class="mf">10.</span><span class="p">)</span>
<a name="rest_code_8b4d7bd0144f4bc1acccce208f3db5ec-3"></a><span class="n">data</span> <span class="o">=</span> <span class="p">{}</span>
<a name="rest_code_8b4d7bd0144f4bc1acccce208f3db5ec-4"></a><span class="n">data</span><span class="p">[</span><span class="s2">&quot;Optimistic Initial Values&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">cumulative_reward</span><span class="p">(</span><span class="n">optimistic_agent</span><span class="p">,</span> <span class="n">trials</span><span class="o">=</span><span class="n">TRIALS</span><span class="p">,</span> <span class="n">times</span><span class="o">=</span><span class="n">TIMES</span><span class="p">)</span>
<a name="rest_code_8b4d7bd0144f4bc1acccce208f3db5ec-5"></a><span class="n">epsilon_agent</span> <span class="o">=</span> <span class="n">EpsilonGreedyNormal</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">one_good_arm_rates</span><span class="p">)</span>
<a name="rest_code_8b4d7bd0144f4bc1acccce208f3db5ec-6"></a><span class="n">data</span><span class="p">[</span><span class="s2">&quot;Epsilon Greedy (0.1)&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">cumulative_reward</span><span class="p">(</span><span class="n">epsilon_agent</span><span class="p">,</span> <span class="n">trials</span><span class="o">=</span><span class="n">TRIALS</span><span class="p">,</span> <span class="n">times</span><span class="o">=</span><span class="n">TIMES</span><span class="p">)</span>
<a name="rest_code_8b4d7bd0144f4bc1acccce208f3db5ec-7"></a><span class="n">data</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<a name="rest_code_8b4d7bd0144f4bc1acccce208f3db5ec-8"></a><span class="n">plot_cumulative</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre><img alt="optimistic_cumulative_one_good_arm.png" src="optimistic_cumulative_one_good_arm.png" />
<p>It looks like the optimistic agent does even better with one dominant arm. Likely because it found it quick enough that always exploiting it gives it a huge advantage over the epsilon greedy, which never stops exploring.</p>
<pre class="code ipython"><a name="rest_code_53fed818f1a144d7b531ba903cbe6919-1"></a><span class="nd">@jit</span>
<a name="rest_code_53fed818f1a144d7b531ba903cbe6919-2"></a><span class="k">def</span> <span class="nf">average_reward</span><span class="p">(</span><span class="n">agent</span><span class="p">,</span> <span class="n">times</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">trials</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
<a name="rest_code_53fed818f1a144d7b531ba903cbe6919-3"></a>    <span class="sd">&quot;&quot;&quot;this generates the average reward for the trials over time</span>
<a name="rest_code_53fed818f1a144d7b531ba903cbe6919-4"></a>
<a name="rest_code_53fed818f1a144d7b531ba903cbe6919-5"></a><span class="sd">    Args:</span>
<a name="rest_code_53fed818f1a144d7b531ba903cbe6919-6"></a><span class="sd">     trials (int): number of times to train the agent</span>
<a name="rest_code_53fed818f1a144d7b531ba903cbe6919-7"></a><span class="sd">     times (int): length of time to train the agent</span>
<a name="rest_code_53fed818f1a144d7b531ba903cbe6919-8"></a><span class="sd">    Returns:</span>
<a name="rest_code_53fed818f1a144d7b531ba903cbe6919-9"></a><span class="sd">     numpy.array: the average reward</span>
<a name="rest_code_53fed818f1a144d7b531ba903cbe6919-10"></a><span class="sd">    &quot;&quot;&quot;</span>
<a name="rest_code_53fed818f1a144d7b531ba903cbe6919-11"></a>    <span class="n">average_rewards</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">times</span><span class="p">)</span>
<a name="rest_code_53fed818f1a144d7b531ba903cbe6919-12"></a>    <span class="k">for</span> <span class="n">trial</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">trials</span><span class="p">):</span>
<a name="rest_code_53fed818f1a144d7b531ba903cbe6919-13"></a>        <span class="k">for</span> <span class="n">time</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">times</span><span class="p">):</span>
<a name="rest_code_53fed818f1a144d7b531ba903cbe6919-14"></a>            <span class="n">arm</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">select_arm</span><span class="p">()</span>
<a name="rest_code_53fed818f1a144d7b531ba903cbe6919-15"></a>            <span class="n">old_reward</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">total_reward</span>
<a name="rest_code_53fed818f1a144d7b531ba903cbe6919-16"></a>            <span class="n">agent</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">arm</span><span class="p">)</span>
<a name="rest_code_53fed818f1a144d7b531ba903cbe6919-17"></a>            <span class="n">average_rewards</span><span class="p">[</span><span class="n">time</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">total_reward</span> <span class="o">-</span> <span class="n">old_reward</span><span class="p">)</span>
<a name="rest_code_53fed818f1a144d7b531ba903cbe6919-18"></a>        <span class="n">agent</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<a name="rest_code_53fed818f1a144d7b531ba903cbe6919-19"></a>    <span class="k">return</span> <span class="n">average_rewards</span><span class="o">/</span><span class="n">trials</span>
</pre><pre class="code ipython"><a name="rest_code_c8bb27b74c8d4d4287a87eaafb0a9b3c-1"></a><span class="n">data</span> <span class="o">=</span> <span class="p">{}</span>
<a name="rest_code_c8bb27b74c8d4d4287a87eaafb0a9b3c-2"></a><span class="n">data</span><span class="p">[</span><span class="s2">&quot;Optimistic Initial Values&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">average_reward</span><span class="p">(</span><span class="n">optimistic_agent</span><span class="p">,</span> <span class="n">TIMES</span><span class="p">,</span> <span class="n">TRIALS</span><span class="p">)</span>
<a name="rest_code_c8bb27b74c8d4d4287a87eaafb0a9b3c-3"></a><span class="n">data</span><span class="p">[</span><span class="s2">&quot;Epsilon 0.1&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">average_reward</span><span class="p">(</span><span class="n">epsilon_agent</span><span class="p">,</span> <span class="n">TIMES</span><span class="p">,</span> <span class="n">TRIALS</span><span class="p">)</span>
</pre><pre class="code ipython"><a name="rest_code_b0c22a8d51474ca093cce5a1f4950a0a-1"></a><span class="n">averages</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre><pre class="code ipython"><a name="rest_code_0a5f94b973dc46fa9df2b1cefa30f57f-1"></a><span class="n">figure</span> <span class="o">=</span> <span class="n">plot</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<a name="rest_code_0a5f94b973dc46fa9df2b1cefa30f57f-2"></a><span class="n">axe</span> <span class="o">=</span> <span class="n">figure</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
<a name="rest_code_0a5f94b973dc46fa9df2b1cefa30f57f-3"></a><span class="n">axe</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Average Reward (One Dominant Arm)&quot;</span><span class="p">)</span>
<a name="rest_code_0a5f94b973dc46fa9df2b1cefa30f57f-4"></a><span class="n">axe</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Time (number of pulls on the arm)&quot;</span><span class="p">)</span>
<a name="rest_code_0a5f94b973dc46fa9df2b1cefa30f57f-5"></a><span class="n">axe</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Average Reward&quot;</span><span class="p">)</span>
<a name="rest_code_0a5f94b973dc46fa9df2b1cefa30f57f-6"></a><span class="n">averages</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axe</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;None&quot;</span><span class="p">)</span>
</pre><img alt="optimistic_averages.png" src="optimistic_averages.png" />
<p>It looks like there was a brief period where the Epsilon Greedy did better, but the Optimistic agent settled in fairly quickly.</p>
</div>
</div>
