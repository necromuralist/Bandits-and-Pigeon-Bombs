<div class="section" id="introduction">
<h1>1 Introduction</h1>
<p>This is one possible to the n-armed bandit problem. It is similar to the <em>Epsilon Greedy</em> algorithm except that instead of using a conditional to decide whether to explore or exploit, the algorithm sets the estimated (mean) payout for each arm to 1 (the theoretical maximum for our case) and then always exploits. As things proceed, the arms will settle down to their actual payoff-rates and those that haven't been explored will be chosen because they are still too high.</p>
</div>
<div class="section" id="the-tangle">
<h1>2 The Tangle</h1>
<p>This is the no-web template to build the final file.</p>
<pre class="code python"><a name="rest_code_0a5c592cc0f146989fccf29434c94b40-1"></a><span class="o">&lt;&lt;</span><span class="n">imports</span><span class="o">&gt;&gt;</span>
<a name="rest_code_0a5c592cc0f146989fccf29434c94b40-2"></a>
<a name="rest_code_0a5c592cc0f146989fccf29434c94b40-3"></a><span class="o">&lt;&lt;</span><span class="n">spec</span><span class="o">&gt;&gt;</span>
<a name="rest_code_0a5c592cc0f146989fccf29434c94b40-4"></a>
<a name="rest_code_0a5c592cc0f146989fccf29434c94b40-5"></a><span class="o">&lt;&lt;</span><span class="n">class</span><span class="o">-</span><span class="n">declaration</span><span class="o">&gt;&gt;</span>
<a name="rest_code_0a5c592cc0f146989fccf29434c94b40-6"></a>
<a name="rest_code_0a5c592cc0f146989fccf29434c94b40-7"></a>    <span class="o">&lt;&lt;</span><span class="n">constructor</span><span class="o">&gt;&gt;</span>
<a name="rest_code_0a5c592cc0f146989fccf29434c94b40-8"></a>
<a name="rest_code_0a5c592cc0f146989fccf29434c94b40-9"></a>    <span class="o">&lt;&lt;</span><span class="n">select</span><span class="o">-</span><span class="n">arm</span><span class="o">&gt;&gt;</span>
<a name="rest_code_0a5c592cc0f146989fccf29434c94b40-10"></a>
<a name="rest_code_0a5c592cc0f146989fccf29434c94b40-11"></a>    <span class="o">&lt;&lt;</span><span class="n">pull</span><span class="o">-</span><span class="n">arm</span><span class="o">&gt;&gt;</span>
<a name="rest_code_0a5c592cc0f146989fccf29434c94b40-12"></a>
<a name="rest_code_0a5c592cc0f146989fccf29434c94b40-13"></a>    <span class="o">&lt;&lt;</span><span class="n">update</span><span class="o">-</span><span class="n">arm</span><span class="o">&gt;&gt;</span>
<a name="rest_code_0a5c592cc0f146989fccf29434c94b40-14"></a>
<a name="rest_code_0a5c592cc0f146989fccf29434c94b40-15"></a>    <span class="o">&lt;&lt;</span><span class="n">reset</span><span class="o">&gt;&gt;</span>
</pre></div>
<div class="section" id="imports">
<h1>3 Imports</h1>
<p>These are our external dependencies.</p>
<pre class="code python"><a name="rest_code_f5591b28c7a24b78bc9cb6ecffb32b52-1"></a><span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">jitclass</span>
<a name="rest_code_f5591b28c7a24b78bc9cb6ecffb32b52-2"></a><span class="kn">import</span> <span class="nn">numba</span>
<a name="rest_code_f5591b28c7a24b78bc9cb6ecffb32b52-3"></a><span class="kn">import</span> <span class="nn">numpy</span>
</pre></div>
<div class="section" id="the-spec">
<h1>4 The Spec</h1>
<p>In order to use numba with the <tt class="docutils literal">OptimisticInitialValues</tt> class you have to create a 'spec' that tells numba what the data-types are for each of its fields.</p>
<pre class="code python"><a name="rest_code_cc215b8b3c754a6a9e5251f8f096849f-1"></a><span class="n">SPEC</span> <span class="o">=</span> <span class="p">[</span>
<a name="rest_code_cc215b8b3c754a6a9e5251f8f096849f-2"></a>    <span class="p">(</span><span class="s2">&quot;arms&quot;</span><span class="p">,</span> <span class="n">numba</span><span class="o">.</span><span class="n">double</span><span class="p">[:]),</span>
<a name="rest_code_cc215b8b3c754a6a9e5251f8f096849f-3"></a>    <span class="p">(</span><span class="s2">&quot;counts&quot;</span><span class="p">,</span> <span class="n">numba</span><span class="o">.</span><span class="n">double</span><span class="p">[:]),</span>
<a name="rest_code_cc215b8b3c754a6a9e5251f8f096849f-4"></a>    <span class="p">(</span><span class="s2">&quot;rewards&quot;</span><span class="p">,</span> <span class="n">numba</span><span class="o">.</span><span class="n">double</span><span class="p">[:]),</span>
<a name="rest_code_cc215b8b3c754a6a9e5251f8f096849f-5"></a>    <span class="p">(</span><span class="s2">&quot;total_reward&quot;</span><span class="p">,</span> <span class="n">numba</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
<a name="rest_code_cc215b8b3c754a6a9e5251f8f096849f-6"></a>    <span class="p">(</span><span class="s2">&quot;initial_reward&quot;</span><span class="p">,</span> <span class="n">numba</span><span class="o">.</span><span class="n">double</span><span class="p">),</span>
<a name="rest_code_cc215b8b3c754a6a9e5251f8f096849f-7"></a><span class="p">]</span>
</pre></div>
<div class="section" id="the-class-declaration">
<h1>5 The Class Declaration</h1>
<pre class="code python"><a name="rest_code_62b8932bad814b5fb42d50916fc9cac0-1"></a><span class="nd">@jitclass</span><span class="p">(</span><span class="n">SPEC</span><span class="p">)</span>
<a name="rest_code_62b8932bad814b5fb42d50916fc9cac0-2"></a><span class="k">class</span> <span class="nc">OptimisticInitialValues</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
<a name="rest_code_62b8932bad814b5fb42d50916fc9cac0-3"></a>    <span class="sd">&quot;&quot;&quot;Optimistic Initial Values greedy algorithm</span>
<a name="rest_code_62b8932bad814b5fb42d50916fc9cac0-4"></a>
<a name="rest_code_62b8932bad814b5fb42d50916fc9cac0-5"></a><span class="sd">    Args:</span>
<a name="rest_code_62b8932bad814b5fb42d50916fc9cac0-6"></a><span class="sd">     numpy.array[float]: payout-probabilities for each arm</span>
<a name="rest_code_62b8932bad814b5fb42d50916fc9cac0-7"></a><span class="sd">    &quot;&quot;&quot;</span>
</pre></div>
<div class="section" id="the-constructor">
<h1>6 The Constructor</h1>
<p>Here's our first change from the epsilon-greedy algorithm. We no longer have an <tt class="docutils literal">epsilon</tt> value and instead of initializing the <tt class="docutils literal">rewards</tt> as zeros we initialize them with an 'initial' reward. Also, although you can't see it here, the arms have to be a list of mean payout values (see the <tt class="docutils literal">pull_arm</tt> method below).</p>
<pre class="code python"><a name="rest_code_14e45d4a2ef049dcab77ef6ebd03eeab-1"></a><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arms</span><span class="p">,</span> <span class="n">initial_reward</span><span class="p">):</span>
<a name="rest_code_14e45d4a2ef049dcab77ef6ebd03eeab-2"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">arms</span> <span class="o">=</span> <span class="n">arms</span>
<a name="rest_code_14e45d4a2ef049dcab77ef6ebd03eeab-3"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">counts</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">arms</span><span class="p">))</span>
<a name="rest_code_14e45d4a2ef049dcab77ef6ebd03eeab-4"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">arms</span><span class="p">))</span> <span class="o">+</span> <span class="n">initial_reward</span>
<a name="rest_code_14e45d4a2ef049dcab77ef6ebd03eeab-5"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>
<a name="rest_code_14e45d4a2ef049dcab77ef6ebd03eeab-6"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">initial_reward</span> <span class="o">=</span> <span class="n">initial_reward</span>
<a name="rest_code_14e45d4a2ef049dcab77ef6ebd03eeab-7"></a>    <span class="k">return</span>
</pre></div>
<div class="section" id="select-arm">
<h1>7 Select Arm</h1>
<p>This chooses the next arm. Unlike the epsilon-greedy algorithm it will always pick the 'best' arm, choosing the first if there is a tie. Since the whole class is in the jit I'm also not using the external <tt class="docutils literal">find_first</tt> method.</p>
<pre class="code python"><a name="rest_code_39ef147cbf0549cba23bda355dc98556-1"></a><span class="k">def</span> <span class="nf">select_arm</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<a name="rest_code_39ef147cbf0549cba23bda355dc98556-2"></a>    <span class="sd">&quot;&quot;&quot;Index of the arm with the most reward</span>
<a name="rest_code_39ef147cbf0549cba23bda355dc98556-3"></a>
<a name="rest_code_39ef147cbf0549cba23bda355dc98556-4"></a><span class="sd">    Returns:</span>
<a name="rest_code_39ef147cbf0549cba23bda355dc98556-5"></a><span class="sd">     integer: index of arm with highest average reward</span>
<a name="rest_code_39ef147cbf0549cba23bda355dc98556-6"></a><span class="sd">    &quot;&quot;&quot;</span>
<a name="rest_code_39ef147cbf0549cba23bda355dc98556-7"></a>    <span class="n">item</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<a name="rest_code_39ef147cbf0549cba23bda355dc98556-8"></a>    <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">)):</span>
<a name="rest_code_39ef147cbf0549cba23bda355dc98556-9"></a>        <span class="k">if</span> <span class="n">item</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">[</span><span class="n">index</span><span class="p">]:</span>
<a name="rest_code_39ef147cbf0549cba23bda355dc98556-10"></a>            <span class="k">return</span> <span class="n">index</span>
</pre></div>
<div class="section" id="pull-arm">
<h1>8 Pull Arm</h1>
<p>This gets the reward for the arm. with a Bernoulli arm, there's a chance that an arm will be set to 0 on its first pull, at which point you will never explore it (since there's no exploration), so even the best arm might get wiped out. To fix this you need a different scheme. This one uses a population mean (selected <tt class="docutils literal">from self.arms</tt>) which has noise added by selecting from the standard normal distribution.</p>
<pre class="code python"><a name="rest_code_a569b19151fe4458a844f91ec8efa77c-1"></a><span class="k">def</span> <span class="nf">pull_arm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arm</span><span class="p">):</span>
<a name="rest_code_a569b19151fe4458a844f91ec8efa77c-2"></a>    <span class="sd">&quot;&quot;&quot;gets the reward</span>
<a name="rest_code_a569b19151fe4458a844f91ec8efa77c-3"></a>
<a name="rest_code_a569b19151fe4458a844f91ec8efa77c-4"></a><span class="sd">    Args:</span>
<a name="rest_code_a569b19151fe4458a844f91ec8efa77c-5"></a><span class="sd">     arm (int): index for the arm population-mean array</span>
<a name="rest_code_a569b19151fe4458a844f91ec8efa77c-6"></a><span class="sd">    Returns:</span>
<a name="rest_code_a569b19151fe4458a844f91ec8efa77c-7"></a><span class="sd">     float: payout for the arm</span>
<a name="rest_code_a569b19151fe4458a844f91ec8efa77c-8"></a><span class="sd">    &quot;&quot;&quot;</span>
<a name="rest_code_a569b19151fe4458a844f91ec8efa77c-9"></a>    <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">arms</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
</pre></div>
<div class="section" id="update-arm">
<h1>9 Update Arm</h1>
<p>This pulls the arm and updates the reward. This works the same as the <tt class="docutils literal"><span class="pre">epsilon-greedy</span></tt> version does.</p>
<pre class="code python"><a name="rest_code_acfef257ad854c32ac7657132f6e0b62-1"></a><span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arm</span><span class="p">):</span>
<a name="rest_code_acfef257ad854c32ac7657132f6e0b62-2"></a>    <span class="sd">&quot;&quot;&quot;pulls the arm and updates the average reward</span>
<a name="rest_code_acfef257ad854c32ac7657132f6e0b62-3"></a>
<a name="rest_code_acfef257ad854c32ac7657132f6e0b62-4"></a><span class="sd">    also updates the total_reward the algorithm has earned so far</span>
<a name="rest_code_acfef257ad854c32ac7657132f6e0b62-5"></a>
<a name="rest_code_acfef257ad854c32ac7657132f6e0b62-6"></a><span class="sd">    Args:</span>
<a name="rest_code_acfef257ad854c32ac7657132f6e0b62-7"></a><span class="sd">     arm (int): index of the arm to pull</span>
<a name="rest_code_acfef257ad854c32ac7657132f6e0b62-8"></a><span class="sd">    &quot;&quot;&quot;</span>
<a name="rest_code_acfef257ad854c32ac7657132f6e0b62-9"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">counts</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
<a name="rest_code_acfef257ad854c32ac7657132f6e0b62-10"></a>    <span class="n">count</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">counts</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
<a name="rest_code_acfef257ad854c32ac7657132f6e0b62-11"></a>    <span class="n">average_reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
<a name="rest_code_acfef257ad854c32ac7657132f6e0b62-12"></a>    <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pull_arm</span><span class="p">(</span><span class="n">arm</span><span class="p">)</span>
<a name="rest_code_acfef257ad854c32ac7657132f6e0b62-13"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">total_reward</span> <span class="o">+=</span> <span class="n">reward</span>
<a name="rest_code_acfef257ad854c32ac7657132f6e0b62-14"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">=</span> <span class="p">(((</span><span class="n">count</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">count</span><span class="p">))</span> <span class="o">*</span> <span class="n">average_reward</span>
<a name="rest_code_acfef257ad854c32ac7657132f6e0b62-15"></a>                         <span class="o">+</span> <span class="p">(</span><span class="n">reward</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">count</span><span class="p">)))</span>
<a name="rest_code_acfef257ad854c32ac7657132f6e0b62-16"></a>    <span class="k">return</span>
</pre></div>
<div class="section" id="reset">
<h1>10 Reset</h1>
<p>This resets the values so that you can re-use the algorithm. As with the constructor, it sets the <tt class="docutils literal">rewards</tt> to all ones instead of zeros as was the case with the epsilon-greedy algorithm.</p>
<pre class="code python"><a name="rest_code_88ce437926164e28b1581cf36da7f9ce-1"></a><span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<a name="rest_code_88ce437926164e28b1581cf36da7f9ce-2"></a>    <span class="sd">&quot;&quot;&quot;sets the counts, rewards, total_reward to 0s</span>
<a name="rest_code_88ce437926164e28b1581cf36da7f9ce-3"></a>
<a name="rest_code_88ce437926164e28b1581cf36da7f9ce-4"></a><span class="sd">    This lets you re-used the EpsilonGreedy</span>
<a name="rest_code_88ce437926164e28b1581cf36da7f9ce-5"></a><span class="sd">    &quot;&quot;&quot;</span>
<a name="rest_code_88ce437926164e28b1581cf36da7f9ce-6"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">counts</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">arms</span><span class="p">))</span>
<a name="rest_code_88ce437926164e28b1581cf36da7f9ce-7"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">arms</span><span class="p">))</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">initial_reward</span>
<a name="rest_code_88ce437926164e28b1581cf36da7f9ce-8"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>
<a name="rest_code_88ce437926164e28b1581cf36da7f9ce-9"></a>    <span class="k">return</span>
</pre></div>
