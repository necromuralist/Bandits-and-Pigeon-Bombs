<div class="section" id="introduction">
<h1>1 Introduction</h1>
<p>This is one possible to the n-armed bandit problem. It is similar to the <em>Epsilon Greedy</em> algorithm except that instead of using a conditional to decide whether to explore or exploit, the algorithm sets the estimated (mean) payout for each arm to 1 (the theoretical maximum for our case) and then always exploits. As things proceed, the arms will settle down to their actual payoff-rates and those that haven't been explored will be chosen because they are still too high.</p>
</div>
<div class="section" id="the-tangle">
<h1>2 The Tangle</h1>
<p>This is the no-web template to build the final file.</p>
<pre class="code python"><a name="rest_code_c7c853d213064849942df139e2e3554b-1"></a><span class="o">&lt;&lt;</span><span class="n">imports</span><span class="o">&gt;&gt;</span>
<a name="rest_code_c7c853d213064849942df139e2e3554b-2"></a>
<a name="rest_code_c7c853d213064849942df139e2e3554b-3"></a><span class="o">&lt;&lt;</span><span class="n">spec</span><span class="o">&gt;&gt;</span>
<a name="rest_code_c7c853d213064849942df139e2e3554b-4"></a>
<a name="rest_code_c7c853d213064849942df139e2e3554b-5"></a><span class="o">&lt;&lt;</span><span class="n">class</span><span class="o">-</span><span class="n">declaration</span><span class="o">&gt;&gt;</span>
<a name="rest_code_c7c853d213064849942df139e2e3554b-6"></a>
<a name="rest_code_c7c853d213064849942df139e2e3554b-7"></a>    <span class="o">&lt;&lt;</span><span class="n">constructor</span><span class="o">&gt;&gt;</span>
<a name="rest_code_c7c853d213064849942df139e2e3554b-8"></a>
<a name="rest_code_c7c853d213064849942df139e2e3554b-9"></a>    <span class="o">&lt;&lt;</span><span class="n">select</span><span class="o">-</span><span class="n">arm</span><span class="o">&gt;&gt;</span>
<a name="rest_code_c7c853d213064849942df139e2e3554b-10"></a>
<a name="rest_code_c7c853d213064849942df139e2e3554b-11"></a>    <span class="o">&lt;&lt;</span><span class="n">pull</span><span class="o">-</span><span class="n">arm</span><span class="o">&gt;&gt;</span>
<a name="rest_code_c7c853d213064849942df139e2e3554b-12"></a>
<a name="rest_code_c7c853d213064849942df139e2e3554b-13"></a>    <span class="o">&lt;&lt;</span><span class="n">update</span><span class="o">-</span><span class="n">arm</span><span class="o">&gt;&gt;</span>
<a name="rest_code_c7c853d213064849942df139e2e3554b-14"></a>
<a name="rest_code_c7c853d213064849942df139e2e3554b-15"></a>    <span class="o">&lt;&lt;</span><span class="n">reset</span><span class="o">&gt;&gt;</span>
</pre></div>
<div class="section" id="imports">
<h1>3 Imports</h1>
<p>These are our external dependencies.</p>
<pre class="code python"><a name="rest_code_2b3755e06b424022b25a9bc30a081d19-1"></a><span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">jitclass</span>
<a name="rest_code_2b3755e06b424022b25a9bc30a081d19-2"></a><span class="kn">import</span> <span class="nn">numba</span>
<a name="rest_code_2b3755e06b424022b25a9bc30a081d19-3"></a><span class="kn">import</span> <span class="nn">numpy</span>
</pre></div>
<div class="section" id="the-spec">
<h1>4 The Spec</h1>
<p>In order to use numba with the <tt class="docutils literal">OptimisticInitialValues</tt> class you have to create a 'spec' that tells numba what the data-types are for each of its fields.</p>
<pre class="code python"><a name="rest_code_6a21c06b8a44478a8d223e43853a8b7a-1"></a><span class="n">SPEC</span> <span class="o">=</span> <span class="p">[</span>
<a name="rest_code_6a21c06b8a44478a8d223e43853a8b7a-2"></a>    <span class="p">(</span><span class="s2">&quot;arms&quot;</span><span class="p">,</span> <span class="n">numba</span><span class="o">.</span><span class="n">double</span><span class="p">[:]),</span>
<a name="rest_code_6a21c06b8a44478a8d223e43853a8b7a-3"></a>    <span class="p">(</span><span class="s2">&quot;counts&quot;</span><span class="p">,</span> <span class="n">numba</span><span class="o">.</span><span class="n">double</span><span class="p">[:]),</span>
<a name="rest_code_6a21c06b8a44478a8d223e43853a8b7a-4"></a>    <span class="p">(</span><span class="s2">&quot;rewards&quot;</span><span class="p">,</span> <span class="n">numba</span><span class="o">.</span><span class="n">double</span><span class="p">[:]),</span>
<a name="rest_code_6a21c06b8a44478a8d223e43853a8b7a-5"></a>    <span class="p">(</span><span class="s2">&quot;total_reward&quot;</span><span class="p">,</span> <span class="n">numba</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
<a name="rest_code_6a21c06b8a44478a8d223e43853a8b7a-6"></a>    <span class="p">(</span><span class="s2">&quot;initial_reward&quot;</span><span class="p">,</span> <span class="n">numba</span><span class="o">.</span><span class="n">double</span><span class="p">),</span>
<a name="rest_code_6a21c06b8a44478a8d223e43853a8b7a-7"></a><span class="p">]</span>
</pre></div>
<div class="section" id="the-class-declaration">
<h1>5 The Class Declaration</h1>
<pre class="code python"><a name="rest_code_bb0024d60e864bf0a8e85c39d35047b9-1"></a><span class="nd">@jitclass</span><span class="p">(</span><span class="n">SPEC</span><span class="p">)</span>
<a name="rest_code_bb0024d60e864bf0a8e85c39d35047b9-2"></a><span class="k">class</span> <span class="nc">OptimisticInitialValues</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
<a name="rest_code_bb0024d60e864bf0a8e85c39d35047b9-3"></a>    <span class="sd">&quot;&quot;&quot;Optimistic Initial Values greedy algorithm</span>
<a name="rest_code_bb0024d60e864bf0a8e85c39d35047b9-4"></a>
<a name="rest_code_bb0024d60e864bf0a8e85c39d35047b9-5"></a><span class="sd">    Args:</span>
<a name="rest_code_bb0024d60e864bf0a8e85c39d35047b9-6"></a><span class="sd">     numpy.array[float]: payout-probabilities for each arm</span>
<a name="rest_code_bb0024d60e864bf0a8e85c39d35047b9-7"></a><span class="sd">    &quot;&quot;&quot;</span>
</pre></div>
<div class="section" id="the-constructor">
<h1>6 The Constructor</h1>
<p>Here's our first change from the epsilon-greedy algorithm. We no longer have an <tt class="docutils literal">epsilon</tt> value and instead of initializing the <tt class="docutils literal">rewards</tt> as zeros we initialize them with an 'initial' reward. Also, although you can't see it here, the arms have to be a list of mean payout values (see the <tt class="docutils literal">pull_arm</tt> method below).</p>
<pre class="code python"><a name="rest_code_12c23a2804f14b98b441ff8343c2f907-1"></a><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arms</span><span class="p">,</span> <span class="n">initial_reward</span><span class="p">):</span>
<a name="rest_code_12c23a2804f14b98b441ff8343c2f907-2"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">arms</span> <span class="o">=</span> <span class="n">arms</span>
<a name="rest_code_12c23a2804f14b98b441ff8343c2f907-3"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">counts</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">arms</span><span class="p">))</span>
<a name="rest_code_12c23a2804f14b98b441ff8343c2f907-4"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">arms</span><span class="p">))</span> <span class="o">+</span> <span class="n">initial_reward</span>
<a name="rest_code_12c23a2804f14b98b441ff8343c2f907-5"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>
<a name="rest_code_12c23a2804f14b98b441ff8343c2f907-6"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">initial_reward</span> <span class="o">=</span> <span class="n">initial_reward</span>
<a name="rest_code_12c23a2804f14b98b441ff8343c2f907-7"></a>    <span class="k">return</span>
</pre></div>
<div class="section" id="select-arm">
<h1>7 Select Arm</h1>
<p>This chooses the next arm. Unlike the epsilon-greedy algorithm it will always pick the 'best' arm, choosing the first if there is a tie. Since the whole class is in the jit I'm also not using the external <tt class="docutils literal">find_first</tt> method.</p>
<pre class="code python"><a name="rest_code_1a4f816143274269b184aece12f928f8-1"></a><span class="k">def</span> <span class="nf">select_arm</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<a name="rest_code_1a4f816143274269b184aece12f928f8-2"></a>    <span class="sd">&quot;&quot;&quot;Index of the arm with the most reward</span>
<a name="rest_code_1a4f816143274269b184aece12f928f8-3"></a>
<a name="rest_code_1a4f816143274269b184aece12f928f8-4"></a><span class="sd">    Returns:</span>
<a name="rest_code_1a4f816143274269b184aece12f928f8-5"></a><span class="sd">     integer: index of arm with highest average reward</span>
<a name="rest_code_1a4f816143274269b184aece12f928f8-6"></a><span class="sd">    &quot;&quot;&quot;</span>
<a name="rest_code_1a4f816143274269b184aece12f928f8-7"></a>    <span class="n">item</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<a name="rest_code_1a4f816143274269b184aece12f928f8-8"></a>    <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">)):</span>
<a name="rest_code_1a4f816143274269b184aece12f928f8-9"></a>        <span class="k">if</span> <span class="n">item</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">[</span><span class="n">index</span><span class="p">]:</span>
<a name="rest_code_1a4f816143274269b184aece12f928f8-10"></a>            <span class="k">return</span> <span class="n">index</span>
</pre></div>
<div class="section" id="pull-arm">
<h1>8 Pull Arm</h1>
<p>This gets the reward for the arm. with a Bernoulli arm, there's a chance that an arm will be set to 0 on its first pull, at which point you will never explore it (since there's no exploration), so even the best arm might get wiped out. To fix this you need a different scheme. This one uses a population mean (selected <tt class="docutils literal">from self.arms</tt>) which has noise added by selecting from the standard normal distribution.</p>
<pre class="code python"><a name="rest_code_1b2fba4c7a2b455a9444ebfb1a418e2c-1"></a><span class="k">def</span> <span class="nf">pull_arm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arm</span><span class="p">):</span>
<a name="rest_code_1b2fba4c7a2b455a9444ebfb1a418e2c-2"></a>    <span class="sd">&quot;&quot;&quot;gets the reward</span>
<a name="rest_code_1b2fba4c7a2b455a9444ebfb1a418e2c-3"></a>
<a name="rest_code_1b2fba4c7a2b455a9444ebfb1a418e2c-4"></a><span class="sd">    Args:</span>
<a name="rest_code_1b2fba4c7a2b455a9444ebfb1a418e2c-5"></a><span class="sd">     arm (int): index for the arm population-mean array</span>
<a name="rest_code_1b2fba4c7a2b455a9444ebfb1a418e2c-6"></a><span class="sd">    Returns:</span>
<a name="rest_code_1b2fba4c7a2b455a9444ebfb1a418e2c-7"></a><span class="sd">     float: payout for the arm</span>
<a name="rest_code_1b2fba4c7a2b455a9444ebfb1a418e2c-8"></a><span class="sd">    &quot;&quot;&quot;</span>
<a name="rest_code_1b2fba4c7a2b455a9444ebfb1a418e2c-9"></a>    <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">arms</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
</pre></div>
<div class="section" id="update-arm">
<h1>9 Update Arm</h1>
<p>This pulls the arm and updates the reward. This works the same as the <tt class="docutils literal"><span class="pre">epsilon-greedy</span></tt> version does.</p>
<pre class="code python"><a name="rest_code_621f7110a2fe425fbbf49200e34e2bda-1"></a><span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arm</span><span class="p">):</span>
<a name="rest_code_621f7110a2fe425fbbf49200e34e2bda-2"></a>    <span class="sd">&quot;&quot;&quot;pulls the arm and updates the average reward</span>
<a name="rest_code_621f7110a2fe425fbbf49200e34e2bda-3"></a>
<a name="rest_code_621f7110a2fe425fbbf49200e34e2bda-4"></a><span class="sd">    also updates the total_reward the algorithm has earned so far</span>
<a name="rest_code_621f7110a2fe425fbbf49200e34e2bda-5"></a>
<a name="rest_code_621f7110a2fe425fbbf49200e34e2bda-6"></a><span class="sd">    Args:</span>
<a name="rest_code_621f7110a2fe425fbbf49200e34e2bda-7"></a><span class="sd">     arm (int): index of the arm to pull</span>
<a name="rest_code_621f7110a2fe425fbbf49200e34e2bda-8"></a><span class="sd">    &quot;&quot;&quot;</span>
<a name="rest_code_621f7110a2fe425fbbf49200e34e2bda-9"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">counts</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
<a name="rest_code_621f7110a2fe425fbbf49200e34e2bda-10"></a>    <span class="n">count</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">counts</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
<a name="rest_code_621f7110a2fe425fbbf49200e34e2bda-11"></a>    <span class="n">average_reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
<a name="rest_code_621f7110a2fe425fbbf49200e34e2bda-12"></a>    <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pull_arm</span><span class="p">(</span><span class="n">arm</span><span class="p">)</span>
<a name="rest_code_621f7110a2fe425fbbf49200e34e2bda-13"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">total_reward</span> <span class="o">+=</span> <span class="n">reward</span>
<a name="rest_code_621f7110a2fe425fbbf49200e34e2bda-14"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">=</span> <span class="p">(((</span><span class="n">count</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">count</span><span class="p">))</span> <span class="o">*</span> <span class="n">average_reward</span>
<a name="rest_code_621f7110a2fe425fbbf49200e34e2bda-15"></a>                         <span class="o">+</span> <span class="p">(</span><span class="n">reward</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">count</span><span class="p">)))</span>
<a name="rest_code_621f7110a2fe425fbbf49200e34e2bda-16"></a>    <span class="k">return</span>
</pre></div>
<div class="section" id="reset">
<h1>10 Reset</h1>
<p>This resets the values so that you can re-use the algorithm. As with the constructor, it sets the <tt class="docutils literal">rewards</tt> to all ones instead of zeros as was the case with the epsilon-greedy algorithm.</p>
<pre class="code python"><a name="rest_code_77c68b94ebff4e5bbc1011066d71a02e-1"></a><span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<a name="rest_code_77c68b94ebff4e5bbc1011066d71a02e-2"></a>    <span class="sd">&quot;&quot;&quot;sets the counts, rewards, total_reward to 0s</span>
<a name="rest_code_77c68b94ebff4e5bbc1011066d71a02e-3"></a>
<a name="rest_code_77c68b94ebff4e5bbc1011066d71a02e-4"></a><span class="sd">    This lets you re-used the EpsilonGreedy</span>
<a name="rest_code_77c68b94ebff4e5bbc1011066d71a02e-5"></a><span class="sd">    &quot;&quot;&quot;</span>
<a name="rest_code_77c68b94ebff4e5bbc1011066d71a02e-6"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">counts</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">arms</span><span class="p">))</span>
<a name="rest_code_77c68b94ebff4e5bbc1011066d71a02e-7"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">arms</span><span class="p">))</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">initial_reward</span>
<a name="rest_code_77c68b94ebff4e5bbc1011066d71a02e-8"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>
<a name="rest_code_77c68b94ebff4e5bbc1011066d71a02e-9"></a>    <span class="k">return</span>
</pre></div>
