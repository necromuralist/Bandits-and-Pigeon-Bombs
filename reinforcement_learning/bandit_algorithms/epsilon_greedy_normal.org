#+TITLE: Epsilon Greedy with Normal-Distribution Payouts
* Epsilon Greedy
  Since the Optimistic Initial Values agent can't use the Bernoulli Arm, I'm creating a version of the Epsilon Greedy Optimized that expects the =arms= to be the population-mean for their payouts and the =pull_arm= will return a set of normally-distributed around that mean.

#+BEGIN_SRC python :tangle epsilon_greedy_normal.py
<<optimized-imports>>

<<spec>>

<<find-first>>
@jitclass(spec)
class EpsilonGreedyNormal(object):
    """The Epsilon Greedy Algorithm With Normal Arm

    Args:
     epsilon (float): fraction of the time to explore
     arms (list): collection of probabilities for bandit arm
    """
    <<optimized-constructor>>

    <<best-arm>>

    <<select-arm>>

    <<optimized-pull-arm>>

    <<optimized-update>>

    <<optimized-reset>>

    <<call>>
#+END_SRC

** Optimized Imports
#+BEGIN_SRC python :noweb-ref optimized-imports
# pypi
from numba import (
    jit,
    jitclass,
    )
import numba
import numpy
#+END_SRC
** The Spec
   This is how you tell numba what attributes the class will have.
#+BEGIN_SRC python :noweb-ref spec
spec = [
    ("epsilon", numba.double),
    ("arms", numba.double[:]),
    ("counts", numba.double[:]),
    ("rewards", numba.double[:]),
    ("total_reward", numba.int64),
]
#+END_SRC

** The Constructor
   The constructor takes two arguments - /epsilon/ and /arms/. The /arms/ list should contain the mean payout for each arm.

#+BEGIN_SRC python :noweb-ref optimized-constructor
def __init__(self, epsilon, arms):
    self.epsilon = epsilon
    self.arms = arms
    self.counts = numpy.zeros(len(arms))
    self.rewards = numpy.zeros(len(arms))
    self.total_reward = 0
    return
#+END_SRC

** Reset
#+BEGIN_SRC python :noweb-ref optimized-reset
def reset(self):
    """sets the counts, rewards, total_reward to 0s

    This lets you re-used the EpsilonGreedy
    """
    self.counts = numpy.zeros(len(self.arms))
    self.rewards = numpy.zeros(len(self.arms))
    self.total_reward = 0
    return
#+END_SRC

** Best Arm
   The =best_arm= property returns the index of the arm that has the highest average reward so far. It returns the index instead of the arm itself because it's used to get the matching counts and rewards in the =update= method. Since I'm using the =jitclass= decorator I'm going to get rid of =first_find=.

#+BEGIN_SRC python :noweb-ref best-arm
@property
def best_arm(self):
    """Index of the arm with the most reward"""
    item = self.rewards.max()
    for index in range(len(self.rewards)):
        if item == self.rewards[index]:
            return index
    return
#+END_SRC

** Select Arm
   This differs from the other Epsilon Greedy code only in that I'm using numpy instead of python for the random function.
#+BEGIN_SRC python :noweb-ref select-arm
def select_arm(self):
    """chooses the next arm to update

    Returns:
     int: index of the next arm to pull
    """
    if numpy.random.random() < self.epsilon:
        return numpy.random.randint(len(self.arms))
    return self.best_arm
#+END_SRC

** Pull Arm
   Since we can't give user-defined objects as attributes of the class, this version will be both algorithm and bandit. This is what's different from the other Epsilon Greedy algorithms in that we're returning the arm's mean plus a random number from the normal distribution. If numba allowed us to pass in objects maybe we could have just switched out bandits. I need to look into how to make that work.

#+BEGIN_SRC python :noweb-ref optimized-pull-arm
def pull_arm(self, arm):
    """gets the reward
    
    Args:
     arm (int): index for the arm-probability array
    Returns:
     float: reward
    """
    return numpy.random.randn() + self.arms[arm]
#+END_SRC
   
** Update
   The update method pulls the arm whose index it is given and then updates the count and reward. Here we're calling the =pull_arm= method instead of using a =BernoulliArm= so we can't re-use the original method.

#+BEGIN_SRC python :noweb-ref optimized-update
def update(self, arm):
    """pulls the arm and updates the value

    Args:
     arm (int): index of the arm to pull
    """
    self.counts[arm] += 1
    count = self.counts[arm]
    average_reward = self.rewards[arm]
    reward = self.pull_arm(arm)
    self.total_reward += reward
    self.rewards[arm] = (((count - 1)/float(count)) * average_reward
                        + (reward/float(count)))
    return
#+END_SRC

