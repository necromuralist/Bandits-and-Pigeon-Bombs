#+BEGIN_COMMENT
.. title: One-Dimensional World
.. slug: one-dimensional-world
.. date: 2018-01-19 11:38:59 UTC-08:00
.. tags: qlearning
.. category: QLearning
.. link: 
.. description: A simple Q Learning example using a one-dimensional environment.
.. type: text
#+END_COMMENT

* Background

This is take from MorvanZhou's `github repository <https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow>`__. It uses Q-learning to train an agent to explore a one-dimensional world that has the reward-state at the end of the world.

* Tangle

#+BEGIN_SRC python :tangle one_dimensional_world.py
<<imports>>

<<constants>>

<<Q-Learner>>
    <<episodes>>

    <<terminal>>

    <<q-environment>>

    <<q-agent>>

    <<q-call>>

<<environment>>
    
    <<evaluate-action>>

    <<update>>

    <<goal-reached>>

    <<reset>>

<<agent>>
  
    <<q-table>>

    <<action>>

    <<act>>
<<main>>
#+END_SRC

* Imports
  The only non-python-standard library requirements are numpy and pandas.
#+BEGIN_SRC python :noweb-ref imports
# python standard library
import time

# from pypi
import numpy
import pandas
#+END_SRC

* Constants
  The =Actions= class holds the strings used for the choice of actions. I tend to change my mind about names so I made a central object to store the strings.

#+BEGIN_SRC python :noweb-ref constants
class Actions(object):
    actions = ['Left', 'Right']
    move_left = 'Left'
    move_right = 'Right'
#+END_SRC
* Q-Learner
  This is what will run the reinforcement learning.

#+BEGIN_SRC python :noweb-ref Q-Learner
class QLearner(object):
    """Runs the reinforcement learning

    Args:
     episodes: Number of time to repeat training
     start_state (int): where to start the agent
     states (int): total size for the environment
     terminal (int): where the goal state is
    """
    def __init__(self, episodes=15, start_state=0, states=6, terminal=None):
        self._episodes = None
        self.episodes = episodes
        self.start_state = start_state
        self.states = states
        self._terminal = terminal
        self._environment = None
        self._agent = None
        return
#+END_SRC
** Terminal
   This is the goal state. If it isn't set then it will default to being the rightmost state.

#+BEGIN_SRC python :noweb-ref terminal
@property
def terminal(self):
    """Goal state

    Returns:
     int: zero-based index of the goal-state
    """
    if self._terminal is None:
        self._terminal = self.states - 1
    return self._terminal
#+END_SRC
** Episodes
   An 'episode' is a single session that is run until the agent reaches the goal. To improve its performance, you can run it over multiple episodes so it can keep learning.

#+BEGIN_SRC python :noweb-ref episodes
@property
def episodes(self):
    """the episodes iterator

    Returns:
     range: (1, episodes + 1)
    """
    return self._episodes

@episodes.setter
def episodes(self, episode_count):
    """creates the episodes iterator

    Args:
     episode_count(int): number of episodes to train the agent
    """
    self._episodes = range(1, episode_count + 1)
    return
#+END_SRC

** The Q-Environment
   I couldn't decide who should build the agent and the environment so I had the Q-Learner do it.

#+BEGIN_SRC python :noweb-ref q-environment
@property
def environment(self):
    """The Environment for the agent

    Returns:
     Environment: one-dimensional environment
    """
    if self._environment is None:
        self._environment = Environment(self.start_state,
                                        self.states,
                                        self.terminal)
    return self._environment
#+END_SRC

** The Q-Agent
   Since the Q-Learner is building the environment I'm going to make it build the Agent too.

#+BEGIN_SRC python :noweb-ref q-agent
@property
def agent(self):
    """The agent that explores the environment

    Returns:
     Agent: agent built for the environment
    """
    if self._agent is None:
        self._agent = Agent(self.environment)
    return self._agent
#+END_SRC

** Call
   This runs the episodes.

#+BEGIN_SRC python :noweb-ref q-call
def __call__(self):
    """runs the episodes to train the agent in the environment

    """
    for episode in self.episodes:
        counter = 0
        self.environment.reset()
        self.environment.update(episode, counter)
        while not self.environment.goal_reached:
            self.agent.act()
            counter += 1
            self.environment.update(episode, counter)
    return
#+END_SRC

* The Environment
  This will hold the environment for the agent to explore.

#+BEGIN_SRC python :noweb-ref environment
class Environment(object):
    """The environment to explore

    Args:
     start_state(int): where the agent will start
     states (int): the size of the world
     terminal (int): where the target state is
     output_pause (float): seconds to sleep after printing to the screen
    """
    def __init__(self, start_state, states, terminal, output_pause=2):
        self.start_state = start_state
        self.state = start_state
        self.next_state = start_state
        self.states = states
        self.terminal = terminal
        self.output_pause = output_pause
        return
#+END_SRC

** Goal Reached
   This is used both to update the q-table and to decide whether to quit the episode. Because updating the q-table requires both the current and next states, it is based on the next state, with the assumption that it will be updated before the next episode

#+BEGIN_SRC python :noweb-ref goal-reached
@property
def goal_reached(self):
    """Checks if the next-state is the goal

    Returns:
     bool: True if next-state is the goal
    """
    return self.next_state == self.terminal
#+END_SRC

** Evaluate Action
   This will check if the =action= will lead to the goal and decide the reward to give for the action. It also sets the =next_state= property based on the current =state= and the chosen =action=.

#+BEGIN_SRC python :noweb-ref evaluate-action
def evaluate(self, action):
    """Checks if the action will lead to the goal

    Args:
     action (str): one of the actions to explore the environment

    Returns:
     int: 1 if this will lead to the goal, 0 otherwise
    """
    if action == Actions.move_right:
        self.next_state = self.state + 1
    else:
        self.next_state = max(self.state - 1, 0)
    reward = 1 if self.next_state == self.terminal else 0
    return reward
#+END_SRC

** Update the Environment
   This prints out the environment for the user and updates the state.

#+BEGIN_SRC python :noweb-ref update
def update(self, episode, step):
    """Emits the updated environment to the user
    
    also sets the state to the next state

    Args:
     episode (int): what episode we're in
     step (int): how long we've been running this episode
    """
    environment = ['-'] * (self.states - 1) + ['T']
    if self.goal_reached:
        print("\nEpisode {}: Total Steps = {}".format(episode, step))
        time.sleep(self.output_pause)
    else:
        environment[self.next_state] = 'O'
        print("{}".format("".join(environment)))
    self.state = self.next_state
    return
#+END_SRC

** Reset the environment
   This sets the current =state= and the =next-state= to the =start state= so the environment can be re-used in different episodes.

#+BEGIN_SRC python :noweb-ref reset
def reset(self):
    """Resets the states to the start state"""
    self.state = self.start_state
    self.next_state = self.start_state
    return
#+END_SRC

* The Agent
  This is the agent that will learn to find the reward.

#+BEGIN_SRC python :noweb-ref agent
class Agent(object):
    """This is the agent that will learn to find the treasure

    Args:
     environment: The environment to explore
     exploitation_rate: Fraction of the time to exploit (epsilon)
     discount_factor: Discount factor (gamma)
     learning_rate: how much to change the reward (alpha)
    """
    def __init__(self, environment, exploitation_rate=0.9, discount_factor=0.9,
                 learning_rate=0.1):
        self.environment = environment
        self.exploitation_rate = exploitation_rate
        self.discount_factor = discount_factor
        self.learning_rate = learning_rate
        self._q_table = None
        return
#+END_SRC

Other than the environment, these are the values the =Agent= needs:

 - =exploitation_rate=: What fraction of the time to use the best known action so far. Setting it to 0 means always use a random action, 1 means only use the best known action.
 - =discount_factor=: The amount to discount the previously discovered reward. Setting it to less than 1 prevents infinite loops.
 - =learning_rate=: How much to update the values in the table. 0 means don't update. 1 means use all of the new information.

** The Q-Table
   The agent learns by building a table of 'Quality' estimates for any action chosen for the current state. Each state is a row in the table and each column is a possible action that can be taken. Initially the table is set to all zeros.

#+BEGIN_SRC python :noweb-ref q-table
@property
def q_table(self):
    """The Quality Estimate table

    Each cell is the quality-estimate for a given state, action pair

    Returns:
     DataFrame: rows are states, columns are actions
    """
    if self._q_table is None:
        self._q_table = pandas.DataFrame(
            numpy.zeros((self.environment.states, len(Actions.actions))),
            columns=Actions.actions,
        )
        assert self.q_table.shape == (self.environment.states, len(Actions.actions))
    return self._q_table
    
#+END_SRC

** Action
   Generates an action based on the current state. This is using an epsilon-greedy algorithm so it will explore if the value is above a certain threshold, otherwise it will exploit the best solution so far. It also will explore if the state has never lead to reaching the goal. This was confusing at first, but the updated reward for the current state is calculated based on the next state that this action leads to, so as the episodes continue, the rewards will fill in from the goal-state back to the start state.

#+BEGIN_SRC python :noweb-ref action
@property
def action(self):
    """Return the next chosen action
    
    Returns:
     str: the next action to take
    """
    # get the row in the q-table matching the current state
    state = self.q_table.iloc[self.environment.state, :]

    # only explore if we generate a value over epsilon
    # or none of the actions have a reward
    if numpy.random.uniform() > self.exploitation_rate or state.all() == 0:
        action = numpy.random.choice(Actions.actions)
    else: # exploit
        # get the column-name of the cell with the largest value
        action = state.idxmax()
    return action
#+END_SRC
** Act
   This gets the next action to take, queries the environment for the reward (which also triggers storing the next state in the environment), then updates the q-table.

#+BEGIN_SRC python :noweb-ref act
def act(self):
    """Updates the Q-table based on the reward from the last action"""
    action = self.action
    reward = self.environment.evaluate(action)
    previous_quality = self.q_table.loc[self.environment.state, action]
    if self.environment.goal_reached:
        new_quality = reward
    else:
        new_quality = reward + self.discount_factor * self.q_table.iloc[self.environment.next_state, :].max()
    self.q_table.loc[self.environment.state, action] += self.learning_rate * (new_quality - previous_quality)
    return
#+END_SRC

* Main
  Runs the simulation.

#+BEGIN_SRC python :noweb-ref main
if __name__ == "__main__":
    learner = QLearner(states=10)
    learner()
    print(learner.agent.q_table)
    print("\nlearned-model with only exploitation set")
    learner.agent.exploitation_rate = 1
    learner.episodes = 1
    learner()
#+END_SRC
