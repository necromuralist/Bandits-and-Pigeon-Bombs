#+BEGIN_COMMENT
.. title: Sample Average Vs Alpha
.. slug: sample-average-vs-alpha
.. date: 2021-07-27 20:22:38 UTC-07:00
.. tags: bandits,tabular model,epsilon-greedy
.. category: EpsilonGreedy
.. link: 
.. description: Comparing the sample average and fixed-alpha greedy epsilon algorithms.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 3
#+PROPERTY: header-args :session ~/.local/share/jupyter/runtime/kernel-5791ecf6-dc4e-4d8e-a718-fbd9c79e1547-ssh.json

#+BEGIN_SRC python :results none :exports none
%load_ext autoreload
%autoreload 2
#+END_SRC
* Beginning
** Imports
#+begin_src python :results none
# python
from argparse import Namespace
from functools import partial

# pypi
from numba import jit

import hvplot.pandas
import numpy
import pandas

# this code
from reinforcement_learning.bandit_algorithms.k_armed_bandit import EpsilonExplorer
from reinforcement_learning.bandit_algorithms.k_armed_non_stationary_bandit import MovingBandit, ConstantMemory
from reinforcement_learning.bandit_algorithms.testbed import TestBed

# some other code
from graeae import EmbedHoloviews, Timer
#+end_src
** Setup
#+begin_src python :results none
TIMER = Timer()
#+end_src

#+begin_src python :results none
Parameters = Namespace(
    arms = 10,
    steps = 10000,
    runs = 2000,
    epsilon = 0.1,
    starting_reward=0,
    center = 0,
    sigma = 0.01,
    alpha = 0.1,
)
#+end_src

#+begin_src python :results none
SLUG = "sample-average-vs-alpha"
Embed = partial(EmbedHoloviews, folder_path=f"files/posts/{SLUG}")
Plot = Namespace(
    width=990,
    height=780,
    fontscale=2,
    tan="#ddb377",
    blue="#4687b7",
    red="#ce7b6d",
 )
#+end_src
* Middle
** The Sample Average Explorer
#+begin_src python :results output :exports both
bandit = MovingBandit(k=Parameters.arms,
                      starting_reward=Parameters.starting_reward,
                      center=Parameters.center, sigma=Parameters.sigma)

average_test_bed = TestBed(epsilon=Parameters.epsilon, arms=Parameters.arms,
                           runs=Parameters.runs, steps=Parameters.steps, bandit=bandit)
with TIMER:
    average_test_bed()
#+end_src

#+RESULTS:
#+begin_example
Started: 2021-08-02 21:39:43.573961
(epsilon=0.1) Run 0
(epsilon=0.1) Run 100
(epsilon=0.1) Run 200
(epsilon=0.1) Run 300
(epsilon=0.1) Run 400
(epsilon=0.1) Run 500
(epsilon=0.1) Run 600
(epsilon=0.1) Run 700
(epsilon=0.1) Run 800
(epsilon=0.1) Run 900
(epsilon=0.1) Run 1000
(epsilon=0.1) Run 1100
(epsilon=0.1) Run 1200
(epsilon=0.1) Run 1300
(epsilon=0.1) Run 1400
(epsilon=0.1) Run 1500
(epsilon=0.1) Run 1600
(epsilon=0.1) Run 1700
(epsilon=0.1) Run 1800
(epsilon=0.1) Run 1900
Ended: 2021-08-02 21:51:26.710965
Elapsed: 0:11:43.137004
#+end_example
** The Alpha Explorer
#+begin_src python :results output :exports both
bandit = MovingBandit(k=Parameters.arms,
                      starting_reward=Parameters.starting_reward,
                      center=Parameters.center,
                      sigma=Parameters.sigma)

memory = ConstantMemory(alpha=Parameters.alpha, arms=Parameters.arms)
walker = EpsilonExplorer(epsilon=Parameters.epsilon,
                         arms=Parameters.arms)
walker._memory = memory
alpha_test_bed = TestBed(epsilon=Parameters.epsilon, arms=Parameters.arms,
                         runs=Parameters.runs, steps=Parameters.steps,
                         explorer=walker, bandit=bandit)

with TIMER:
    alpha_test_bed()
#+end_src

#+RESULTS:
#+begin_example
Started: 2021-08-02 22:59:05.981031
(epsilon=0.1) Run 0
(epsilon=0.1) Run 100
(epsilon=0.1) Run 200
(epsilon=0.1) Run 300
(epsilon=0.1) Run 400
(epsilon=0.1) Run 500
(epsilon=0.1) Run 600
(epsilon=0.1) Run 700
(epsilon=0.1) Run 800
(epsilon=0.1) Run 900
(epsilon=0.1) Run 1000
(epsilon=0.1) Run 1100
(epsilon=0.1) Run 1200
(epsilon=0.1) Run 1300
(epsilon=0.1) Run 1400
(epsilon=0.1) Run 1500
(epsilon=0.1) Run 1600
(epsilon=0.1) Run 1700
(epsilon=0.1) Run 1800
(epsilon=0.1) Run 1900
Ended: 2021-08-02 23:10:56.717860
Elapsed: 0:11:50.736829
#+end_example

** Plotting
*** Average Rewards
#+begin_src python :results none
average = average_test_bed.total_rewards/average_test_bed.runs
alpha = alpha_test_bed.total_rewards/alpha_test_bed.runs

plotter = pandas.DataFrame.from_dict({"Average": average,
                                      "Alpha": alpha})

plot = plotter.hvplot().opts(
    title="Average vs Alpha Reward",
    width=Plot.width,
    height=Plot.height,
    fontscale=Plot.fontscale,
    xlabel="Step",
    ylabel="Reward"
)

outcome = Embed(plot=plot, file_name="average_vs_alpharewards")()
#+end_src

#+begin_src python :results output html :exports output
print(outcome)
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="average_vs_alpharewards.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export
*** Optimal Choices
#+begin_src python :results none
average = 100 * average_test_bed.optimal_choices/average_test_bed.runs
alpha = 100 * alpha_test_bed.optimal_choices/alpha_test_bed.runs

plotter = pandas.DataFrame.from_dict({"Average": average,
                                      "Alpha": alpha})

plot = plotter.hvplot().opts(
    title="% Optimal Arm Chosen",
    width=Plot.width,
    height=Plot.height,
    fontscale=Plot.fontscale,
    xlabel="Step",
    ylabel="% Optimal"
)

outcome = Embed(plot=plot, file_name="average_vs_alpha_optimal_arm")()
#+end_src

#+begin_src python :results output html :exports output
print(outcome)
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="average_vs_alpha_optimal_arm.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

* End

Using a constant learning rate instead of one that diminishes over time seems to have helped with these bandits where the arms didn't payout from a distribution with a constant mean.
