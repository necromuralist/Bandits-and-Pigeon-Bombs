#+BEGIN_COMMENT
.. title: K-Armed Bandits
.. slug: k-armed-bandits
.. date: 2021-07-16 15:31:44 UTC-07:00
.. tags: bandits,tabular model,epsilon-greedy
.. category: EpsilonGreedy
.. link: 
.. description: Another version of the k-armed bandit.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 3

* Beginning
#+begin_src python :exports none :tangle ../reinforcement_learning/bandit_algorithms/k_armed_bandit.py
<<imports>>

<<setup>>


<<the-arm>>


<<the-bandit>>


<<average-memory>>


<<epsilon-explorer>>
#+end_src
** Imports
#+begin_src python :noweb-ref imports
# python
import random

# pypi
import numpy
#+end_src
** Set Up
#+begin_src python :noweb-ref setup
numpy_random = numpy.random.default_rng()
#+end_src
* Middle
** The Arm
#+begin_src python :noweb-ref the-arm
class Arm:
    """An arm for the bandit

    Args:
     center: the mean of the reward distribution
     sigma: the spread of the reward distribution
    """
    def __init__(self, center: float=None, sigma: float=1):
        self._center = center
        self.sigma = sigma
        return

    @property
    def center(self) -> float:
        """The center of the payout distribution"""
        if self._center is None:
            self._center = random.gauss(mu=0, sigma=self.sigma)
        return self._center

    def pull(self) -> float:
        """Get the payout for pulling this arm

        Returns:
         reward for this pull
        """
        return random.gauss(mu=self.center, sigma=self.sigma)
#+end_src
** The Bandit
#+begin_src python :noweb-ref the-bandit
class Bandit:
    """A k-armed bandit

    Args:
     k: number of arms for the bandit
    """
    def __init__(self, k: int=10):
        self.k = k
        self._arms = None
        self._best_arm = None
        return

    @property
    def arms(self) -> list:
        """The arms for the bandit"""
        if self._arms is None:
            self._arms = [Arm() for arm in range(self.k)]
        return self._arms

    @property
    def best_arm(self) -> int:
        """The arm with the highest mean

        Returns:
         index of the arm with the highest mean payoff
        """
        if self._best_arm is None:
            centers = numpy.array([
                arm.center for arm in self.arms])
            highest = numpy.amax(centers)
            best = numpy.where(centers == highest)[0]
            self._best_arm = numpy_random.choice(best)
        return self._best_arm

    def reset(self):
        """Resets the arms and best arm"""
        self._arms = None
        self._best_arm = None
        return

    def __call__(self, arm: int) -> float:
        """Pulls the arm

        Args:
         arm: the index for the arm to pull
        
        Returns:
         the payout from the arm
        """
        return self.arms[arm].pull()
#+end_src
** The Memory
   Average Sample memory for the epsilon agent.
#+begin_src python :noweb-ref average-memory
class AverageMemory:
    """The average sample memory for the epsilon greedy agent

    Args:
     arms: number of arms on the bandit
    """
    def __init__(self, arms: int):
        self.arms = arms
        self._pulled = None
        self._expected_reward = None        
        return

    @property
    def pulled(self) -> numpy.ndarray:
        """Count of how many times each arm was pulled"""
        if self._pulled is None:
            self._pulled = numpy.zeros(self.arms)
        return self._pulled

    @property
    def expected_reward(self) -> numpy.ndarray:
        """The expected reward for each arm"""
        if self._expected_reward is None:
            self._expected_reward = numpy.zeros(self.arms)
        return self._expected_reward

    @property
    def best_arm(self) -> int:
        """The index of the best arm"""
        best = numpy.amax(self.expected_reward)
        bestest = [index for index in range(len(self.expected_reward))
                   if self.expected_reward[index] == best]
        return numpy.random.choice(bestest)

    @property
    def random_arm(self) -> int:
        """Index of a random arm"""
        return numpy_random.integers(self.arms)

    def reset(self):
        """Resets the memory"""
        self._expected_reward = None
        self._pulled = None

    def update(self, arm: int, reward: float) -> None:
        """Updates the expected reward

        Args:
         arm: the arm that was pulled to earn the reward
         reward: the reward earned by pulling the arm
        """
        self.pulled[arm] += 1
        expected = self.expected_reward[arm]
        self.expected_reward[arm] = expected + (reward - expected)/self.pulled[arm]
        return
#+end_src
** An Epsilon Explorer
#+begin_src python :noweb-ref epsilon-explorer
class EpsilonExplorer:
    """runs the epsilon-greedy algorithm

    Args:
     epsilon: fraction of the time to explore
     arms: number of arms for the bandit
    """
    def __init__(self, epsilon: float, arms: int):
        self.epsilon = epsilon
        self.arms = arms
        self._memory = None
        return

    @property
    def memory(self) -> AverageMemory:
        """The memory of rewards earned"""
        if self._memory is None:
            self._memory = AverageMemory(arms = self.arms)
        return self._memory

    @property
    def first_arm(self) -> int:
        """The first arm to use"""
        self.most_recent_arm = self.memory.best_arm
        return self.most_recent_arm

    def reset(self):
        """Resets the memory"""
        self.memory.reset()
        return

    def __call__(self, reward: float) -> int:
        """Runs the epsilon-greedy algorithm

        Args:
         reward: the reward from the bandit

        Returns:
         the next arm to pull
        """
        self.memory.update(self.most_recent_arm, reward)
        exploit = random.random()
        self.most_recent_arm = (
            self.memory.best_arm if exploit > self.epsilon
            else self.memory.random_arm)
        return self.most_recent_arm
#+end_src
* End
  - {{% lancelot title="Reinforcement Learning by Sutton and Barto" %}}reinforcement-learning-sutton-barto{{% /lancelot %}}
