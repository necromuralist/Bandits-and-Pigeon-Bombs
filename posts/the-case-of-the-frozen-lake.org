#+BEGIN_COMMENT
.. title: The Case of the Frozen Lake
.. slug: the-case-of-the-frozen-lake
.. date: 2018-07-17 16:58:44 UTC-07:00
.. tags: markov tutorial
.. category: tutorial
.. link: 
.. description: Solving the Frozen Lake problem using Markov Decision Processes. 
.. type: text
#+END_COMMENT

* Introduction
  A person want to walk home (his goal) across a frozen lake, but the lake has holes in it that he can fall into. Our goal is to create a Markov Decision Process to get him across the lake to his home without falling into the lake.

#+BEGIN_SRC ditaa :file frozen_lake.png :exports none :results none
+-------+
|S F F F|
|F H F H|
|F F F H|
|H F F G|
+-------+
#+END_SRC

[[file:frozen_lake.png]]

| Symbol | Meaning        |
|--------+----------------|
| *S*    | Starting Point |
| *F*    | Frozen (safe)  |
| *H*    | Hole (Failure) |
| *G*    | Goal           |

So we can restate our agent's goal as  /to get from state *S* to state *G* using the shortest path without hitting any states *H*/.

* Imports
  
#+BEGIN_SRC ipython :session frozen :results none
# from pypi
import gym
import numpy
import tensorflow
#+END_SRC

* The Environment
  The OpenAI Gym provides a Frozen Lake environment. This is the general description for this type of environment.

#+BEGIN_EXAMPLE
Type:            TimeLimit
String form:     <TimeLimit<FrozenLakeEnv<FrozenLake-v0>>>
File:            ~/.virtualenvs/reinforcement-learning/lib/python3.6/site-packages/gym/wrappers/time_limit.py
Docstring:       <no docstring>
Class docstring:
The main OpenAI Gym class. It encapsulates an environment with
arbitrary behind-the-scenes dynamics. An environment can be
partially or fully observed.

The main API methods that users of this class need to know are:

    step
    reset
    render
    close
    seed

And set the following attributes:

    action_space: The Space object corresponding to valid actions
    observation_space: The Space object corresponding to valid observations
    reward_range: A tuple corresponding to the min and max possible rewards

Note: a default reward range set to [-inf,+inf] already exists. Set it if you want a narrower range.

The methods are accessed publicly as "step", "reset", etc.. The
non-underscored versions are wrapper methods to which we may add
functionality over time.
#+END_EXAMPLE

We'll make it.

#+BEGIN_SRC ipython :session frozen :results none
environment = gym.make('FrozenLake-v0')
#+END_SRC

The =observation_space= for the environment is the lake. Here's its size.

#+BEGIN_SRC ipython :session frozen :results output
print(environment.observation_space.n)
#+END_SRC

#+RESULTS:
: 16

It's 16 because we're using a 4 x 4 grid.

#+BEGIN_SRC ipython :session frozen :results output
print(environment.action_space.n)
#+END_SRC

#+RESULTS:
: 4

#+BEGIN_SRC ipython :session frozen :results output
print(environment.reward_range)
#+END_SRC

#+RESULTS:
: (0, 1)

** The Value Calculation
#+BEGIN_SRC ipython :session frozen :results none
def value(environment, gamma=1.0):
    """calculates the value table and Q-values

    Args:
     environment: the environment to explore
     gamma (float): the discount factor

    Returns:
     tuple: values-table, list of Q-values
    """
    values = numpy.zeros(environment.observation_space.n)
    iterations = 10**5
    threshold = 1e-20

    for iteration in range(1, iterations + 1):
        updated_values = numpy.copy(values)
        for state in range(environment.observation_space.n):
            Q_values = []
            for action in range(environment.action_space.n):
                next_states_rewards = []
                for next_state_reward in environment.P[state][action]:
                    transition_probability, next_state, reward_probability, _ = next_state_reward
                    next_states_rewards.append(transition_probability
                                               * (reward_probability
                                                  + gamma * updated_values))
                Q_values.append(numpy.sum(next_states_rewards))
            values[state] = max(Q_values)
        if (numpy.sum(numpy.fabs(updated_values - values))) <= threshold:
            print("convereged at iteration {}".format(iteration))
            break
    return values, Q_values
#+END_SRC

#+BEGIN_SRC ipython :session frozen :results none
optimal_values, q_values = value(environment)
#+END_SRC
