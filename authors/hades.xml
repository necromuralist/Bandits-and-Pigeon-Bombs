<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Bandits &amp; Pigeon Bombs (Posts by hades)</title><link>https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/</link><description></description><atom:link href="https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/authors/hades.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Fri, 16 Jul 2021 22:14:37 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Assessing the Performance</title><link>https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/assessing-the-performance/</link><dc:creator>hades</dc:creator><description>&lt;div&gt;&lt;section id="introduction"&gt;
&lt;h2&gt;1 Introduction&lt;/h2&gt;
&lt;p&gt;As with the Epsilon-Greedy algorithm I'm going to use the Cumulative Reward as the metric. In this case we don't really have a parameter to tune.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="imports"&gt;
&lt;h2&gt;2 Imports&lt;/h2&gt;
&lt;p&gt;The dependencies.&lt;/p&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: ERROR/3 (&lt;span class="docutils literal"&gt;&amp;lt;string&amp;gt;&lt;/span&gt;, line 13)&lt;/p&gt;
&lt;p&gt;Cannot find pygments lexer for language "ipython"&lt;/p&gt;
&lt;pre class="literal-block"&gt;.. code:: ipython

    # python standard library
    from datetime import datetime

    # pypi
    from numba import jit
    import numpy
    import pandas
    import matplotlib.pyplot as plot
    import seaborn

    # this project
    from optimistic_initial_values import OptimisticInitialValues
    from epsilon_greedy_normal import EpsilonGreedyNormal
&lt;/pre&gt;
&lt;/div&gt;
&lt;/section&gt;
&lt;section id="set-up-the-plotting"&gt;
&lt;h2&gt;3 Set-up the Plotting&lt;/h2&gt;
&lt;p&gt;This will enable the plotting and set the style.&lt;/p&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: ERROR/3 (&lt;span class="docutils literal"&gt;&amp;lt;string&amp;gt;&lt;/span&gt;, line 34)&lt;/p&gt;
&lt;p&gt;Cannot find pygments lexer for language "ipython"&lt;/p&gt;
&lt;pre class="literal-block"&gt;.. code:: ipython

    %matplotlib inline
    seaborn.set_style("whitegrid")
&lt;/pre&gt;
&lt;/div&gt;
&lt;/section&gt;
&lt;section id="running-the-assessment"&gt;
&lt;h2&gt;4 Running the Assessment&lt;/h2&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: ERROR/3 (&lt;span class="docutils literal"&gt;&amp;lt;string&amp;gt;&lt;/span&gt;, line 42)&lt;/p&gt;
&lt;p&gt;Cannot find pygments lexer for language "ipython"&lt;/p&gt;
&lt;pre class="literal-block"&gt;.. code:: ipython

    @jit
    def cumulative_reward(agent, times=5000, trials=400):
        """this generates the cumulative reward as the agent pulls the arms

        Args:
         agent: implementation that selects and updates the arms
         trials (int): number of times to train the agent
         times (int): length of time to train the agent
        Returns:
         numpy.array: average cumulative rewards over time
        """
        cumulative_rewards = numpy.zeros(times)
        for trial in range(trials):
            for time in range(times):
                arm = agent.select_arm()
                agent.update(arm)
                cumulative_rewards[time] += agent.total_reward
            agent.reset()
        return cumulative_rewards/trials
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: ERROR/3 (&lt;span class="docutils literal"&gt;&amp;lt;string&amp;gt;&lt;/span&gt;, line 64)&lt;/p&gt;
&lt;p&gt;Cannot find pygments lexer for language "ipython"&lt;/p&gt;
&lt;pre class="literal-block"&gt;.. code:: ipython

    def plot_cumulative(cumulative):
        """generates and plots cumulative average

        Args:
         cumulative (pandas.DataFrame): data to plot
        """
        figure = plot.figure(figsize=(10, 6))
        axe = figure.gca()
        axe.set_title("Cumulative Reward of the Optimistic Initial Values Algorithm ({} trials)".format(TRIALS))
        axe.set_xlabel("Time (number of pulls on the arm)")
        axe.set_ylabel("Cumulative Reward")
        cumulative.plot(ax=axe)
        return
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: ERROR/3 (&lt;span class="docutils literal"&gt;&amp;lt;string&amp;gt;&lt;/span&gt;, line 80)&lt;/p&gt;
&lt;p&gt;Cannot find pygments lexer for language "ipython"&lt;/p&gt;
&lt;pre class="literal-block"&gt;.. code:: ipython

    TRIALS = 5000
    TIMES = 400
    similar_payout_rates = numpy.arange(1.0, 6.0)
    numpy.random.shuffle(similar_payout_rates)
    one_good_arm_rates = numpy.array([1.0] * 10 + [9.0])
    numpy.random.shuffle(one_good_arm_rates)
&lt;/pre&gt;
&lt;/div&gt;
&lt;section id="similar-arms"&gt;
&lt;h3&gt;4.1 Similar Arms&lt;/h3&gt;
&lt;p&gt;This will create a range where each arm only differs by 0.1&lt;/p&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: ERROR/3 (&lt;span class="docutils literal"&gt;&amp;lt;string&amp;gt;&lt;/span&gt;, line 94)&lt;/p&gt;
&lt;p&gt;Cannot find pygments lexer for language "ipython"&lt;/p&gt;
&lt;pre class="literal-block"&gt;.. code:: ipython

    optimistic_agent = OptimisticInitialValues(similar_payout_rates, 10.0)
    data = {}
    data["Optimistic Initial Values"] = cumulative_reward(optimistic_agent, trials=TRIALS, times=TIMES)
    epsilon_agent = EpsilonGreedyNormal(0.1, similar_payout_rates)
    data["Epsilon Greedy (0.1)"] = cumulative_reward(epsilon_agent, trials=TRIALS, times=TIMES)
    data = pandas.DataFrame.from_dict(data)
    plot_cumulative(data)
&lt;/pre&gt;
&lt;/div&gt;
&lt;img alt="optimistic_similar_cumulative.png" src="https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/assessing-the-performance/optimistic_similar_cumulative.png"&gt;
&lt;p&gt;The Optimistic Initial Values agent does better than the Epsilon Greedy, as you would expect (since it eventually stops exploring). But it looks suspisciously linear.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="one-good-arm"&gt;
&lt;h3&gt;4.2 One Good Arm&lt;/h3&gt;
&lt;p&gt;Lets see how it goes when one arm dominates the payouts.&lt;/p&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: ERROR/3 (&lt;span class="docutils literal"&gt;&amp;lt;string&amp;gt;&lt;/span&gt;, line 113)&lt;/p&gt;
&lt;p&gt;Cannot find pygments lexer for language "ipython"&lt;/p&gt;
&lt;pre class="literal-block"&gt;.. code:: ipython

    numpy.random.shuffle(one_good_arm_rates)
    optimistic_agent = OptimisticInitialValues(one_good_arm_rates, 10.)
    data = {}
    data["Optimistic Initial Values"] = cumulative_reward(optimistic_agent, trials=TRIALS, times=TIMES)
    epsilon_agent = EpsilonGreedyNormal(0.1, one_good_arm_rates)
    data["Epsilon Greedy (0.1)"] = cumulative_reward(epsilon_agent, trials=TRIALS, times=TIMES)
    data = pandas.DataFrame.from_dict(data)
    plot_cumulative(data)
&lt;/pre&gt;
&lt;/div&gt;
&lt;img alt="optimistic_cumulative_one_good_arm.png" src="https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/assessing-the-performance/optimistic_cumulative_one_good_arm.png"&gt;
&lt;p&gt;It looks like the optimistic agent does even better with one dominant arm. Likely because it found it quick enough that always exploiting it gives it a huge advantage over the epsilon greedy, which never stops exploring.&lt;/p&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: ERROR/3 (&lt;span class="docutils literal"&gt;&amp;lt;string&amp;gt;&lt;/span&gt;, line 128)&lt;/p&gt;
&lt;p&gt;Cannot find pygments lexer for language "ipython"&lt;/p&gt;
&lt;pre class="literal-block"&gt;.. code:: ipython

    @jit
    def average_reward(agent, times=1000, trials=100):
        """this generates the average reward for the trials over time

        Args:
         trials (int): number of times to train the agent
         times (int): length of time to train the agent
        Returns:
         numpy.array: the average reward
        """
        average_rewards = numpy.zeros(times)
        for trial in range(trials):
            for time in range(times):
                arm = agent.select_arm()
                old_reward = agent.total_reward
                agent.update(arm)
                average_rewards[time] += (agent.total_reward - old_reward)
            agent.reset()
        return average_rewards/trials
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: ERROR/3 (&lt;span class="docutils literal"&gt;&amp;lt;string&amp;gt;&lt;/span&gt;, line 150)&lt;/p&gt;
&lt;p&gt;Cannot find pygments lexer for language "ipython"&lt;/p&gt;
&lt;pre class="literal-block"&gt;.. code:: ipython

    data = {}
    data["Optimistic Initial Values"] = average_reward(optimistic_agent, TIMES, TRIALS)
    data["Epsilon 0.1"] = average_reward(epsilon_agent, TIMES, TRIALS)
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: ERROR/3 (&lt;span class="docutils literal"&gt;&amp;lt;string&amp;gt;&lt;/span&gt;, line 156)&lt;/p&gt;
&lt;p&gt;Cannot find pygments lexer for language "ipython"&lt;/p&gt;
&lt;pre class="literal-block"&gt;.. code:: ipython

    averages = pandas.DataFrame.from_dict(data)
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: ERROR/3 (&lt;span class="docutils literal"&gt;&amp;lt;string&amp;gt;&lt;/span&gt;, line 160)&lt;/p&gt;
&lt;p&gt;Cannot find pygments lexer for language "ipython"&lt;/p&gt;
&lt;pre class="literal-block"&gt;.. code:: ipython

    figure = plot.figure(figsize=(10, 6))
    axe = figure.gca()
    axe.set_title("Average Reward (One Dominant Arm)")
    axe.set_xlabel("Time (number of pulls on the arm)")
    axe.set_ylabel("Average Reward")
    averages.plot(ax=axe, marker='.', linestyle="None")
&lt;/pre&gt;
&lt;/div&gt;
&lt;img alt="optimistic_averages.png" src="https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/assessing-the-performance/optimistic_averages.png"&gt;
&lt;p&gt;It looks like there was a brief period where the Epsilon Greedy did better, but the Optimistic agent settled in fairly quickly.&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;&lt;/div&gt;</description><category>bandits reinforcementLearning</category><guid>https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/assessing-the-performance/</guid><pubDate>Wed, 02 Aug 2017 01:46:00 GMT</pubDate></item><item><title>Finding the Best Epsilon</title><link>https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/finding-the-best-epsilon/</link><dc:creator>hades</dc:creator><description>&lt;div&gt;&lt;div class="contents topic" id="contents"&gt;
&lt;p class="topic-title"&gt;Contents&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference internal" href="https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/finding-the-best-epsilon/#introduction" id="id1"&gt;1 Introduction&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference internal" href="https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/finding-the-best-epsilon/#imports" id="id2"&gt;2 Imports&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference internal" href="https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/finding-the-best-epsilon/#set-up-the-plotting" id="id3"&gt;3 Set-up the Plotting&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference internal" href="https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/finding-the-best-epsilon/#the-probabilities" id="id4"&gt;4 The Probabilities&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference internal" href="https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/finding-the-best-epsilon/#generate-the-probabilities" id="id5"&gt;4.1 Generate the Probabilities&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference internal" href="https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/finding-the-best-epsilon/#inspecting-the-outcome" id="id6"&gt;4.2 Inspecting the Outcome&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference internal" href="https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/finding-the-best-epsilon/#average-reward" id="id7"&gt;5 Average Reward&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference internal" href="https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/finding-the-best-epsilon/#cumulative-reward" id="id8"&gt;6 Cumulative Reward&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference internal" href="https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/finding-the-best-epsilon/#imbalanced-case" id="id9"&gt;6.1 Imbalanced Case&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;section id="introduction"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/finding-the-best-epsilon/#id1"&gt;1 Introduction&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This is one of the ways to characterize the performance of the &lt;em&gt;Epsilon Greedy&lt;/em&gt; agent using our &lt;em&gt;Bernoulli Arm&lt;/em&gt;. We are going to look at three ways to evaluate how well the algorithm does.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;probability of using the best arm&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;average reward&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;cumulative reward&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;section id="imports"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/finding-the-best-epsilon/#id2"&gt;2 Imports&lt;/a&gt;&lt;/h2&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: ERROR/3 (&lt;span class="docutils literal"&gt;&amp;lt;string&amp;gt;&lt;/span&gt;, line 19)&lt;/p&gt;
&lt;p&gt;Cannot find pygments lexer for language "ipython"&lt;/p&gt;
&lt;pre class="literal-block"&gt;.. code:: ipython

    # python standard library
    import random
    from datetime import datetime

    # pypi
    from numba import jit
    import numpy
    import pandas
    import matplotlib.pyplot as plot
    import seaborn

    # this project
    from epsilon_greedy import (
        EpsilonGreedy,
        find_first
    )
    from epsilon_greedy_optimized import EpsilonGreedyOptimized
    from bernoulli_arm import BernoulliArm
&lt;/pre&gt;
&lt;/div&gt;
&lt;/section&gt;
&lt;section id="set-up-the-plotting"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/finding-the-best-epsilon/#id3"&gt;3 Set-up the Plotting&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This will enable the plotting and set the style.&lt;/p&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: ERROR/3 (&lt;span class="docutils literal"&gt;&amp;lt;string&amp;gt;&lt;/span&gt;, line 45)&lt;/p&gt;
&lt;p&gt;Cannot find pygments lexer for language "ipython"&lt;/p&gt;
&lt;pre class="literal-block"&gt;.. code:: ipython

    %matplotlib inline
    seaborn.set_style("whitegrid")
&lt;/pre&gt;
&lt;/div&gt;
&lt;/section&gt;
&lt;section id="the-probabilities"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/finding-the-best-epsilon/#id4"&gt;4 The Probabilities&lt;/a&gt;&lt;/h2&gt;
&lt;section id="generate-the-probabilities"&gt;
&lt;h3&gt;&lt;a class="toc-backref" href="https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/finding-the-best-epsilon/#id5"&gt;4.1 Generate the Probabilities&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;This code will run generate the probabilities. Although I made it so that using the &lt;code class="docutils literal"&gt;EpsilonGreedy&lt;/code&gt; call method would both choose the arm and update the reward, in this case we need to know which arm was selected so I'm going to do the steps individually.&lt;/p&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: ERROR/3 (&lt;span class="docutils literal"&gt;&amp;lt;string&amp;gt;&lt;/span&gt;, line 58)&lt;/p&gt;
&lt;p&gt;Cannot find pygments lexer for language "ipython"&lt;/p&gt;
&lt;pre class="literal-block"&gt;.. code:: ipython

    def generate_probabilities(times=1000, trials=100):
        """this generates the probabilites for finding the best arm

        Args:
         trials (int): number of times to train the agent
         times (int): length of time to train the agent
        Returns:
         Dict: the probabilites for each epsilon over time
         """
        arm_probabilities = [0.1, 0.1, 0.1, 0.9, 0.1]
        random.shuffle(arm_probabilities)
        best_arm = arm_probabilities.index(max(arm_probabilities))
        arms = [BernoulliArm(probability) for probability in arm_probabilities]
        epsilons = numpy.array([0.05, 0.1, 0.2, 0.3, 0.4, 0.5])

        outcomes = {}
        for epsilon in epsilons:
            agent = EpsilonGreedy(epsilon, arms)
            probabilities = numpy.zeros(times)
            for trial in range(trials):
                for time in range(times):
                    arm = agent.select_arm()
                    agent.update(arm)
                    if arm == best_arm:
                        probabilities[time] += 1
                agent.reset()
            outcomes["Epsilon {:.02f}".format(epsilon)] = probabilities/times
        return outcomes
&lt;/pre&gt;
&lt;/div&gt;
&lt;/section&gt;
&lt;section id="inspecting-the-outcome"&gt;
&lt;h3&gt;&lt;a class="toc-backref" href="https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/finding-the-best-epsilon/#id6"&gt;4.2 Inspecting the Outcome&lt;/a&gt;&lt;/h3&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: ERROR/3 (&lt;span class="docutils literal"&gt;&amp;lt;string&amp;gt;&lt;/span&gt;, line 92)&lt;/p&gt;
&lt;p&gt;Cannot find pygments lexer for language "ipython"&lt;/p&gt;
&lt;pre class="literal-block"&gt;.. code:: ipython

    TRIALS = 5000
    TIMES = 400
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: ERROR/3 (&lt;span class="docutils literal"&gt;&amp;lt;string&amp;gt;&lt;/span&gt;, line 97)&lt;/p&gt;
&lt;p&gt;Cannot find pygments lexer for language "ipython"&lt;/p&gt;
&lt;pre class="literal-block"&gt;.. code:: ipython

    start = datetime.now()
    probabilities = generate_probabilities(trials=TRIALS, times=TIMES)
    print("Run Time: {0}".format(datetime.now() - start))
    probabilities = pandas.DataFrame.from_dict(probabilities)
    probabilities.describe()
&lt;/pre&gt;
&lt;/div&gt;
&lt;pre class="literal-block"&gt;       Epsilon 0.05  Epsilon 0.10  Epsilon 0.20  Epsilon 0.30  Epsilon 0.40  \
count    400.000000    400.000000    400.000000    400.000000    400.000000
mean       8.784956      9.971200      9.801344      9.084844      8.235194
std        3.064916      2.523568      1.775677      1.319447      0.998305
min        0.120000      0.275000      0.495000      0.757500      1.050000
25%        7.141875      9.740000     10.264375      9.390000      8.409375
50%        9.937500     11.187500     10.456250      9.475000      8.480000
75%       11.225000     11.457500     10.515000      9.530625      8.545000
max       11.720000     11.605000     10.727500      9.665000      8.712500

       Epsilon 0.50
count    400.000000
mean       7.310087
std        0.763259
min        1.190000
25%        7.406875
50%        7.473750
75%        7.535625
max        7.725000&lt;/pre&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: ERROR/3 (&lt;span class="docutils literal"&gt;&amp;lt;string&amp;gt;&lt;/span&gt;, line 127)&lt;/p&gt;
&lt;p&gt;Cannot find pygments lexer for language "ipython"&lt;/p&gt;
&lt;pre class="literal-block"&gt;.. code:: ipython

    probabilities.to_csv("epsilon_greedy_accuracy.csv")
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: ERROR/3 (&lt;span class="docutils literal"&gt;&amp;lt;string&amp;gt;&lt;/span&gt;, line 131)&lt;/p&gt;
&lt;p&gt;Cannot find pygments lexer for language "ipython"&lt;/p&gt;
&lt;pre class="literal-block"&gt;.. code:: ipython

    figure = plot.figure(figsize=(10, 6))
    axe = figure.gca()
    axe.set_title("Accuracy of the Epsilon Greedy Algorithm ({} trials)".format(TRIALS))
    axe.set_xlabel("Time (number of pulls on the arm)")
    axe.set_ylabel("Probability of retrieving the best arm")
    probabilities.plot(ax=axe)
&lt;/pre&gt;
&lt;/div&gt;
&lt;img alt="epsilon_greedy_probablilities.png" src="https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/finding-the-best-epsilon/epsilon_greedy_probablilities.png"&gt;
&lt;p&gt;Looking at the plots, it appears that the epsilons greater than 0.05 converge faster that 0.05 (their curves are steeper at the beginning), as you would expect, but they also don't do as well in the long run, as you might also expect, since they're doing more exploration. In the long run, the more exploitation, the better the profit, but I suppose it depends on the window you have to work with, if you have a short one, then the more aggresive explorers might be better. Anything less than 350 would do better with 0.1 rather than 0.05, for instance.&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;
&lt;section id="average-reward"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/finding-the-best-epsilon/#id7"&gt;5 Average Reward&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;One of the things to note about the previous trials is that there was one arm that did notably better than all the others. When they are more uniform using the probability of retrieving the best arm might not be as revealing. Instead, using the average reward so far would give us more information.&lt;/p&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: ERROR/3 (&lt;span class="docutils literal"&gt;&amp;lt;string&amp;gt;&lt;/span&gt;, line 149)&lt;/p&gt;
&lt;p&gt;Cannot find pygments lexer for language "ipython"&lt;/p&gt;
&lt;pre class="literal-block"&gt;.. code:: ipython

    @jit
    def average_reward(times=1000, trials=100):
        """this generates the probabilites for finding the best arm

        Args:
         trials (int): number of times to train the agent
         times (int): length of time to train the agent
        Returns:
         Dict: the probabilites for each epsilon over time
        """
        arm_probabilities = numpy.array([0.1, 0.2, 0.3, 0.4, 0.5])
        random.shuffle(arm_probabilities)
        # arms = [BernoulliArm(probability) for probability in arm_probabilities]
        epsilons = numpy.array([0.1, 0.2, 0.3, 0.4, 0.5])

        outcomes = {}
        for epsilon in epsilons:
            agent = EpsilonGreedyOptimized(epsilon, arm_probabilities)
            average_rewards = numpy.zeros(times)
            for trial in range(trials):
                for time in range(times):
                    arm = agent.select_arm()
                    old_reward = agent.total_reward
                    agent.update(arm)
                    average_rewards[time] += (agent.total_reward - old_reward)
                agent.reset()
            outcomes["Epsilon {0:.02f}".format(epsilon)] = average_rewards/trials
        return outcomes
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: ERROR/3 (&lt;span class="docutils literal"&gt;&amp;lt;string&amp;gt;&lt;/span&gt;, line 180)&lt;/p&gt;
&lt;p&gt;Cannot find pygments lexer for language "ipython"&lt;/p&gt;
&lt;pre class="literal-block"&gt;.. code:: ipython

    start = datetime.now()
    averages = average_reward(TIMES, TRIALS)
    print("Run Time: {0}".format(datetime.now() - start))
    averages = pandas.DataFrame.from_dict(averages)
&lt;/pre&gt;
&lt;/div&gt;
&lt;pre class="literal-block"&gt;Run Time: 0:01:08.727723&lt;/pre&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: ERROR/3 (&lt;span class="docutils literal"&gt;&amp;lt;string&amp;gt;&lt;/span&gt;, line 191)&lt;/p&gt;
&lt;p&gt;Cannot find pygments lexer for language "ipython"&lt;/p&gt;
&lt;pre class="literal-block"&gt;.. code:: ipython

    figure = plot.figure(figsize=(10, 6))
    axe = figure.gca()
    axe.set_title("Reward of the Epsilon Greedy Algorithm ({} trials)".format(TRIALS))
    axe.set_xlabel("Time (number of pulls on the arm)")
    axe.set_ylabel("Average Reward")
    averages.plot(ax=axe, marker='.', linestyle="None")
&lt;/pre&gt;
&lt;/div&gt;
&lt;img alt="epsilon_averages.png" src="https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/finding-the-best-epsilon/epsilon_averages.png"&gt;
&lt;p&gt;There's much more variablity and overlap here, as you might expect since I made the probabilities closer. Interestingly, the strongly exploratory agents seem to do worse, even from the beginning, while the more exploitative ones do better.  Although it looks like 0.2 might be doing as well or better than 0.1 once you get over 100.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="cumulative-reward"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/finding-the-best-epsilon/#id8"&gt;6 Cumulative Reward&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The previous two metrics turn out to be useful, but somewhat unfair to the aggresively exploring models, which we know won't ultimately do as well, but do have an advantage in the initial phase. To better qualify the overall effect of exploration versus exploitation, it's better to use a cumulative sum of the rewards.&lt;/p&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: ERROR/3 (&lt;span class="docutils literal"&gt;&amp;lt;string&amp;gt;&lt;/span&gt;, line 209)&lt;/p&gt;
&lt;p&gt;Cannot find pygments lexer for language "ipython"&lt;/p&gt;
&lt;pre class="literal-block"&gt;.. code:: ipython

    @jit
    def cumulative_reward(arms, times=1000, trials=100):
        """this generates the cumulative reward as the agent pulls the arms

        Args:
         arms (numpy.array): array of probabilities that the arm will pay-off
         trials (int): number of times to train the agent
         times (int): length of time to train the agent
        Returns:
         Dict: the probabilites for each epsilon over time
        """
        random.shuffle(arms)
        epsilons = numpy.array([0.1, 0.2, 0.3, 0.4, 0.5])

        outcomes = {}
        for epsilon in epsilons:
            agent = EpsilonGreedyOptimized(epsilon, arms)
            cumulative_rewards = numpy.zeros(times)
            for trial in range(trials):
                for time in range(times):
                    arm = agent.select_arm()
                    agent.update(arm)
                    cumulative_rewards[time] = agent.total_reward
                agent.reset()
            outcomes["Epsilon {:.02f}".format(epsilon)] = cumulative_rewards/trials
        return outcomes
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: ERROR/3 (&lt;span class="docutils literal"&gt;&amp;lt;string&amp;gt;&lt;/span&gt;, line 238)&lt;/p&gt;
&lt;p&gt;Cannot find pygments lexer for language "ipython"&lt;/p&gt;
&lt;pre class="literal-block"&gt;.. code:: ipython

    def generate_cumulative(arms):
        """runs the cumulative output function

        Args:
         arms (numpy.array): probabilities that arms will pay out

        Returns:
         pandas.DataFrame: the average cumulative rewards
        """
        start = datetime.now()
        cumulative = cumulative_reward(arms, times=TIMES, trials=TRIALS)
        print("Run Time: {0}".format(datetime.now() - start))
        return pandas.DataFrame.from_dict(cumulative)
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: ERROR/3 (&lt;span class="docutils literal"&gt;&amp;lt;string&amp;gt;&lt;/span&gt;, line 254)&lt;/p&gt;
&lt;p&gt;Cannot find pygments lexer for language "ipython"&lt;/p&gt;
&lt;pre class="literal-block"&gt;.. code:: ipython

    def plot_cumulative(cumulative):
        """generates and plots cumulative average

        Args:
         cumulative (pandas.DataFrame): data to plot
        """
        figure = plot.figure(figsize=(10, 6))
        axe = figure.gca()
        axe.set_title("Cumulative Reward of the Epsilon Greedy Algorithm ({} trials)".format(TRIALS))
        axe.set_xlabel("Time (number of pulls on the arm)")
        axe.set_ylabel("Cumulative Reward")
        cumulative.plot(ax=axe)
        return
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: ERROR/3 (&lt;span class="docutils literal"&gt;&amp;lt;string&amp;gt;&lt;/span&gt;, line 270)&lt;/p&gt;
&lt;p&gt;Cannot find pygments lexer for language "ipython"&lt;/p&gt;
&lt;pre class="literal-block"&gt;.. code:: ipython

    cumulative = generate_cumulative(numpy.arange(0.1, 0.6, 0.1))
    plot_cumulative(cumulative)
&lt;/pre&gt;
&lt;/div&gt;
&lt;img alt="epsilon_greedy_cumulative.png" src="https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/finding-the-best-epsilon/epsilon_greedy_cumulative.png"&gt;
&lt;p&gt;Because of the randomness this will change everytime you run it, but we can see that in this case, the average cumulative reward was better for the 0.3 and 0.5 epsilon values than the more conservative values up until around 275, and the second most conservative case (0.2) actually did worse on average than the more exploratory cases did.&lt;/p&gt;
&lt;section id="imbalanced-case"&gt;
&lt;h3&gt;&lt;a class="toc-backref" href="https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/finding-the-best-epsilon/#id9"&gt;6.1 Imbalanced Case&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;I'll re-run this again with more arms and a only one clear good arm to see if this changes things.&lt;/p&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: ERROR/3 (&lt;span class="docutils literal"&gt;&amp;lt;string&amp;gt;&lt;/span&gt;, line 284)&lt;/p&gt;
&lt;p&gt;Cannot find pygments lexer for language "ipython"&lt;/p&gt;
&lt;pre class="literal-block"&gt;.. code:: ipython

    plot_cumulative(generate_cumulative(numpy.array([0.1] * 10 + [0.9])))
&lt;/pre&gt;
&lt;/div&gt;
&lt;img alt="epsilon_cumulative_2.png" src="https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/finding-the-best-epsilon/epsilon_cumulative_2.png"&gt;
&lt;p&gt;In this case, the most exploitive agent did much worse than the other agents. It looks like it didn't find the best arm until around the 240th pull. In this case, when most arms pay off poorly and one arm pays off much better, the exploratory arms accumulate more reward within our time frame. I'm guessing that the 0.10 epsilon would, given enough time, pull ahead, and you can in fact see that the most exploratory agent has already been surpassed by the 0.2 agent, so eventually exploration would probably take a back seat to exploitation, but not in this case. It's important to note, however, that if the most exploitive agent had happened to find the best arm at the start, he would likely have ended up the best, it's just the nature of randomization that you aren't guaranteed that this would be the case.&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;&lt;/div&gt;</description><category>algorithm</category><guid>https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/finding-the-best-epsilon/</guid><pubDate>Mon, 31 Jul 2017 01:41:00 GMT</pubDate></item><item><title>A Bernoulli Arm</title><link>https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/A-Bernoulli-Arm/</link><dc:creator>hades</dc:creator><description>&lt;div&gt;&lt;section id="introduction"&gt;
&lt;h2&gt;1 Introduction&lt;/h2&gt;
&lt;p&gt;This is an implementation of one arm of a &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Multi-armed_bandit"&gt;n-armed bandit&lt;/a&gt; to test the Epsilon Greedy algorithm. It takes a probability that it will return a reward. It also optionally let's you set the penalty and reward values, but defaults to a reward of 1 and a penalty of 0 (so it's really no reward more than a penalty).&lt;/p&gt;
&lt;/section&gt;
&lt;section id="imports"&gt;
&lt;h2&gt;2 Imports&lt;/h2&gt;
&lt;pre class="code python"&gt;&lt;a id="rest_code_59c23e93ff73428f8d6772e3a4df0317-1" name="rest_code_59c23e93ff73428f8d6772e3a4df0317-1"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# python standard library&lt;/span&gt;
&lt;a id="rest_code_59c23e93ff73428f8d6772e3a4df0317-2" name="rest_code_59c23e93ff73428f8d6772e3a4df0317-2"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt;
&lt;/pre&gt;&lt;/section&gt;
&lt;section id="bernoulli-arm"&gt;
&lt;h2&gt;3 Bernoulli Arm&lt;/h2&gt;
&lt;p&gt;The Bernoulli Arm will generate a value when its arm is pulled at a payout rate specified by the `probability` value.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a id="rest_code_e364a3b548814ee2b708d2ccf0d4e03d-1" name="rest_code_e364a3b548814ee2b708d2ccf0d4e03d-1"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;imports&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a id="rest_code_e364a3b548814ee2b708d2ccf0d4e03d-2" name="rest_code_e364a3b548814ee2b708d2ccf0d4e03d-2"&gt;&lt;/a&gt;
&lt;a id="rest_code_e364a3b548814ee2b708d2ccf0d4e03d-3" name="rest_code_e364a3b548814ee2b708d2ccf0d4e03d-3"&gt;&lt;/a&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;BernoulliArm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a id="rest_code_e364a3b548814ee2b708d2ccf0d4e03d-4" name="rest_code_e364a3b548814ee2b708d2ccf0d4e03d-4"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""A simulation of one arm of a multi-armed bandit&lt;/span&gt;
&lt;a id="rest_code_e364a3b548814ee2b708d2ccf0d4e03d-5" name="rest_code_e364a3b548814ee2b708d2ccf0d4e03d-5"&gt;&lt;/a&gt;
&lt;a id="rest_code_e364a3b548814ee2b708d2ccf0d4e03d-6" name="rest_code_e364a3b548814ee2b708d2ccf0d4e03d-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a id="rest_code_e364a3b548814ee2b708d2ccf0d4e03d-7" name="rest_code_e364a3b548814ee2b708d2ccf0d4e03d-7"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     probability (float): probability of a reward&lt;/span&gt;
&lt;a id="rest_code_e364a3b548814ee2b708d2ccf0d4e03d-8" name="rest_code_e364a3b548814ee2b708d2ccf0d4e03d-8"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     reward (float): value to return on a win&lt;/span&gt;
&lt;a id="rest_code_e364a3b548814ee2b708d2ccf0d4e03d-9" name="rest_code_e364a3b548814ee2b708d2ccf0d4e03d-9"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     penalty (float): value to return on a loss&lt;/span&gt;
&lt;a id="rest_code_e364a3b548814ee2b708d2ccf0d4e03d-10" name="rest_code_e364a3b548814ee2b708d2ccf0d4e03d-10"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a id="rest_code_e364a3b548814ee2b708d2ccf0d4e03d-11" name="rest_code_e364a3b548814ee2b708d2ccf0d4e03d-11"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;constructor&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a id="rest_code_e364a3b548814ee2b708d2ccf0d4e03d-12" name="rest_code_e364a3b548814ee2b708d2ccf0d4e03d-12"&gt;&lt;/a&gt;
&lt;a id="rest_code_e364a3b548814ee2b708d2ccf0d4e03d-13" name="rest_code_e364a3b548814ee2b708d2ccf0d4e03d-13"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;call&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;section id="constructor"&gt;
&lt;h3&gt;3.1 Constructor&lt;/h3&gt;
&lt;p&gt;The constructor takes three values:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;probability of winning&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;reward on winning&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;penalty on losing&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Because of the way the problem is set up, the reward and penalty are already set at 1 and 0, but I didn't want there to be magic numbers so they can be changed if needed.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a id="rest_code_27d7828a65af493f9f91fb65915e59f3-1" name="rest_code_27d7828a65af493f9f91fb65915e59f3-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;probability&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;penalty&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a id="rest_code_27d7828a65af493f9f91fb65915e59f3-2" name="rest_code_27d7828a65af493f9f91fb65915e59f3-2"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;probability&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;probability&lt;/span&gt;
&lt;a id="rest_code_27d7828a65af493f9f91fb65915e59f3-3" name="rest_code_27d7828a65af493f9f91fb65915e59f3-3"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;
&lt;a id="rest_code_27d7828a65af493f9f91fb65915e59f3-4" name="rest_code_27d7828a65af493f9f91fb65915e59f3-4"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;penalty&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;penalty&lt;/span&gt;
&lt;a id="rest_code_27d7828a65af493f9f91fb65915e59f3-5" name="rest_code_27d7828a65af493f9f91fb65915e59f3-5"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/section&gt;
&lt;section id="the-call"&gt;
&lt;h3&gt;3.2 The Call&lt;/h3&gt;
&lt;p&gt;This is called &lt;code class="docutils literal"&gt;pull&lt;/code&gt; in most cases, but I thought it would be more uniform to put it in a call.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a id="rest_code_cf02803d1a3649b4a4f5b98acd742754-1" name="rest_code_cf02803d1a3649b4a4f5b98acd742754-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__call__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a id="rest_code_cf02803d1a3649b4a4f5b98acd742754-2" name="rest_code_cf02803d1a3649b4a4f5b98acd742754-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""pulls the arm and returns a reward or penalty&lt;/span&gt;
&lt;a id="rest_code_cf02803d1a3649b4a4f5b98acd742754-3" name="rest_code_cf02803d1a3649b4a4f5b98acd742754-3"&gt;&lt;/a&gt;
&lt;a id="rest_code_cf02803d1a3649b4a4f5b98acd742754-4" name="rest_code_cf02803d1a3649b4a4f5b98acd742754-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;a id="rest_code_cf02803d1a3649b4a4f5b98acd742754-5" name="rest_code_cf02803d1a3649b4a4f5b98acd742754-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     float: value returned on pulling the arm&lt;/span&gt;
&lt;a id="rest_code_cf02803d1a3649b4a4f5b98acd742754-6" name="rest_code_cf02803d1a3649b4a4f5b98acd742754-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a id="rest_code_cf02803d1a3649b4a4f5b98acd742754-7" name="rest_code_cf02803d1a3649b4a4f5b98acd742754-7"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;probability&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a id="rest_code_cf02803d1a3649b4a4f5b98acd742754-8" name="rest_code_cf02803d1a3649b4a4f5b98acd742754-8"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;penalty&lt;/span&gt;
&lt;a id="rest_code_cf02803d1a3649b4a4f5b98acd742754-9" name="rest_code_cf02803d1a3649b4a4f5b98acd742754-9"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reward&lt;/span&gt;
&lt;/pre&gt;&lt;/section&gt;
&lt;/section&gt;&lt;/div&gt;</description><category>algorithm</category><guid>https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/A-Bernoulli-Arm/</guid><pubDate>Mon, 31 Jul 2017 01:37:00 GMT</pubDate></item><item><title>The Epsilon Greedy Algorithm</title><link>https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/The-Epsilon-Greedy-Algorithm/</link><dc:creator>hades</dc:creator><description>&lt;div&gt;&lt;section id="background"&gt;
&lt;h2&gt;1 Background&lt;/h2&gt;
&lt;p&gt;This is an implementation of the Epsilon Greedy algorithm to find solutions for the multi-arm-bandit problem.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="imports"&gt;
&lt;h2&gt;2 Imports&lt;/h2&gt;
&lt;pre class="code python"&gt;&lt;a id="rest_code_ecd54e4c2c744fc7aff54de6c02ddf22-1" name="rest_code_ecd54e4c2c744fc7aff54de6c02ddf22-1"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# python&lt;/span&gt;
&lt;a id="rest_code_ecd54e4c2c744fc7aff54de6c02ddf22-2" name="rest_code_ecd54e4c2c744fc7aff54de6c02ddf22-2"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt;
&lt;a id="rest_code_ecd54e4c2c744fc7aff54de6c02ddf22-3" name="rest_code_ecd54e4c2c744fc7aff54de6c02ddf22-3"&gt;&lt;/a&gt;
&lt;a id="rest_code_ecd54e4c2c744fc7aff54de6c02ddf22-4" name="rest_code_ecd54e4c2c744fc7aff54de6c02ddf22-4"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# pypi&lt;/span&gt;
&lt;a id="rest_code_ecd54e4c2c744fc7aff54de6c02ddf22-5" name="rest_code_ecd54e4c2c744fc7aff54de6c02ddf22-5"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numba&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;jit&lt;/span&gt;
&lt;a id="rest_code_ecd54e4c2c744fc7aff54de6c02ddf22-6" name="rest_code_ecd54e4c2c744fc7aff54de6c02ddf22-6"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt;
&lt;/pre&gt;&lt;/section&gt;
&lt;section id="find-first"&gt;
&lt;h2&gt;3 Find First&lt;/h2&gt;
&lt;p&gt;This is a helper function to find the first matching item in an array-like collection.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a id="rest_code_868c84a3c3454949adb89ddd9b742f61-1" name="rest_code_868c84a3c3454949adb89ddd9b742f61-1"&gt;&lt;/a&gt;&lt;span class="nd"&gt;@jit&lt;/span&gt;
&lt;a id="rest_code_868c84a3c3454949adb89ddd9b742f61-2" name="rest_code_868c84a3c3454949adb89ddd9b742f61-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;find_first&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vector&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a id="rest_code_868c84a3c3454949adb89ddd9b742f61-3" name="rest_code_868c84a3c3454949adb89ddd9b742f61-3"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""find the first item in the vector&lt;/span&gt;
&lt;a id="rest_code_868c84a3c3454949adb89ddd9b742f61-4" name="rest_code_868c84a3c3454949adb89ddd9b742f61-4"&gt;&lt;/a&gt;
&lt;a id="rest_code_868c84a3c3454949adb89ddd9b742f61-5" name="rest_code_868c84a3c3454949adb89ddd9b742f61-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a id="rest_code_868c84a3c3454949adb89ddd9b742f61-6" name="rest_code_868c84a3c3454949adb89ddd9b742f61-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     item: thing to match&lt;/span&gt;
&lt;a id="rest_code_868c84a3c3454949adb89ddd9b742f61-7" name="rest_code_868c84a3c3454949adb89ddd9b742f61-7"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     vector: thing to search&lt;/span&gt;
&lt;a id="rest_code_868c84a3c3454949adb89ddd9b742f61-8" name="rest_code_868c84a3c3454949adb89ddd9b742f61-8"&gt;&lt;/a&gt;
&lt;a id="rest_code_868c84a3c3454949adb89ddd9b742f61-9" name="rest_code_868c84a3c3454949adb89ddd9b742f61-9"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;a id="rest_code_868c84a3c3454949adb89ddd9b742f61-10" name="rest_code_868c84a3c3454949adb89ddd9b742f61-10"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     value: index of first matching item, -1 if not found&lt;/span&gt;
&lt;a id="rest_code_868c84a3c3454949adb89ddd9b742f61-11" name="rest_code_868c84a3c3454949adb89ddd9b742f61-11"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a id="rest_code_868c84a3c3454949adb89ddd9b742f61-12" name="rest_code_868c84a3c3454949adb89ddd9b742f61-12"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vector&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
&lt;a id="rest_code_868c84a3c3454949adb89ddd9b742f61-13" name="rest_code_868c84a3c3454949adb89ddd9b742f61-13"&gt;&lt;/a&gt;        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;vector&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
&lt;a id="rest_code_868c84a3c3454949adb89ddd9b742f61-14" name="rest_code_868c84a3c3454949adb89ddd9b742f61-14"&gt;&lt;/a&gt;            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;
&lt;a id="rest_code_868c84a3c3454949adb89ddd9b742f61-15" name="rest_code_868c84a3c3454949adb89ddd9b742f61-15"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/section&gt;
&lt;section id="epsilon-greedy"&gt;
&lt;h2&gt;4 Epsilon Greedy&lt;/h2&gt;
&lt;p&gt;The &lt;em&gt;epsilon-greedy&lt;/em&gt; algorithm tries to solve the exploitation-exploration dilemna by exploring a fraction of the time (set by &lt;em&gt;epsilon&lt;/em&gt;) and using the best solution found so far the rest of the time. This implementation is based on the one in Bandit Algorithms for Website Optimization &lt;a class="footnote-reference brackets" href="https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/The-Epsilon-Greedy-Algorithm/#id5" id="id1"&gt;1&lt;/a&gt; .&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a id="rest_code_c0664ab7f04344e3ae2bfe42729285da-1" name="rest_code_c0664ab7f04344e3ae2bfe42729285da-1"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;imports&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a id="rest_code_c0664ab7f04344e3ae2bfe42729285da-2" name="rest_code_c0664ab7f04344e3ae2bfe42729285da-2"&gt;&lt;/a&gt;
&lt;a id="rest_code_c0664ab7f04344e3ae2bfe42729285da-3" name="rest_code_c0664ab7f04344e3ae2bfe42729285da-3"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a id="rest_code_c0664ab7f04344e3ae2bfe42729285da-4" name="rest_code_c0664ab7f04344e3ae2bfe42729285da-4"&gt;&lt;/a&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;EpsilonGreedy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a id="rest_code_c0664ab7f04344e3ae2bfe42729285da-5" name="rest_code_c0664ab7f04344e3ae2bfe42729285da-5"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""The Epsilon Greedy Algorithm&lt;/span&gt;
&lt;a id="rest_code_c0664ab7f04344e3ae2bfe42729285da-6" name="rest_code_c0664ab7f04344e3ae2bfe42729285da-6"&gt;&lt;/a&gt;
&lt;a id="rest_code_c0664ab7f04344e3ae2bfe42729285da-7" name="rest_code_c0664ab7f04344e3ae2bfe42729285da-7"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a id="rest_code_c0664ab7f04344e3ae2bfe42729285da-8" name="rest_code_c0664ab7f04344e3ae2bfe42729285da-8"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     epsilon (float): fraction of the time to explore&lt;/span&gt;
&lt;a id="rest_code_c0664ab7f04344e3ae2bfe42729285da-9" name="rest_code_c0664ab7f04344e3ae2bfe42729285da-9"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     arms (list): collection of bandits to pull&lt;/span&gt;
&lt;a id="rest_code_c0664ab7f04344e3ae2bfe42729285da-10" name="rest_code_c0664ab7f04344e3ae2bfe42729285da-10"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a id="rest_code_c0664ab7f04344e3ae2bfe42729285da-11" name="rest_code_c0664ab7f04344e3ae2bfe42729285da-11"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;constructor&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a id="rest_code_c0664ab7f04344e3ae2bfe42729285da-12" name="rest_code_c0664ab7f04344e3ae2bfe42729285da-12"&gt;&lt;/a&gt;
&lt;a id="rest_code_c0664ab7f04344e3ae2bfe42729285da-13" name="rest_code_c0664ab7f04344e3ae2bfe42729285da-13"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;best&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a id="rest_code_c0664ab7f04344e3ae2bfe42729285da-14" name="rest_code_c0664ab7f04344e3ae2bfe42729285da-14"&gt;&lt;/a&gt;
&lt;a id="rest_code_c0664ab7f04344e3ae2bfe42729285da-15" name="rest_code_c0664ab7f04344e3ae2bfe42729285da-15"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a id="rest_code_c0664ab7f04344e3ae2bfe42729285da-16" name="rest_code_c0664ab7f04344e3ae2bfe42729285da-16"&gt;&lt;/a&gt;
&lt;a id="rest_code_c0664ab7f04344e3ae2bfe42729285da-17" name="rest_code_c0664ab7f04344e3ae2bfe42729285da-17"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a id="rest_code_c0664ab7f04344e3ae2bfe42729285da-18" name="rest_code_c0664ab7f04344e3ae2bfe42729285da-18"&gt;&lt;/a&gt;
&lt;a id="rest_code_c0664ab7f04344e3ae2bfe42729285da-19" name="rest_code_c0664ab7f04344e3ae2bfe42729285da-19"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;select&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a id="rest_code_c0664ab7f04344e3ae2bfe42729285da-20" name="rest_code_c0664ab7f04344e3ae2bfe42729285da-20"&gt;&lt;/a&gt;
&lt;a id="rest_code_c0664ab7f04344e3ae2bfe42729285da-21" name="rest_code_c0664ab7f04344e3ae2bfe42729285da-21"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a id="rest_code_c0664ab7f04344e3ae2bfe42729285da-22" name="rest_code_c0664ab7f04344e3ae2bfe42729285da-22"&gt;&lt;/a&gt;
&lt;a id="rest_code_c0664ab7f04344e3ae2bfe42729285da-23" name="rest_code_c0664ab7f04344e3ae2bfe42729285da-23"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a id="rest_code_c0664ab7f04344e3ae2bfe42729285da-24" name="rest_code_c0664ab7f04344e3ae2bfe42729285da-24"&gt;&lt;/a&gt;
&lt;a id="rest_code_c0664ab7f04344e3ae2bfe42729285da-25" name="rest_code_c0664ab7f04344e3ae2bfe42729285da-25"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;call&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;section id="the-constructor"&gt;
&lt;h3&gt;4.1 The Constructor&lt;/h3&gt;
&lt;p&gt;The constructor takes two arguments - &lt;em&gt;epsilon&lt;/em&gt; and &lt;em&gt;arms&lt;/em&gt;. The &lt;em&gt;arms&lt;/em&gt; list should contain bandits that return a reward or penalty when pulled (called).&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a id="rest_code_89bda47ae7164949b16d86851c8cc1cd-1" name="rest_code_89bda47ae7164949b16d86851c8cc1cd-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a id="rest_code_89bda47ae7164949b16d86851c8cc1cd-2" name="rest_code_89bda47ae7164949b16d86851c8cc1cd-2"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;
&lt;a id="rest_code_89bda47ae7164949b16d86851c8cc1cd-3" name="rest_code_89bda47ae7164949b16d86851c8cc1cd-3"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;arms&lt;/span&gt;
&lt;a id="rest_code_89bda47ae7164949b16d86851c8cc1cd-4" name="rest_code_89bda47ae7164949b16d86851c8cc1cd-4"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
&lt;a id="rest_code_89bda47ae7164949b16d86851c8cc1cd-5" name="rest_code_89bda47ae7164949b16d86851c8cc1cd-5"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_rewards&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
&lt;a id="rest_code_89bda47ae7164949b16d86851c8cc1cd-6" name="rest_code_89bda47ae7164949b16d86851c8cc1cd-6"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
&lt;a id="rest_code_89bda47ae7164949b16d86851c8cc1cd-7" name="rest_code_89bda47ae7164949b16d86851c8cc1cd-7"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/section&gt;
&lt;section id="best-arm"&gt;
&lt;h3&gt;4.2 Best Arm&lt;/h3&gt;
&lt;p&gt;The &lt;code class="docutils literal"&gt;best_arm&lt;/code&gt; property returns the index of the arm that has the highest average reward so far. It returns the index instead of the arm itself because it's used to get the matching counts and rewards in the &lt;code class="docutils literal"&gt;update&lt;/code&gt; method.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a id="rest_code_32f1761261cc453ebc6af8f2fe301ea0-1" name="rest_code_32f1761261cc453ebc6af8f2fe301ea0-1"&gt;&lt;/a&gt;&lt;span class="nd"&gt;@property&lt;/span&gt;
&lt;a id="rest_code_32f1761261cc453ebc6af8f2fe301ea0-2" name="rest_code_32f1761261cc453ebc6af8f2fe301ea0-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;best_arm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a id="rest_code_32f1761261cc453ebc6af8f2fe301ea0-3" name="rest_code_32f1761261cc453ebc6af8f2fe301ea0-3"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""Index of the arm with the most reward"""&lt;/span&gt;
&lt;a id="rest_code_32f1761261cc453ebc6af8f2fe301ea0-4" name="rest_code_32f1761261cc453ebc6af8f2fe301ea0-4"&gt;&lt;/a&gt;    &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a id="rest_code_32f1761261cc453ebc6af8f2fe301ea0-5" name="rest_code_32f1761261cc453ebc6af8f2fe301ea0-5"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;find_first&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/section&gt;
&lt;section id="counts"&gt;
&lt;h3&gt;4.3 Counts&lt;/h3&gt;
&lt;p&gt;The `counts` keeps track of the number of times each arm is pulled.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a id="rest_code_8f8efd5dbf16439297b82899dfd9759a-1" name="rest_code_8f8efd5dbf16439297b82899dfd9759a-1"&gt;&lt;/a&gt;&lt;span class="nd"&gt;@property&lt;/span&gt;
&lt;a id="rest_code_8f8efd5dbf16439297b82899dfd9759a-2" name="rest_code_8f8efd5dbf16439297b82899dfd9759a-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a id="rest_code_8f8efd5dbf16439297b82899dfd9759a-3" name="rest_code_8f8efd5dbf16439297b82899dfd9759a-3"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""counts of times each arm is pulled&lt;/span&gt;
&lt;a id="rest_code_8f8efd5dbf16439297b82899dfd9759a-4" name="rest_code_8f8efd5dbf16439297b82899dfd9759a-4"&gt;&lt;/a&gt;
&lt;a id="rest_code_8f8efd5dbf16439297b82899dfd9759a-5" name="rest_code_8f8efd5dbf16439297b82899dfd9759a-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;a id="rest_code_8f8efd5dbf16439297b82899dfd9759a-6" name="rest_code_8f8efd5dbf16439297b82899dfd9759a-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     numpy.array: array of counts&lt;/span&gt;
&lt;a id="rest_code_8f8efd5dbf16439297b82899dfd9759a-7" name="rest_code_8f8efd5dbf16439297b82899dfd9759a-7"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a id="rest_code_8f8efd5dbf16439297b82899dfd9759a-8" name="rest_code_8f8efd5dbf16439297b82899dfd9759a-8"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_counts&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a id="rest_code_8f8efd5dbf16439297b82899dfd9759a-9" name="rest_code_8f8efd5dbf16439297b82899dfd9759a-9"&gt;&lt;/a&gt;        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a id="rest_code_8f8efd5dbf16439297b82899dfd9759a-10" name="rest_code_8f8efd5dbf16439297b82899dfd9759a-10"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_counts&lt;/span&gt;
&lt;/pre&gt;&lt;/section&gt;
&lt;section id="rewards"&gt;
&lt;h3&gt;4.4 Rewards&lt;/h3&gt;
&lt;p&gt;The &lt;code class="docutils literal"&gt;rewards&lt;/code&gt; attributes holds the running average reward that each arm has returned.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a id="rest_code_c3baf85d2f9041279cc4fee2b77d9bfa-1" name="rest_code_c3baf85d2f9041279cc4fee2b77d9bfa-1"&gt;&lt;/a&gt;&lt;span class="nd"&gt;@property&lt;/span&gt;
&lt;a id="rest_code_c3baf85d2f9041279cc4fee2b77d9bfa-2" name="rest_code_c3baf85d2f9041279cc4fee2b77d9bfa-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a id="rest_code_c3baf85d2f9041279cc4fee2b77d9bfa-3" name="rest_code_c3baf85d2f9041279cc4fee2b77d9bfa-3"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""array of running average of rewards for each arms&lt;/span&gt;
&lt;a id="rest_code_c3baf85d2f9041279cc4fee2b77d9bfa-4" name="rest_code_c3baf85d2f9041279cc4fee2b77d9bfa-4"&gt;&lt;/a&gt;
&lt;a id="rest_code_c3baf85d2f9041279cc4fee2b77d9bfa-5" name="rest_code_c3baf85d2f9041279cc4fee2b77d9bfa-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;a id="rest_code_c3baf85d2f9041279cc4fee2b77d9bfa-6" name="rest_code_c3baf85d2f9041279cc4fee2b77d9bfa-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     numpy.array: running averages&lt;/span&gt;
&lt;a id="rest_code_c3baf85d2f9041279cc4fee2b77d9bfa-7" name="rest_code_c3baf85d2f9041279cc4fee2b77d9bfa-7"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a id="rest_code_c3baf85d2f9041279cc4fee2b77d9bfa-8" name="rest_code_c3baf85d2f9041279cc4fee2b77d9bfa-8"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_rewards&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a id="rest_code_c3baf85d2f9041279cc4fee2b77d9bfa-9" name="rest_code_c3baf85d2f9041279cc4fee2b77d9bfa-9"&gt;&lt;/a&gt;        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_rewards&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a id="rest_code_c3baf85d2f9041279cc4fee2b77d9bfa-10" name="rest_code_c3baf85d2f9041279cc4fee2b77d9bfa-10"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_rewards&lt;/span&gt;
&lt;/pre&gt;&lt;/section&gt;
&lt;section id="reset"&gt;
&lt;h3&gt;4.5 Reset&lt;/h3&gt;
&lt;pre class="code python"&gt;&lt;a id="rest_code_b27b8511556544abb728790a26bb1b8e-1" name="rest_code_b27b8511556544abb728790a26bb1b8e-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a id="rest_code_b27b8511556544abb728790a26bb1b8e-2" name="rest_code_b27b8511556544abb728790a26bb1b8e-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""sets the counts and rewards to None&lt;/span&gt;
&lt;a id="rest_code_b27b8511556544abb728790a26bb1b8e-3" name="rest_code_b27b8511556544abb728790a26bb1b8e-3"&gt;&lt;/a&gt;
&lt;a id="rest_code_b27b8511556544abb728790a26bb1b8e-4" name="rest_code_b27b8511556544abb728790a26bb1b8e-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    This lets you re-used the EpsilonGreedy without re-constructing&lt;/span&gt;
&lt;a id="rest_code_b27b8511556544abb728790a26bb1b8e-5" name="rest_code_b27b8511556544abb728790a26bb1b8e-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    the arms&lt;/span&gt;
&lt;a id="rest_code_b27b8511556544abb728790a26bb1b8e-6" name="rest_code_b27b8511556544abb728790a26bb1b8e-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a id="rest_code_b27b8511556544abb728790a26bb1b8e-7" name="rest_code_b27b8511556544abb728790a26bb1b8e-7"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
&lt;a id="rest_code_b27b8511556544abb728790a26bb1b8e-8" name="rest_code_b27b8511556544abb728790a26bb1b8e-8"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_rewards&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
&lt;a id="rest_code_b27b8511556544abb728790a26bb1b8e-9" name="rest_code_b27b8511556544abb728790a26bb1b8e-9"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;a id="rest_code_b27b8511556544abb728790a26bb1b8e-10" name="rest_code_b27b8511556544abb728790a26bb1b8e-10"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/section&gt;
&lt;section id="select-arm"&gt;
&lt;h3&gt;4.6 Select Arm&lt;/h3&gt;
&lt;p&gt;The &lt;em&gt;select_arm&lt;/em&gt; method will choose either the best arm or a random one based on a randomly drawn value and how it compares to epsilon.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a id="rest_code_495f89c934b24b3391b6cf4693d600d7-1" name="rest_code_495f89c934b24b3391b6cf4693d600d7-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;select_arm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a id="rest_code_495f89c934b24b3391b6cf4693d600d7-2" name="rest_code_495f89c934b24b3391b6cf4693d600d7-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""chooses the next arm to update&lt;/span&gt;
&lt;a id="rest_code_495f89c934b24b3391b6cf4693d600d7-3" name="rest_code_495f89c934b24b3391b6cf4693d600d7-3"&gt;&lt;/a&gt;
&lt;a id="rest_code_495f89c934b24b3391b6cf4693d600d7-4" name="rest_code_495f89c934b24b3391b6cf4693d600d7-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;a id="rest_code_495f89c934b24b3391b6cf4693d600d7-5" name="rest_code_495f89c934b24b3391b6cf4693d600d7-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     int: index of the next arm to pull&lt;/span&gt;
&lt;a id="rest_code_495f89c934b24b3391b6cf4693d600d7-6" name="rest_code_495f89c934b24b3391b6cf4693d600d7-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a id="rest_code_495f89c934b24b3391b6cf4693d600d7-7" name="rest_code_495f89c934b24b3391b6cf4693d600d7-7"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a id="rest_code_495f89c934b24b3391b6cf4693d600d7-8" name="rest_code_495f89c934b24b3391b6cf4693d600d7-8"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randrange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a id="rest_code_495f89c934b24b3391b6cf4693d600d7-9" name="rest_code_495f89c934b24b3391b6cf4693d600d7-9"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_arm&lt;/span&gt;
&lt;/pre&gt;&lt;/section&gt;
&lt;section id="update"&gt;
&lt;h3&gt;4.7 Update&lt;/h3&gt;
&lt;p&gt;The update method pulls the arm whose index it is given and then updates the count and reward.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a id="rest_code_47647fb97db5474daa92ea11ad1813dd-1" name="rest_code_47647fb97db5474daa92ea11ad1813dd-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a id="rest_code_47647fb97db5474daa92ea11ad1813dd-2" name="rest_code_47647fb97db5474daa92ea11ad1813dd-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""pulls the arm and updates the value&lt;/span&gt;
&lt;a id="rest_code_47647fb97db5474daa92ea11ad1813dd-3" name="rest_code_47647fb97db5474daa92ea11ad1813dd-3"&gt;&lt;/a&gt;
&lt;a id="rest_code_47647fb97db5474daa92ea11ad1813dd-4" name="rest_code_47647fb97db5474daa92ea11ad1813dd-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a id="rest_code_47647fb97db5474daa92ea11ad1813dd-5" name="rest_code_47647fb97db5474daa92ea11ad1813dd-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     arm (int): index of the arm to pull&lt;/span&gt;
&lt;a id="rest_code_47647fb97db5474daa92ea11ad1813dd-6" name="rest_code_47647fb97db5474daa92ea11ad1813dd-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a id="rest_code_47647fb97db5474daa92ea11ad1813dd-7" name="rest_code_47647fb97db5474daa92ea11ad1813dd-7"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;a id="rest_code_47647fb97db5474daa92ea11ad1813dd-8" name="rest_code_47647fb97db5474daa92ea11ad1813dd-8"&gt;&lt;/a&gt;    &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a id="rest_code_47647fb97db5474daa92ea11ad1813dd-9" name="rest_code_47647fb97db5474daa92ea11ad1813dd-9"&gt;&lt;/a&gt;    &lt;span class="n"&gt;average_reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a id="rest_code_47647fb97db5474daa92ea11ad1813dd-10" name="rest_code_47647fb97db5474daa92ea11ad1813dd-10"&gt;&lt;/a&gt;    &lt;span class="n"&gt;reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]()&lt;/span&gt;
&lt;a id="rest_code_47647fb97db5474daa92ea11ad1813dd-11" name="rest_code_47647fb97db5474daa92ea11ad1813dd-11"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;
&lt;a id="rest_code_47647fb97db5474daa92ea11ad1813dd-12" name="rest_code_47647fb97db5474daa92ea11ad1813dd-12"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(((&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;average_reward&lt;/span&gt;
&lt;a id="rest_code_47647fb97db5474daa92ea11ad1813dd-13" name="rest_code_47647fb97db5474daa92ea11ad1813dd-13"&gt;&lt;/a&gt;                        &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;a id="rest_code_47647fb97db5474daa92ea11ad1813dd-14" name="rest_code_47647fb97db5474daa92ea11ad1813dd-14"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/section&gt;
&lt;section id="call"&gt;
&lt;h3&gt;4.8 Call&lt;/h3&gt;
&lt;p&gt;The &lt;em&gt;__call__&lt;/em&gt; method will be the main update method that unifies the naming conventions found in the books.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a id="rest_code_55e97db74d894f119fd985c741e44ea3-1" name="rest_code_55e97db74d894f119fd985c741e44ea3-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__call__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a id="rest_code_55e97db74d894f119fd985c741e44ea3-2" name="rest_code_55e97db74d894f119fd985c741e44ea3-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""chooses an arm and updates the rewards"""&lt;/span&gt;
&lt;a id="rest_code_55e97db74d894f119fd985c741e44ea3-3" name="rest_code_55e97db74d894f119fd985c741e44ea3-3"&gt;&lt;/a&gt;    &lt;span class="n"&gt;arm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select_arm&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a id="rest_code_55e97db74d894f119fd985c741e44ea3-4" name="rest_code_55e97db74d894f119fd985c741e44ea3-4"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a id="rest_code_55e97db74d894f119fd985c741e44ea3-5" name="rest_code_55e97db74d894f119fd985c741e44ea3-5"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/section&gt;
&lt;/section&gt;
&lt;section id="epsilon-greedy-optimized"&gt;
&lt;h2&gt;5 Epsilon Greedy Optimized&lt;/h2&gt;
&lt;p&gt;It turns out that while the implementation above works correctly, it can be rather slow, given that we need to train it thousands of times to get meaningful results. This is a numba-compatible version that drops the testing time from around 11 minutes to a minute or less. One of the restrictions of using classes in numba is that you have to declare the types of all the attributes of the class (this happens in the &lt;cite&gt;spec&lt;/cite&gt; passed to the &lt;cite&gt;jitclass&lt;/cite&gt; decorator). This means that I can't pass in &lt;cite&gt;BernoulliArm&lt;/cite&gt; objects to the constructor, because &lt;cite&gt;numba&lt;/cite&gt; has no idea what they are, so this solution is a hybrid greedy algorithm and bandit arm mashed together.&lt;/p&gt;
&lt;p&gt;The documentation for &lt;cite&gt;numba&lt;/cite&gt; states that you have to initialize the attributes in the &lt;cite&gt;__init__&lt;/cite&gt; method so I'm getting rid of the properties that build the numpy arrays and moving their creation to the constructor. In addition, the code that no longer expects the =BernoulliArm= objects will have to be re-implemented. In the tangle code anything with the &lt;cite&gt;optimized-&lt;/cite&gt; prefix is re-implemented (other than the &lt;cite&gt;spec&lt;/cite&gt;), otherwise the code is being pulled in from the original &lt;cite&gt;EpsilonGreedy&lt;/cite&gt; implementation.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a id="rest_code_172fbbbcc03840eda295f97745d4b26b-1" name="rest_code_172fbbbcc03840eda295f97745d4b26b-1"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;optimized&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;imports&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a id="rest_code_172fbbbcc03840eda295f97745d4b26b-2" name="rest_code_172fbbbcc03840eda295f97745d4b26b-2"&gt;&lt;/a&gt;
&lt;a id="rest_code_172fbbbcc03840eda295f97745d4b26b-3" name="rest_code_172fbbbcc03840eda295f97745d4b26b-3"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;spec&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a id="rest_code_172fbbbcc03840eda295f97745d4b26b-4" name="rest_code_172fbbbcc03840eda295f97745d4b26b-4"&gt;&lt;/a&gt;
&lt;a id="rest_code_172fbbbcc03840eda295f97745d4b26b-5" name="rest_code_172fbbbcc03840eda295f97745d4b26b-5"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a id="rest_code_172fbbbcc03840eda295f97745d4b26b-6" name="rest_code_172fbbbcc03840eda295f97745d4b26b-6"&gt;&lt;/a&gt;&lt;span class="nd"&gt;@jitclass&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;spec&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a id="rest_code_172fbbbcc03840eda295f97745d4b26b-7" name="rest_code_172fbbbcc03840eda295f97745d4b26b-7"&gt;&lt;/a&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;EpsilonGreedyOptimized&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a id="rest_code_172fbbbcc03840eda295f97745d4b26b-8" name="rest_code_172fbbbcc03840eda295f97745d4b26b-8"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""The Epsilon Greedy Algorithm&lt;/span&gt;
&lt;a id="rest_code_172fbbbcc03840eda295f97745d4b26b-9" name="rest_code_172fbbbcc03840eda295f97745d4b26b-9"&gt;&lt;/a&gt;
&lt;a id="rest_code_172fbbbcc03840eda295f97745d4b26b-10" name="rest_code_172fbbbcc03840eda295f97745d4b26b-10"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a id="rest_code_172fbbbcc03840eda295f97745d4b26b-11" name="rest_code_172fbbbcc03840eda295f97745d4b26b-11"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     epsilon (float): fraction of the time to explore&lt;/span&gt;
&lt;a id="rest_code_172fbbbcc03840eda295f97745d4b26b-12" name="rest_code_172fbbbcc03840eda295f97745d4b26b-12"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     arms (list): collection of probabilities for bandit arm&lt;/span&gt;
&lt;a id="rest_code_172fbbbcc03840eda295f97745d4b26b-13" name="rest_code_172fbbbcc03840eda295f97745d4b26b-13"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a id="rest_code_172fbbbcc03840eda295f97745d4b26b-14" name="rest_code_172fbbbcc03840eda295f97745d4b26b-14"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;optimized&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;constructor&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a id="rest_code_172fbbbcc03840eda295f97745d4b26b-15" name="rest_code_172fbbbcc03840eda295f97745d4b26b-15"&gt;&lt;/a&gt;
&lt;a id="rest_code_172fbbbcc03840eda295f97745d4b26b-16" name="rest_code_172fbbbcc03840eda295f97745d4b26b-16"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;best&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a id="rest_code_172fbbbcc03840eda295f97745d4b26b-17" name="rest_code_172fbbbcc03840eda295f97745d4b26b-17"&gt;&lt;/a&gt;
&lt;a id="rest_code_172fbbbcc03840eda295f97745d4b26b-18" name="rest_code_172fbbbcc03840eda295f97745d4b26b-18"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;select&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a id="rest_code_172fbbbcc03840eda295f97745d4b26b-19" name="rest_code_172fbbbcc03840eda295f97745d4b26b-19"&gt;&lt;/a&gt;
&lt;a id="rest_code_172fbbbcc03840eda295f97745d4b26b-20" name="rest_code_172fbbbcc03840eda295f97745d4b26b-20"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;optimized&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;pull&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a id="rest_code_172fbbbcc03840eda295f97745d4b26b-21" name="rest_code_172fbbbcc03840eda295f97745d4b26b-21"&gt;&lt;/a&gt;
&lt;a id="rest_code_172fbbbcc03840eda295f97745d4b26b-22" name="rest_code_172fbbbcc03840eda295f97745d4b26b-22"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;optimized&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a id="rest_code_172fbbbcc03840eda295f97745d4b26b-23" name="rest_code_172fbbbcc03840eda295f97745d4b26b-23"&gt;&lt;/a&gt;
&lt;a id="rest_code_172fbbbcc03840eda295f97745d4b26b-24" name="rest_code_172fbbbcc03840eda295f97745d4b26b-24"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;optimized&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a id="rest_code_172fbbbcc03840eda295f97745d4b26b-25" name="rest_code_172fbbbcc03840eda295f97745d4b26b-25"&gt;&lt;/a&gt;
&lt;a id="rest_code_172fbbbcc03840eda295f97745d4b26b-26" name="rest_code_172fbbbcc03840eda295f97745d4b26b-26"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;call&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;section id="optimized-imports"&gt;
&lt;h3&gt;5.1 Optimized Imports&lt;/h3&gt;
&lt;pre class="code python"&gt;&lt;a id="rest_code_545a38dc711b4160b5053c4cb2d896a4-1" name="rest_code_545a38dc711b4160b5053c4cb2d896a4-1"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# python&lt;/span&gt;
&lt;a id="rest_code_545a38dc711b4160b5053c4cb2d896a4-2" name="rest_code_545a38dc711b4160b5053c4cb2d896a4-2"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt;
&lt;a id="rest_code_545a38dc711b4160b5053c4cb2d896a4-3" name="rest_code_545a38dc711b4160b5053c4cb2d896a4-3"&gt;&lt;/a&gt;
&lt;a id="rest_code_545a38dc711b4160b5053c4cb2d896a4-4" name="rest_code_545a38dc711b4160b5053c4cb2d896a4-4"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# pypi&lt;/span&gt;
&lt;a id="rest_code_545a38dc711b4160b5053c4cb2d896a4-5" name="rest_code_545a38dc711b4160b5053c4cb2d896a4-5"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numba&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
&lt;a id="rest_code_545a38dc711b4160b5053c4cb2d896a4-6" name="rest_code_545a38dc711b4160b5053c4cb2d896a4-6"&gt;&lt;/a&gt;    &lt;span class="n"&gt;jit&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a id="rest_code_545a38dc711b4160b5053c4cb2d896a4-7" name="rest_code_545a38dc711b4160b5053c4cb2d896a4-7"&gt;&lt;/a&gt;    &lt;span class="n"&gt;jitclass&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a id="rest_code_545a38dc711b4160b5053c4cb2d896a4-8" name="rest_code_545a38dc711b4160b5053c4cb2d896a4-8"&gt;&lt;/a&gt;    &lt;span class="p"&gt;)&lt;/span&gt;
&lt;a id="rest_code_545a38dc711b4160b5053c4cb2d896a4-9" name="rest_code_545a38dc711b4160b5053c4cb2d896a4-9"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numba&lt;/span&gt;
&lt;a id="rest_code_545a38dc711b4160b5053c4cb2d896a4-10" name="rest_code_545a38dc711b4160b5053c4cb2d896a4-10"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt;
&lt;/pre&gt;&lt;/section&gt;
&lt;section id="the-spec"&gt;
&lt;h3&gt;5.2 The Spec&lt;/h3&gt;
&lt;p&gt;This is how you tell numba what attributes the class will have. This is where most of the errors were when I first tried this. The error-messages aren't particularly helpful. Just be aware that this is the first place you should look if things crash.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a id="rest_code_d26f1895cc404d6291ef36256866b3d3-1" name="rest_code_d26f1895cc404d6291ef36256866b3d3-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;spec&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
&lt;a id="rest_code_d26f1895cc404d6291ef36256866b3d3-2" name="rest_code_d26f1895cc404d6291ef36256866b3d3-2"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"epsilon"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numba&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;a id="rest_code_d26f1895cc404d6291ef36256866b3d3-3" name="rest_code_d26f1895cc404d6291ef36256866b3d3-3"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"arms"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numba&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double&lt;/span&gt;&lt;span class="p"&gt;[:]),&lt;/span&gt;
&lt;a id="rest_code_d26f1895cc404d6291ef36256866b3d3-4" name="rest_code_d26f1895cc404d6291ef36256866b3d3-4"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"counts"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numba&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double&lt;/span&gt;&lt;span class="p"&gt;[:]),&lt;/span&gt;
&lt;a id="rest_code_d26f1895cc404d6291ef36256866b3d3-5" name="rest_code_d26f1895cc404d6291ef36256866b3d3-5"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"rewards"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numba&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double&lt;/span&gt;&lt;span class="p"&gt;[:]),&lt;/span&gt;
&lt;a id="rest_code_d26f1895cc404d6291ef36256866b3d3-6" name="rest_code_d26f1895cc404d6291ef36256866b3d3-6"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"total_reward"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numba&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int64&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;a id="rest_code_d26f1895cc404d6291ef36256866b3d3-7" name="rest_code_d26f1895cc404d6291ef36256866b3d3-7"&gt;&lt;/a&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/section&gt;
&lt;section id="id2"&gt;
&lt;h3&gt;5.3 The Constructor&lt;/h3&gt;
&lt;p&gt;The constructor takes two arguments - &lt;em&gt;epsilon&lt;/em&gt; and &lt;em&gt;arms&lt;/em&gt;. The &lt;em&gt;arms&lt;/em&gt; list should contain probabilities that a reward or penalty will be returned when pulled.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a id="rest_code_b805c8f25b5c49c39f33b7497908ec81-1" name="rest_code_b805c8f25b5c49c39f33b7497908ec81-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a id="rest_code_b805c8f25b5c49c39f33b7497908ec81-2" name="rest_code_b805c8f25b5c49c39f33b7497908ec81-2"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;
&lt;a id="rest_code_b805c8f25b5c49c39f33b7497908ec81-3" name="rest_code_b805c8f25b5c49c39f33b7497908ec81-3"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;arms&lt;/span&gt;
&lt;a id="rest_code_b805c8f25b5c49c39f33b7497908ec81-4" name="rest_code_b805c8f25b5c49c39f33b7497908ec81-4"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a id="rest_code_b805c8f25b5c49c39f33b7497908ec81-5" name="rest_code_b805c8f25b5c49c39f33b7497908ec81-5"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a id="rest_code_b805c8f25b5c49c39f33b7497908ec81-6" name="rest_code_b805c8f25b5c49c39f33b7497908ec81-6"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;a id="rest_code_b805c8f25b5c49c39f33b7497908ec81-7" name="rest_code_b805c8f25b5c49c39f33b7497908ec81-7"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/section&gt;
&lt;section id="id3"&gt;
&lt;h3&gt;5.4 Reset&lt;/h3&gt;
&lt;pre class="code python"&gt;&lt;a id="rest_code_c680a14e2797444d9d1e3b047b3fce29-1" name="rest_code_c680a14e2797444d9d1e3b047b3fce29-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a id="rest_code_c680a14e2797444d9d1e3b047b3fce29-2" name="rest_code_c680a14e2797444d9d1e3b047b3fce29-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""sets the counts, rewards, total_reward to 0s&lt;/span&gt;
&lt;a id="rest_code_c680a14e2797444d9d1e3b047b3fce29-3" name="rest_code_c680a14e2797444d9d1e3b047b3fce29-3"&gt;&lt;/a&gt;
&lt;a id="rest_code_c680a14e2797444d9d1e3b047b3fce29-4" name="rest_code_c680a14e2797444d9d1e3b047b3fce29-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    This lets you re-used the EpsilonGreedy&lt;/span&gt;
&lt;a id="rest_code_c680a14e2797444d9d1e3b047b3fce29-5" name="rest_code_c680a14e2797444d9d1e3b047b3fce29-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a id="rest_code_c680a14e2797444d9d1e3b047b3fce29-6" name="rest_code_c680a14e2797444d9d1e3b047b3fce29-6"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a id="rest_code_c680a14e2797444d9d1e3b047b3fce29-7" name="rest_code_c680a14e2797444d9d1e3b047b3fce29-7"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a id="rest_code_c680a14e2797444d9d1e3b047b3fce29-8" name="rest_code_c680a14e2797444d9d1e3b047b3fce29-8"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;a id="rest_code_c680a14e2797444d9d1e3b047b3fce29-9" name="rest_code_c680a14e2797444d9d1e3b047b3fce29-9"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/section&gt;
&lt;section id="pull-arm"&gt;
&lt;h3&gt;5.5 Pull Arm&lt;/h3&gt;
&lt;p&gt;Since we can't give user-defined objects as attributes of the class, this version will be both algorithm and bandit.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a id="rest_code_f224c17122d549b0993b211e7c9d6aad-1" name="rest_code_f224c17122d549b0993b211e7c9d6aad-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;pull_arm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a id="rest_code_f224c17122d549b0993b211e7c9d6aad-2" name="rest_code_f224c17122d549b0993b211e7c9d6aad-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""gets the reward&lt;/span&gt;
&lt;a id="rest_code_f224c17122d549b0993b211e7c9d6aad-3" name="rest_code_f224c17122d549b0993b211e7c9d6aad-3"&gt;&lt;/a&gt;
&lt;a id="rest_code_f224c17122d549b0993b211e7c9d6aad-4" name="rest_code_f224c17122d549b0993b211e7c9d6aad-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a id="rest_code_f224c17122d549b0993b211e7c9d6aad-5" name="rest_code_f224c17122d549b0993b211e7c9d6aad-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     arm (int): index for the arm-probability array&lt;/span&gt;
&lt;a id="rest_code_f224c17122d549b0993b211e7c9d6aad-6" name="rest_code_f224c17122d549b0993b211e7c9d6aad-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;a id="rest_code_f224c17122d549b0993b211e7c9d6aad-7" name="rest_code_f224c17122d549b0993b211e7c9d6aad-7"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     int: reward or no reward&lt;/span&gt;
&lt;a id="rest_code_f224c17122d549b0993b211e7c9d6aad-8" name="rest_code_f224c17122d549b0993b211e7c9d6aad-8"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a id="rest_code_f224c17122d549b0993b211e7c9d6aad-9" name="rest_code_f224c17122d549b0993b211e7c9d6aad-9"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
&lt;a id="rest_code_f224c17122d549b0993b211e7c9d6aad-10" name="rest_code_f224c17122d549b0993b211e7c9d6aad-10"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;a id="rest_code_f224c17122d549b0993b211e7c9d6aad-11" name="rest_code_f224c17122d549b0993b211e7c9d6aad-11"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/section&gt;
&lt;section id="id4"&gt;
&lt;h3&gt;5.6 Update&lt;/h3&gt;
&lt;p&gt;The update method pulls the arm whose index it is given and then updates the count and reward. Here we're calling the &lt;code class="docutils literal"&gt;pull_arm&lt;/code&gt; method instead of using a &lt;code class="docutils literal"&gt;BernoulliArm&lt;/code&gt; so we can't re-use the original method.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a id="rest_code_b38215c0f4254868901ae8487ccc4254-1" name="rest_code_b38215c0f4254868901ae8487ccc4254-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a id="rest_code_b38215c0f4254868901ae8487ccc4254-2" name="rest_code_b38215c0f4254868901ae8487ccc4254-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""pulls the arm and updates the value&lt;/span&gt;
&lt;a id="rest_code_b38215c0f4254868901ae8487ccc4254-3" name="rest_code_b38215c0f4254868901ae8487ccc4254-3"&gt;&lt;/a&gt;
&lt;a id="rest_code_b38215c0f4254868901ae8487ccc4254-4" name="rest_code_b38215c0f4254868901ae8487ccc4254-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a id="rest_code_b38215c0f4254868901ae8487ccc4254-5" name="rest_code_b38215c0f4254868901ae8487ccc4254-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     arm (int): index of the arm to pull&lt;/span&gt;
&lt;a id="rest_code_b38215c0f4254868901ae8487ccc4254-6" name="rest_code_b38215c0f4254868901ae8487ccc4254-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a id="rest_code_b38215c0f4254868901ae8487ccc4254-7" name="rest_code_b38215c0f4254868901ae8487ccc4254-7"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;a id="rest_code_b38215c0f4254868901ae8487ccc4254-8" name="rest_code_b38215c0f4254868901ae8487ccc4254-8"&gt;&lt;/a&gt;    &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a id="rest_code_b38215c0f4254868901ae8487ccc4254-9" name="rest_code_b38215c0f4254868901ae8487ccc4254-9"&gt;&lt;/a&gt;    &lt;span class="n"&gt;average_reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a id="rest_code_b38215c0f4254868901ae8487ccc4254-10" name="rest_code_b38215c0f4254868901ae8487ccc4254-10"&gt;&lt;/a&gt;    &lt;span class="n"&gt;reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pull_arm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a id="rest_code_b38215c0f4254868901ae8487ccc4254-11" name="rest_code_b38215c0f4254868901ae8487ccc4254-11"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;
&lt;a id="rest_code_b38215c0f4254868901ae8487ccc4254-12" name="rest_code_b38215c0f4254868901ae8487ccc4254-12"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(((&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;average_reward&lt;/span&gt;
&lt;a id="rest_code_b38215c0f4254868901ae8487ccc4254-13" name="rest_code_b38215c0f4254868901ae8487ccc4254-13"&gt;&lt;/a&gt;                        &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;a id="rest_code_b38215c0f4254868901ae8487ccc4254-14" name="rest_code_b38215c0f4254868901ae8487ccc4254-14"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/section&gt;
&lt;/section&gt;
&lt;section id="references"&gt;
&lt;h2&gt;6 References&lt;/h2&gt;
&lt;dl class="footnote brackets"&gt;
&lt;dt class="label" id="id5"&gt;&lt;span class="brackets"&gt;&lt;a class="fn-backref" href="https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/The-Epsilon-Greedy-Algorithm/#id1"&gt;1&lt;/a&gt;&lt;/span&gt;&lt;/dt&gt;
&lt;dd&gt;&lt;p&gt;Bandit Algorithms for Website Optimization by John Myles White. Copyright 2013 John Myles White, 978-1-449-34133-6&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/section&gt;&lt;/div&gt;</description><category>algorithm reinforcementlearning</category><guid>https://necromuralist.github.io/Bandits-and-Pigeon-Bombs/posts/The-Epsilon-Greedy-Algorithm/</guid><pubDate>Mon, 31 Jul 2017 01:22:00 GMT</pubDate></item></channel></rss>