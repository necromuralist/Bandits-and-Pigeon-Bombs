<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="The Optimistic Initial Values agent.">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Optimistic Initial Values | Reinforcement Learning</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="https://necromuralist.github.io/data_science/posts/Optimistic-Initial-Values/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script><meta name="author" content="Brunhilde">
<link rel="prev" href="../assessing-the-performance/" title="Assessing the Performance" type="text/html">
<link rel="next" href="../Epsilon-Greedy-with-Normal-Distribution-Payouts/" title="Epsilon Greedy with Normal-Distribution Payouts" type="text/html">
<meta property="og:site_name" content="Reinforcement Learning">
<meta property="og:title" content="Optimistic Initial Values">
<meta property="og:url" content="https://necromuralist.github.io/data_science/posts/Optimistic-Initial-Values/">
<meta property="og:description" content="The Optimistic Initial Values agent.">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2017-08-01T18:47:00-07:00">
<meta property="article:tag" content="bandits reinforcementLearning">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://necromuralist.github.io/data_science/">

                <span id="blog-title">Reinforcement Learning</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="../../archive.html">Archive</a>
                </li>
<li>
<a href="../../categories/">Tags</a>
                </li>
<li>
<a href="../../rss.xml">RSS feed</a>

                
            </li>
</ul>
<!-- Google custom search --><form method="get" action="https://www.google.com/search" class="navbar-form
 navbar-right" role="search">
<div class="form-group">
<input type="text" name="q" class="form-control" placeholder="Search">
</div>
<button type="submit" class="btn btn-primary">
<span class="glyphicon glyphicon-search"></span>
</button>
<input type="hidden" name="sitesearch" value="https://necromuralist.github.io/data_science/">
</form>
<!-- End of custom search -->


            <ul class="nav navbar-nav navbar-right">
<li>
    <a href="index.rst" id="sourcelink">Source</a>
    </li>

                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Optimistic Initial Values</a></h1>

        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                    <a href="../../authors/brunhilde/">Brunhilde</a>
            </span></p>
            <p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2017-08-01T18:47:00-07:00" itemprop="datePublished" title="2017-08-01 18:47">2017-08-01 18:47</time></a></p>
            
        <p class="sourceline"><a href="index.rst" class="sourcelink">Source</a></p>

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<div class="section" id="introduction">
<h2>1 Introduction</h2>
<p>This is one possible to the n-armed bandit problem. It is similar to the <em>Epsilon Greedy</em> algorithm except that instead of using a conditional to decide whether to explore or exploit, the algorithm sets the estimated (mean) payout for each arm to 1 (the theoretical maximum for our case) and then always exploits. As things proceed, the arms will settle down to their actual payoff-rates and those that haven't been explored will be chosen because they are still too high.</p>
</div>
<div class="section" id="the-tangle">
<h2>2 The Tangle</h2>
<p>This is the no-web template to build the final file.</p>
<pre class="code python"><a name="rest_code_0a5c592cc0f146989fccf29434c94b40-1"></a><span class="o">&lt;&lt;</span><span class="n">imports</span><span class="o">&gt;&gt;</span>
<a name="rest_code_0a5c592cc0f146989fccf29434c94b40-2"></a>
<a name="rest_code_0a5c592cc0f146989fccf29434c94b40-3"></a><span class="o">&lt;&lt;</span><span class="n">spec</span><span class="o">&gt;&gt;</span>
<a name="rest_code_0a5c592cc0f146989fccf29434c94b40-4"></a>
<a name="rest_code_0a5c592cc0f146989fccf29434c94b40-5"></a><span class="o">&lt;&lt;</span><span class="n">class</span><span class="o">-</span><span class="n">declaration</span><span class="o">&gt;&gt;</span>
<a name="rest_code_0a5c592cc0f146989fccf29434c94b40-6"></a>
<a name="rest_code_0a5c592cc0f146989fccf29434c94b40-7"></a>    <span class="o">&lt;&lt;</span><span class="n">constructor</span><span class="o">&gt;&gt;</span>
<a name="rest_code_0a5c592cc0f146989fccf29434c94b40-8"></a>
<a name="rest_code_0a5c592cc0f146989fccf29434c94b40-9"></a>    <span class="o">&lt;&lt;</span><span class="n">select</span><span class="o">-</span><span class="n">arm</span><span class="o">&gt;&gt;</span>
<a name="rest_code_0a5c592cc0f146989fccf29434c94b40-10"></a>
<a name="rest_code_0a5c592cc0f146989fccf29434c94b40-11"></a>    <span class="o">&lt;&lt;</span><span class="n">pull</span><span class="o">-</span><span class="n">arm</span><span class="o">&gt;&gt;</span>
<a name="rest_code_0a5c592cc0f146989fccf29434c94b40-12"></a>
<a name="rest_code_0a5c592cc0f146989fccf29434c94b40-13"></a>    <span class="o">&lt;&lt;</span><span class="n">update</span><span class="o">-</span><span class="n">arm</span><span class="o">&gt;&gt;</span>
<a name="rest_code_0a5c592cc0f146989fccf29434c94b40-14"></a>
<a name="rest_code_0a5c592cc0f146989fccf29434c94b40-15"></a>    <span class="o">&lt;&lt;</span><span class="n">reset</span><span class="o">&gt;&gt;</span>
</pre>
</div>
<div class="section" id="imports">
<h2>3 Imports</h2>
<p>These are our external dependencies.</p>
<pre class="code python"><a name="rest_code_f5591b28c7a24b78bc9cb6ecffb32b52-1"></a><span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">jitclass</span>
<a name="rest_code_f5591b28c7a24b78bc9cb6ecffb32b52-2"></a><span class="kn">import</span> <span class="nn">numba</span>
<a name="rest_code_f5591b28c7a24b78bc9cb6ecffb32b52-3"></a><span class="kn">import</span> <span class="nn">numpy</span>
</pre>
</div>
<div class="section" id="the-spec">
<h2>4 The Spec</h2>
<p>In order to use numba with the <tt class="docutils literal">OptimisticInitialValues</tt> class you have to create a 'spec' that tells numba what the data-types are for each of its fields.</p>
<pre class="code python"><a name="rest_code_cc215b8b3c754a6a9e5251f8f096849f-1"></a><span class="n">SPEC</span> <span class="o">=</span> <span class="p">[</span>
<a name="rest_code_cc215b8b3c754a6a9e5251f8f096849f-2"></a>    <span class="p">(</span><span class="s2">"arms"</span><span class="p">,</span> <span class="n">numba</span><span class="o">.</span><span class="n">double</span><span class="p">[:]),</span>
<a name="rest_code_cc215b8b3c754a6a9e5251f8f096849f-3"></a>    <span class="p">(</span><span class="s2">"counts"</span><span class="p">,</span> <span class="n">numba</span><span class="o">.</span><span class="n">double</span><span class="p">[:]),</span>
<a name="rest_code_cc215b8b3c754a6a9e5251f8f096849f-4"></a>    <span class="p">(</span><span class="s2">"rewards"</span><span class="p">,</span> <span class="n">numba</span><span class="o">.</span><span class="n">double</span><span class="p">[:]),</span>
<a name="rest_code_cc215b8b3c754a6a9e5251f8f096849f-5"></a>    <span class="p">(</span><span class="s2">"total_reward"</span><span class="p">,</span> <span class="n">numba</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
<a name="rest_code_cc215b8b3c754a6a9e5251f8f096849f-6"></a>    <span class="p">(</span><span class="s2">"initial_reward"</span><span class="p">,</span> <span class="n">numba</span><span class="o">.</span><span class="n">double</span><span class="p">),</span>
<a name="rest_code_cc215b8b3c754a6a9e5251f8f096849f-7"></a><span class="p">]</span>
</pre>
</div>
<div class="section" id="the-class-declaration">
<h2>5 The Class Declaration</h2>
<pre class="code python"><a name="rest_code_62b8932bad814b5fb42d50916fc9cac0-1"></a><span class="nd">@jitclass</span><span class="p">(</span><span class="n">SPEC</span><span class="p">)</span>
<a name="rest_code_62b8932bad814b5fb42d50916fc9cac0-2"></a><span class="k">class</span> <span class="nc">OptimisticInitialValues</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
<a name="rest_code_62b8932bad814b5fb42d50916fc9cac0-3"></a>    <span class="sd">"""Optimistic Initial Values greedy algorithm</span>
<a name="rest_code_62b8932bad814b5fb42d50916fc9cac0-4"></a>
<a name="rest_code_62b8932bad814b5fb42d50916fc9cac0-5"></a><span class="sd">    Args:</span>
<a name="rest_code_62b8932bad814b5fb42d50916fc9cac0-6"></a><span class="sd">     numpy.array[float]: payout-probabilities for each arm</span>
<a name="rest_code_62b8932bad814b5fb42d50916fc9cac0-7"></a><span class="sd">    """</span>
</pre>
</div>
<div class="section" id="the-constructor">
<h2>6 The Constructor</h2>
<p>Here's our first change from the epsilon-greedy algorithm. We no longer have an <tt class="docutils literal">epsilon</tt> value and instead of initializing the <tt class="docutils literal">rewards</tt> as zeros we initialize them with an 'initial' reward. Also, although you can't see it here, the arms have to be a list of mean payout values (see the <tt class="docutils literal">pull_arm</tt> method below).</p>
<pre class="code python"><a name="rest_code_14e45d4a2ef049dcab77ef6ebd03eeab-1"></a><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arms</span><span class="p">,</span> <span class="n">initial_reward</span><span class="p">):</span>
<a name="rest_code_14e45d4a2ef049dcab77ef6ebd03eeab-2"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">arms</span> <span class="o">=</span> <span class="n">arms</span>
<a name="rest_code_14e45d4a2ef049dcab77ef6ebd03eeab-3"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">counts</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">arms</span><span class="p">))</span>
<a name="rest_code_14e45d4a2ef049dcab77ef6ebd03eeab-4"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">arms</span><span class="p">))</span> <span class="o">+</span> <span class="n">initial_reward</span>
<a name="rest_code_14e45d4a2ef049dcab77ef6ebd03eeab-5"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>
<a name="rest_code_14e45d4a2ef049dcab77ef6ebd03eeab-6"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">initial_reward</span> <span class="o">=</span> <span class="n">initial_reward</span>
<a name="rest_code_14e45d4a2ef049dcab77ef6ebd03eeab-7"></a>    <span class="k">return</span>
</pre>
</div>
<div class="section" id="select-arm">
<h2>7 Select Arm</h2>
<p>This chooses the next arm. Unlike the epsilon-greedy algorithm it will always pick the 'best' arm, choosing the first if there is a tie. Since the whole class is in the jit I'm also not using the external <tt class="docutils literal">find_first</tt> method.</p>
<pre class="code python"><a name="rest_code_39ef147cbf0549cba23bda355dc98556-1"></a><span class="k">def</span> <span class="nf">select_arm</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<a name="rest_code_39ef147cbf0549cba23bda355dc98556-2"></a>    <span class="sd">"""Index of the arm with the most reward</span>
<a name="rest_code_39ef147cbf0549cba23bda355dc98556-3"></a>
<a name="rest_code_39ef147cbf0549cba23bda355dc98556-4"></a><span class="sd">    Returns:</span>
<a name="rest_code_39ef147cbf0549cba23bda355dc98556-5"></a><span class="sd">     integer: index of arm with highest average reward</span>
<a name="rest_code_39ef147cbf0549cba23bda355dc98556-6"></a><span class="sd">    """</span>
<a name="rest_code_39ef147cbf0549cba23bda355dc98556-7"></a>    <span class="n">item</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<a name="rest_code_39ef147cbf0549cba23bda355dc98556-8"></a>    <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">)):</span>
<a name="rest_code_39ef147cbf0549cba23bda355dc98556-9"></a>        <span class="k">if</span> <span class="n">item</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">[</span><span class="n">index</span><span class="p">]:</span>
<a name="rest_code_39ef147cbf0549cba23bda355dc98556-10"></a>            <span class="k">return</span> <span class="n">index</span>
</pre>
</div>
<div class="section" id="pull-arm">
<h2>8 Pull Arm</h2>
<p>This gets the reward for the arm. with a Bernoulli arm, there's a chance that an arm will be set to 0 on its first pull, at which point you will never explore it (since there's no exploration), so even the best arm might get wiped out. To fix this you need a different scheme. This one uses a population mean (selected <tt class="docutils literal">from self.arms</tt>) which has noise added by selecting from the standard normal distribution.</p>
<pre class="code python"><a name="rest_code_a569b19151fe4458a844f91ec8efa77c-1"></a><span class="k">def</span> <span class="nf">pull_arm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arm</span><span class="p">):</span>
<a name="rest_code_a569b19151fe4458a844f91ec8efa77c-2"></a>    <span class="sd">"""gets the reward</span>
<a name="rest_code_a569b19151fe4458a844f91ec8efa77c-3"></a>
<a name="rest_code_a569b19151fe4458a844f91ec8efa77c-4"></a><span class="sd">    Args:</span>
<a name="rest_code_a569b19151fe4458a844f91ec8efa77c-5"></a><span class="sd">     arm (int): index for the arm population-mean array</span>
<a name="rest_code_a569b19151fe4458a844f91ec8efa77c-6"></a><span class="sd">    Returns:</span>
<a name="rest_code_a569b19151fe4458a844f91ec8efa77c-7"></a><span class="sd">     float: payout for the arm</span>
<a name="rest_code_a569b19151fe4458a844f91ec8efa77c-8"></a><span class="sd">    """</span>
<a name="rest_code_a569b19151fe4458a844f91ec8efa77c-9"></a>    <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">arms</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
</pre>
</div>
<div class="section" id="update-arm">
<h2>9 Update Arm</h2>
<p>This pulls the arm and updates the reward. This works the same as the <tt class="docutils literal"><span class="pre">epsilon-greedy</span></tt> version does.</p>
<pre class="code python"><a name="rest_code_acfef257ad854c32ac7657132f6e0b62-1"></a><span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arm</span><span class="p">):</span>
<a name="rest_code_acfef257ad854c32ac7657132f6e0b62-2"></a>    <span class="sd">"""pulls the arm and updates the average reward</span>
<a name="rest_code_acfef257ad854c32ac7657132f6e0b62-3"></a>
<a name="rest_code_acfef257ad854c32ac7657132f6e0b62-4"></a><span class="sd">    also updates the total_reward the algorithm has earned so far</span>
<a name="rest_code_acfef257ad854c32ac7657132f6e0b62-5"></a>
<a name="rest_code_acfef257ad854c32ac7657132f6e0b62-6"></a><span class="sd">    Args:</span>
<a name="rest_code_acfef257ad854c32ac7657132f6e0b62-7"></a><span class="sd">     arm (int): index of the arm to pull</span>
<a name="rest_code_acfef257ad854c32ac7657132f6e0b62-8"></a><span class="sd">    """</span>
<a name="rest_code_acfef257ad854c32ac7657132f6e0b62-9"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">counts</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
<a name="rest_code_acfef257ad854c32ac7657132f6e0b62-10"></a>    <span class="n">count</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">counts</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
<a name="rest_code_acfef257ad854c32ac7657132f6e0b62-11"></a>    <span class="n">average_reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
<a name="rest_code_acfef257ad854c32ac7657132f6e0b62-12"></a>    <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pull_arm</span><span class="p">(</span><span class="n">arm</span><span class="p">)</span>
<a name="rest_code_acfef257ad854c32ac7657132f6e0b62-13"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">total_reward</span> <span class="o">+=</span> <span class="n">reward</span>
<a name="rest_code_acfef257ad854c32ac7657132f6e0b62-14"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">=</span> <span class="p">(((</span><span class="n">count</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">count</span><span class="p">))</span> <span class="o">*</span> <span class="n">average_reward</span>
<a name="rest_code_acfef257ad854c32ac7657132f6e0b62-15"></a>                         <span class="o">+</span> <span class="p">(</span><span class="n">reward</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">count</span><span class="p">)))</span>
<a name="rest_code_acfef257ad854c32ac7657132f6e0b62-16"></a>    <span class="k">return</span>
</pre>
</div>
<div class="section" id="reset">
<h2>10 Reset</h2>
<p>This resets the values so that you can re-use the algorithm. As with the constructor, it sets the <tt class="docutils literal">rewards</tt> to all ones instead of zeros as was the case with the epsilon-greedy algorithm.</p>
<pre class="code python"><a name="rest_code_88ce437926164e28b1581cf36da7f9ce-1"></a><span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<a name="rest_code_88ce437926164e28b1581cf36da7f9ce-2"></a>    <span class="sd">"""sets the counts, rewards, total_reward to 0s</span>
<a name="rest_code_88ce437926164e28b1581cf36da7f9ce-3"></a>
<a name="rest_code_88ce437926164e28b1581cf36da7f9ce-4"></a><span class="sd">    This lets you re-used the EpsilonGreedy</span>
<a name="rest_code_88ce437926164e28b1581cf36da7f9ce-5"></a><span class="sd">    """</span>
<a name="rest_code_88ce437926164e28b1581cf36da7f9ce-6"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">counts</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">arms</span><span class="p">))</span>
<a name="rest_code_88ce437926164e28b1581cf36da7f9ce-7"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">arms</span><span class="p">))</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">initial_reward</span>
<a name="rest_code_88ce437926164e28b1581cf36da7f9ce-8"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>
<a name="rest_code_88ce437926164e28b1581cf36da7f9ce-9"></a>    <span class="k">return</span>
</pre>
</div>
</div>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/bandits-reinforcementlearning/" rel="tag">bandits reinforcementLearning</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../assessing-the-performance/" rel="prev" title="Assessing the Performance">Previous post</a>
            </li>
            <li class="next">
                <a href="../Epsilon-Greedy-with-Normal-Distribution-Payouts/" rel="next" title="Epsilon Greedy with Normal-Distribution Payouts">Next post</a>
            </li>
        </ul></nav></aside></article>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents Â© 2018         <a href="mailto:necromuralist@gmail.com">necromuralist</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
<a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
<img alt="Creative Commons License BY-NC-SA" style="border-width:0; margin-bottom:12px;" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png"></a>
            
        </footer>
</div>
</div>


            <script src="../../assets/js/all-nocdn.js"></script><script>$('a.image-reference:not(.islink) img:not(.islink)').parent().colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates -->
</body>
</html>
