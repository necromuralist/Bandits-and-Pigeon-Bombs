<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Reinforcement Learning Notes</title><link>https://necromuralist.github.io/reinforcement_learning/</link><description>Notes while learning reinforcement learning.</description><atom:link href="https://necromuralist.github.io/reinforcement_learning/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Mon, 09 Jul 2018 00:33:38 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Teaching A Robot To Walk</title><link>https://necromuralist.github.io/reinforcement_learning/posts/teaching-a-robot-to-walk/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="outline-container-org6b5e40d" class="outline-2"&gt;
&lt;h2 id="org6b5e40d"&gt;Imports&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org6b5e40d"&gt;
&lt;p&gt;
We need to import &lt;a href="https://gym.openai.com/"&gt;openai-gym&lt;/a&gt; (as gym).
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# from pypi&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;gym&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org1a5fd34" class="outline-2"&gt;
&lt;h2 id="org1a5fd34"&gt;Setting Up the Environment&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org1a5fd34"&gt;
&lt;p&gt;
You build the environment with &lt;code&gt;gym.make&lt;/code&gt;. In this case we're going to use the &lt;a href="https://gym.openai.com/envs/BipedalWalker-v2/"&gt;BipedalWalker-v2&lt;/a&gt;, which the documentation describes like this:
&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;
Reward is given for moving forward, total 300+ points up to the far end. If the robot falls, it gets -100. Applying motor torque costs a small amount of points, more optimal agent will get better score. State consists of hull angle speed, angular velocity, horizontal speed, vertical speed, position of joints and joints angular speed, legs contact with ground, and 10 lidar rangefinder measurements. There's no coordinates in the state vector.
&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;environment&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gym&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;make&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"BipedalWalker-v2"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgc9a2f4f" class="outline-2"&gt;
&lt;h2 id="orgc9a2f4f"&gt;Running It&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgc9a2f4f"&gt;
&lt;p&gt;
We're going to run this for 10 episodes (each run from start to finish is an episode), resetting the environment for each episode. Then within an episode we're going to have the environment run up to 1,000 steps (a time-step that updates the environment). If it falls down or walks off screen the environment will indicate that it's done.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;EPISODES&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;STEPS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;episode&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;EPISODES&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;observation&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;environment&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;step&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;STEPS&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	&lt;span class="n"&gt;environment&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;render&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
	&lt;span class="n"&gt;action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;environment&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;action_space&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
	&lt;span class="n"&gt;observation&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;info&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;environment&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"It took {} steps to finish this episode."&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
	    &lt;span class="k"&gt;break&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
It only finished five out of the ten times, probably by falling down each time. Note that I didn't implement any logic to teach the robot to walk, this is all run with built-in functionality.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>openai</category><guid>https://necromuralist.github.io/reinforcement_learning/posts/teaching-a-robot-to-walk/</guid><pubDate>Sat, 07 Jul 2018 20:42:15 GMT</pubDate></item><item><title>One-Dimensional World</title><link>https://necromuralist.github.io/reinforcement_learning/posts/one-dimensional-world/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="outline-container-org5b49773" class="outline-2"&gt;
&lt;h2 id="org5b49773"&gt;Background&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org5b49773"&gt;
&lt;p&gt;
This is take from MorvanZhou's &lt;a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow"&gt;github repository&lt;/a&gt;. It uses Q-learning to train an agent to explore a one-dimensional world that has the reward-state at the end of the world.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgb3113cb" class="outline-2"&gt;
&lt;h2 id="orgb3113cb"&gt;Tangle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgb3113cb"&gt;
&lt;p&gt;
This is the no-web tangle block. It shows how the code is organized.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;imports&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;constants&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;Q&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;Learner&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;episodes&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;terminal&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;environment&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;call&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;environment&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;goal&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;reached&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;act&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org474dbc0" class="outline-2"&gt;
&lt;h2 id="org474dbc0"&gt;Imports&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org474dbc0"&gt;
&lt;p&gt;
The only non-python-standard library requirements are numpy and pandas.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# python standard library&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt;

&lt;span class="c1"&gt;# from pypi&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org61cd562" class="outline-2"&gt;
&lt;h2 id="org61cd562"&gt;Constants&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org61cd562"&gt;
&lt;p&gt;
The &lt;code&gt;Actions&lt;/code&gt; class holds the strings used for the choice of actions. I tend to change my mind about names so I made a central object to store the strings.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Actions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;actions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'Left'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'Right'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;move_left&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'Left'&lt;/span&gt;
    &lt;span class="n"&gt;move_right&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'Right'&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgfbad3bd" class="outline-2"&gt;
&lt;h2 id="orgfbad3bd"&gt;Q-Learner&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgfbad3bd"&gt;
&lt;p&gt;
This is what will run the reinforcement learning.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;QLearner&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""Runs the reinforcement learning&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;     episodes: Number of time to repeat training&lt;/span&gt;
&lt;span class="sd"&gt;     start_state (int): where to start the agent&lt;/span&gt;
&lt;span class="sd"&gt;     states (int): total size for the environment&lt;/span&gt;
&lt;span class="sd"&gt;     terminal (int): where the goal state is&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;episodes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;start_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;states&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;terminal&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_episodes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;episodes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;episodes&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;start_state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;start_state&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;states&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;states&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_terminal&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;terminal&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_environment&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_agent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
	&lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga28526a" class="outline-3"&gt;
&lt;h3 id="orga28526a"&gt;Terminal&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga28526a"&gt;
&lt;p&gt;
This is the goal state. If it isn't set then it will default to being the rightmost state.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nd"&gt;@property&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;terminal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""Goal state&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;     int: zero-based index of the goal-state&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_terminal&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_terminal&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;states&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_terminal&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga2b4f3b" class="outline-3"&gt;
&lt;h3 id="orga2b4f3b"&gt;Episodes&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga2b4f3b"&gt;
&lt;p&gt;
An 'episode' is a single session that is run until the agent reaches the goal. To improve its performance, you can run it over multiple episodes so it can keep learning.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nd"&gt;@property&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;episodes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""the episodes iterator&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;     range: (1, episodes + 1)&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_episodes&lt;/span&gt;

&lt;span class="nd"&gt;@episodes.setter&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;episodes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;episode_count&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""creates the episodes iterator&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;     episode_count(int): number of episodes to train the agent&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_episodes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;episode_count&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgf86e99c" class="outline-3"&gt;
&lt;h3 id="orgf86e99c"&gt;The Q-Environment&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgf86e99c"&gt;
&lt;p&gt;
I couldn't decide who should build the agent and the environment so I had the Q-Learner do it.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nd"&gt;@property&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;environment&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""The Environment for the agent&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;     Environment: one-dimensional environment&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_environment&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_environment&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Environment&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;start_state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
					&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;states&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
					&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;terminal&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_environment&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org96bbf70" class="outline-3"&gt;
&lt;h3 id="org96bbf70"&gt;The Q-Agent&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org96bbf70"&gt;
&lt;p&gt;
Since the Q-Learner is building the environment I'm going to make it build the Agent too.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nd"&gt;@property&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;agent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""The agent that explores the environment&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;     Agent: agent built for the environment&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_agent&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_agent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Agent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environment&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_agent&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org4478e46" class="outline-3"&gt;
&lt;h3 id="org4478e46"&gt;Call&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org4478e46"&gt;
&lt;p&gt;
This runs the episodes.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__call__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""runs the episodes to train the agent in the environment&lt;/span&gt;

&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;episode&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;episodes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="n"&gt;counter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environment&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environment&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;episode&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;counter&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	&lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environment&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;goal_reached&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;act&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
	    &lt;span class="n"&gt;counter&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
	    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environment&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;episode&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;counter&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org3d38f7e" class="outline-2"&gt;
&lt;h2 id="org3d38f7e"&gt;The Environment&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org3d38f7e"&gt;
&lt;p&gt;
This will hold the environment for the agent to explore.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Environment&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""The environment to explore&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;     start_state(int): where the agent will start&lt;/span&gt;
&lt;span class="sd"&gt;     states (int): the size of the world&lt;/span&gt;
&lt;span class="sd"&gt;     terminal (int): where the target state is&lt;/span&gt;
&lt;span class="sd"&gt;     output_pause (float): seconds to sleep after printing to the screen&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;start_state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;states&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;terminal&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_pause&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;start_state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;start_state&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;start_state&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next_state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;start_state&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;states&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;states&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;terminal&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;terminal&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_pause&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;output_pause&lt;/span&gt;
	&lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orga31a631" class="outline-3"&gt;
&lt;h3 id="orga31a631"&gt;Goal Reached&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga31a631"&gt;
&lt;p&gt;
This is used both to update the q-table and to decide whether to quit the episode. Because updating the q-table requires both the current and next states, it is based on the next state, with the assumption that it will be updated before the next episode
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nd"&gt;@property&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;goal_reached&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""Checks if the next-state is the goal&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;     bool: True if next-state is the goal&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next_state&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;terminal&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgb62d9b7" class="outline-3"&gt;
&lt;h3 id="orgb62d9b7"&gt;Evaluate Action&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb62d9b7"&gt;
&lt;p&gt;
This will check if the &lt;code&gt;action&lt;/code&gt; will lead to the goal and decide the reward to give for the action. It also sets the &lt;code&gt;next_state&lt;/code&gt; property based on the current &lt;code&gt;state&lt;/code&gt; and the chosen &lt;code&gt;action&lt;/code&gt;.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""Checks if the action will lead to the goal&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;     action (str): one of the actions to explore the environment&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;     int: 1 if this will lead to the goal, 0 otherwise&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;Actions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;move_right&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next_state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next_state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next_state&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;terminal&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orge2cc630" class="outline-3"&gt;
&lt;h3 id="orge2cc630"&gt;Update the Environment&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orge2cc630"&gt;
&lt;p&gt;
This prints out the environment for the user and updates the state.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;episode&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""Emits the updated environment to the user&lt;/span&gt;

&lt;span class="sd"&gt;    also sets the state to the next state&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;     episode (int): what episode we're in&lt;/span&gt;
&lt;span class="sd"&gt;     step (int): how long we've been running this episode&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="n"&gt;environment&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'-'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;states&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'T'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;goal_reached&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;Episode {}: Total Steps = {}"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;episode&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
	&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sleep&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_pause&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="n"&gt;environment&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next_state&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'O'&lt;/span&gt;
	&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"{}"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;""&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;environment&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next_state&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org1515e4e" class="outline-3"&gt;
&lt;h3 id="org1515e4e"&gt;Reset the environment&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org1515e4e"&gt;
&lt;p&gt;
This sets the current &lt;code&gt;state&lt;/code&gt; and the &lt;code&gt;next-state&lt;/code&gt; to the &lt;code&gt;start state&lt;/code&gt; so the environment can be re-used in different episodes.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""Resets the states to the start state"""&lt;/span&gt;
    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;start_state&lt;/span&gt;
    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next_state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;start_state&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org67f7e35" class="outline-2"&gt;
&lt;h2 id="org67f7e35"&gt;The Agent&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org67f7e35"&gt;
&lt;p&gt;
This is the agent that will learn to find the reward.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Agent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""This is the agent that will learn to find the treasure&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;     environment: The environment to explore&lt;/span&gt;
&lt;span class="sd"&gt;     exploitation_rate: Fraction of the time to exploit (epsilon)&lt;/span&gt;
&lt;span class="sd"&gt;     discount_factor: Discount factor (gamma)&lt;/span&gt;
&lt;span class="sd"&gt;     learning_rate: how much to change the reward (alpha)&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;environment&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;exploitation_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;discount_factor&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		 &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environment&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;environment&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exploitation_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;exploitation_rate&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;discount_factor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;discount_factor&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_q_table&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
	&lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Other than the environment, these are the values the &lt;code&gt;Agent&lt;/code&gt; needs:
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;exploitation_rate&lt;/code&gt;: What fraction of the time to use the best known action so far. Setting it to 0 means always use a random action, 1 means only use the best known action.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;discount_factor&lt;/code&gt;: The amount to discount the previously discovered reward. Setting it to less than 1 prevents infinite loops.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;learning_rate&lt;/code&gt;: How much to update the values in the table. 0 means don't update. 1 means use all of the new information.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id="outline-container-org29dbd29" class="outline-3"&gt;
&lt;h3 id="org29dbd29"&gt;The Q-Table&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org29dbd29"&gt;
&lt;p&gt;
The agent learns by building a table of 'Quality' estimates for any action chosen for the current state. Each state is a row in the table and each column is a possible action that can be taken. Initially the table is set to all zeros.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nd"&gt;@property&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""The Quality Estimate table&lt;/span&gt;

&lt;span class="sd"&gt;    Each cell is the quality-estimate for a given state, action pair&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;     DataFrame: rows are states, columns are actions&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_q_table&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_q_table&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
	    &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environment&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;states&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Actions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;actions&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
	    &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Actions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;actions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
	&lt;span class="p"&gt;)&lt;/span&gt;
	&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environment&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;states&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Actions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;actions&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_q_table&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org17937a4" class="outline-3"&gt;
&lt;h3 id="org17937a4"&gt;Action&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org17937a4"&gt;
&lt;p&gt;
Generates an action based on the current state. This is using an epsilon-greedy algorithm so it will explore if the value is above a certain threshold, otherwise it will exploit the best solution so far. It also will explore if the state has never lead to reaching the goal. This was confusing at first, but the updated reward for the current state is calculated based on the next state that this action leads to, so as the episodes continue, the rewards will fill in from the goal-state back to the start state.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nd"&gt;@property&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;action&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""Return the next chosen action&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;     str: the next action to take&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="c1"&gt;# get the row in the q-table matching the current state&lt;/span&gt;
    &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iloc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environment&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt;

    &lt;span class="c1"&gt;# only explore if we generate a value over epsilon&lt;/span&gt;
    &lt;span class="c1"&gt;# or none of the actions have a reward&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exploitation_rate&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="n"&gt;action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;choice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Actions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;actions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="c1"&gt;# exploit&lt;/span&gt;
	&lt;span class="c1"&gt;# get the column-name of the cell with the largest value&lt;/span&gt;
	&lt;span class="n"&gt;action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;idxmax&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb6226c3" class="outline-3"&gt;
&lt;h3 id="orgb6226c3"&gt;Act&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb6226c3"&gt;
&lt;p&gt;
This gets the next action to take, queries the environment for the reward (which also triggers storing the next state in the environment), then updates the q-table.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;act&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""Updates the Q-table based on the reward from the last action"""&lt;/span&gt;
    &lt;span class="n"&gt;action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;action&lt;/span&gt;
    &lt;span class="n"&gt;reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environment&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;previous_quality&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environment&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environment&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;goal_reached&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="n"&gt;new_quality&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="n"&gt;new_quality&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;discount_factor&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iloc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environment&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next_state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environment&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new_quality&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;previous_quality&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org54fe58b" class="outline-2"&gt;
&lt;h2 id="org54fe58b"&gt;Main&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org54fe58b"&gt;
&lt;p&gt;
Runs the simulation.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;"__main__"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;learner&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;QLearner&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;states&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;learner&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learner&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;learned-model with only exploitation set"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;learner&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exploitation_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="n"&gt;learner&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;episodes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="n"&gt;learner&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>qlearning</category><guid>https://necromuralist.github.io/reinforcement_learning/posts/one-dimensional-world/</guid><pubDate>Fri, 19 Jan 2018 19:38:59 GMT</pubDate></item><item><title>PyBrain Optimization Example</title><link>https://necromuralist.github.io/reinforcement_learning/posts/PyBrain-Optimization-Example/</link><dc:creator>necromuralist</dc:creator><description>&lt;div&gt;&lt;p&gt;This is the &lt;a class="reference external" href="http://www.pybrain.org/docs/tutorial/optimization.html#general-optimization-using-evolvable"&gt;general optimization example&lt;/a&gt; from the pybrain documentation.&lt;/p&gt;
&lt;div class="section" id="imports"&gt;
&lt;h2&gt;1 Imports&lt;/h2&gt;
&lt;pre class="code ipython"&gt;&lt;a name="rest_code_2f02fdb50d8b4f4bad83603001c8099f-1"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# python standard library&lt;/span&gt;
&lt;a name="rest_code_2f02fdb50d8b4f4bad83603001c8099f-2"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;
&lt;a name="rest_code_2f02fdb50d8b4f4bad83603001c8099f-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_2f02fdb50d8b4f4bad83603001c8099f-4"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# pypi&lt;/span&gt;
&lt;a name="rest_code_2f02fdb50d8b4f4bad83603001c8099f-5"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pybrain.structure.evolvables.evolvable&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Evolvable&lt;/span&gt;
&lt;a name="rest_code_2f02fdb50d8b4f4bad83603001c8099f-6"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pybrain.optimization&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;HillClimber&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="the-evolvable"&gt;
&lt;h2&gt;2 The Evolvable&lt;/h2&gt;
&lt;p&gt;To make an optimization that can take arbitrary values (not just continuous numbers), you can implement a sub-class of the PyBrain &lt;a class="reference external" href="http://www.pybrain.org/docs/api/structure/evolvables.html"&gt;Evolvable&lt;/a&gt; class.&lt;/p&gt;
&lt;div class="section" id="the-constructor"&gt;
&lt;h3&gt;2.1 The Constructor&lt;/h3&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;Evolvable&lt;/tt&gt; class doesn't implement a constructor so you can create one with any parameters you need.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-mutate-method"&gt;
&lt;h3&gt;2.2 The Mutate Method&lt;/h3&gt;
&lt;p&gt;This is the method that is called after each round to change the parameters a little (a &lt;cite&gt;tweak&lt;/cite&gt;). It takes positional arguments, but I think it's called by the Hill Climber so I don't know where it gets passed in.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-copy-method"&gt;
&lt;h3&gt;2.3 The Copy Method&lt;/h3&gt;
&lt;p&gt;The tutorial says this is a required method, but the documentation for the API says it should default to a deep-copy. Anyway, I think this is only used if you use something like a Genetic Algorithm.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-randomize-method"&gt;
&lt;h3&gt;2.4 The Randomize Method&lt;/h3&gt;
&lt;p&gt;This is used to initialize the parameters to a random value. This is required but I'm pretty sure it doesn't get used in this case.&lt;/p&gt;
&lt;pre class="code ipython"&gt;&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Mutant&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Evolvable&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""A simple evolvable class&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     x: a starting value to mimic the fitness of the model&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     mininmum: smallest allowed value&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-7"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     maximum: biggest allowed value&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-8"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-9"&gt;&lt;/a&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;minimum&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maximum&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-10"&gt;&lt;/a&gt;        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimum&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;minimum&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-11"&gt;&lt;/a&gt;        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maximum&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;maximum&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-12"&gt;&lt;/a&gt;        &lt;span class="c1"&gt;# minimum &amp;lt;= x &amp;lt;= maximum&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-13"&gt;&lt;/a&gt;        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-14"&gt;&lt;/a&gt;        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-15"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-16"&gt;&lt;/a&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-17"&gt;&lt;/a&gt;    &lt;span class="nd"&gt;@property&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-18"&gt;&lt;/a&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;x&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-19"&gt;&lt;/a&gt;        &lt;span class="sd"&gt;"""The value to optimize&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-20"&gt;&lt;/a&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-21"&gt;&lt;/a&gt;&lt;span class="sd"&gt;        Returns:&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-22"&gt;&lt;/a&gt;&lt;span class="sd"&gt;         x (float): value to optimize&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-23"&gt;&lt;/a&gt;&lt;span class="sd"&gt;        """&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-24"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_x&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-25"&gt;&lt;/a&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-26"&gt;&lt;/a&gt;    &lt;span class="nd"&gt;@x.setter&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-27"&gt;&lt;/a&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;x&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;new_x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-28"&gt;&lt;/a&gt;        &lt;span class="sd"&gt;"""sets x, constraining the value&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-29"&gt;&lt;/a&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-30"&gt;&lt;/a&gt;&lt;span class="sd"&gt;        Args:&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-31"&gt;&lt;/a&gt;&lt;span class="sd"&gt;         new_x: float from minimum to maximum&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-32"&gt;&lt;/a&gt;&lt;span class="sd"&gt;        """&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-33"&gt;&lt;/a&gt;        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimum&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maximum&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-34"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-35"&gt;&lt;/a&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-36"&gt;&lt;/a&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;mutate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-37"&gt;&lt;/a&gt;        &lt;span class="sd"&gt;"""Updates x with a random change&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-38"&gt;&lt;/a&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-39"&gt;&lt;/a&gt;&lt;span class="sd"&gt;        Maintains the constraint of the value&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-40"&gt;&lt;/a&gt;&lt;span class="sd"&gt;        """&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-41"&gt;&lt;/a&gt;        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mf"&gt;0.3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-42"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-43"&gt;&lt;/a&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-44"&gt;&lt;/a&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-45"&gt;&lt;/a&gt;        &lt;span class="sd"&gt;"""Returns a new instance with the same x-value&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-46"&gt;&lt;/a&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-47"&gt;&lt;/a&gt;&lt;span class="sd"&gt;        Returns:&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-48"&gt;&lt;/a&gt;&lt;span class="sd"&gt;         Mutant: copy of this instance&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-49"&gt;&lt;/a&gt;&lt;span class="sd"&gt;        """&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-50"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;Mutant&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-51"&gt;&lt;/a&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-52"&gt;&lt;/a&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;randomize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-53"&gt;&lt;/a&gt;        &lt;span class="sd"&gt;"""A method to randomize the x-value"""&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-54"&gt;&lt;/a&gt;        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maximum&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-55"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-56"&gt;&lt;/a&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-57"&gt;&lt;/a&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__repr__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-58"&gt;&lt;/a&gt;        &lt;span class="sd"&gt;"""String representation&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-59"&gt;&lt;/a&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-60"&gt;&lt;/a&gt;&lt;span class="sd"&gt;        Returns:&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-61"&gt;&lt;/a&gt;&lt;span class="sd"&gt;         str: formatted version of x&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-62"&gt;&lt;/a&gt;&lt;span class="sd"&gt;        """&lt;/span&gt;
&lt;a name="rest_code_19a1f63604094cba9483258d3970580d-63"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;"&amp;lt; {:.2f} (Maximized={})&amp;gt;"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maximum&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="hill-climbing"&gt;
&lt;h2&gt;3 Hill Climbing&lt;/h2&gt;
&lt;p&gt;The &lt;a class="reference external" href="http://www.pybrain.org/docs/api/optimization/optimization.html#module-pybrain.optimization"&gt;HillClimber&lt;/a&gt; is the simplest search - it assumes the first minima or maxima it finds is the global one. By default it tries to maximize the outcome. None of the arguments are required at instantiation, but in this case we're setting:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;an &lt;tt class="docutils literal"&gt;evaluator&lt;/tt&gt;: a callable that outputs how well the object to be evaluated did&lt;/li&gt;
&lt;li&gt;an &lt;tt class="docutils literal"&gt;evaluable&lt;/tt&gt;: the object to be evaluated in this case our &lt;tt class="docutils literal"&gt;Mutant&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;maxEvaluations&lt;/tt&gt;: The maximum number of times the &lt;tt class="docutils literal"&gt;evaluable&lt;/tt&gt; is evaluated&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;verbose&lt;/tt&gt;: print each step&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;desiredEvaluation&lt;/tt&gt;: the value that is good enough so the climber can stop&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="section" id="the-evaluator-function"&gt;
&lt;h3&gt;3.1 The Evaluator Function&lt;/h3&gt;
&lt;p&gt;In this case we're just going to return the x value of the object.&lt;/p&gt;
&lt;pre class="code ipython"&gt;&lt;a name="rest_code_2c10287e073e4e1f934f808e2e113701-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;evaluator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mutant&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_2c10287e073e4e1f934f808e2e113701-2"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;mutant&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="the-instances"&gt;
&lt;h3&gt;3.2 The Instances&lt;/h3&gt;
&lt;pre class="code ipython"&gt;&lt;a name="rest_code_4ad8633146024de695ed7e61b38631f3-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;mutant&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Mutant&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_4ad8633146024de695ed7e61b38631f3-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;climber&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;HillClimber&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;evaluator&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mutant&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxEvaluations&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;desiredEvaluation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="the-optimization"&gt;
&lt;h3&gt;3.3 The Optimization&lt;/h3&gt;
&lt;p&gt;The optimization classes get run using their &lt;tt class="docutils literal"&gt;learn&lt;/tt&gt; methods.&lt;/p&gt;
&lt;pre class="code ipython"&gt;&lt;a name="rest_code_fd9b54e941cf4b60b7634b6c38103a17-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;outcome&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;climber&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;learn&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_fd9b54e941cf4b60b7634b6c38103a17-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;outcome&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;pre class="literal-block"&gt;
('Step:', 0, 'best:', 6.780765339892317)
('Step:', 1, 'best:', 6.780765339892317)
('Step:', 2, 'best:', 6.807553650921801)
('Step:', 3, 'best:', 7.282574697921699)
('Step:', 4, 'best:', 7.45592511459156)
('Step:', 5, 'best:', 7.533694376079802)
('Step:', 6, 'best:', 7.751507552794123)
('Step:', 7, 'best:', 8.184303418505593)
('Step:', 8, 'best:', 8.184303418505593)
('Step:', 9, 'best:', 8.224264996606221)
('Step:', 10, 'best:', 8.4835021736195)
('Step:', 11, 'best:', 9.153976071682798)
('Step:', 12, 'best:', 9.55795557780446)
('Step:', 13, 'best:', 10)
(&amp;lt; 10.00 (Maximized=True)&amp;gt;, 10)
&lt;/pre&gt;
&lt;p&gt;It managed to find the maximum in 13 steps.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>pybrain optimization</category><guid>https://necromuralist.github.io/reinforcement_learning/posts/PyBrain-Optimization-Example/</guid><pubDate>Sat, 13 Jan 2018 00:16:00 GMT</pubDate></item><item><title>Epsilon Greedy with Normal-Distribution Payouts</title><link>https://necromuralist.github.io/reinforcement_learning/posts/Epsilon-Greedy-with-Normal-Distribution-Payouts/</link><dc:creator>Brunhilde</dc:creator><description>&lt;div&gt;&lt;p&gt;Since the Optimistic Initial Values agent can't use the Bernoulli Arm, I'm creating a version of the Epsilon Greedy Optimized that expects the &lt;tt class="docutils literal"&gt;arms&lt;/tt&gt; to be the population-mean for their payouts and the &lt;tt class="docutils literal"&gt;pull_arm&lt;/tt&gt; will return a set of normally-distributed around that mean.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-1"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;optimized&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;imports&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-2"&gt;&lt;/a&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-3"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;spec&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-5"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-6"&gt;&lt;/a&gt;&lt;span class="nd"&gt;@jitclass&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;spec&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-7"&gt;&lt;/a&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;EpsilonGreedyNormal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-8"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""The Epsilon Greedy Algorithm With Normal Arm&lt;/span&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-9"&gt;&lt;/a&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-10"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-11"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     epsilon (float): fraction of the time to explore&lt;/span&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-12"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     arms (list): collection of probabilities for bandit arm&lt;/span&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-13"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-14"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;optimized&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;constructor&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-15"&gt;&lt;/a&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-16"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;best&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-17"&gt;&lt;/a&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-18"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;select&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-19"&gt;&lt;/a&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-20"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;optimized&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;pull&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-21"&gt;&lt;/a&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-22"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;optimized&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-23"&gt;&lt;/a&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-24"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;optimized&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-25"&gt;&lt;/a&gt;
&lt;a name="rest_code_da387969a3bd43969c2850da5a7c2d62-26"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;call&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;div class="section" id="optimized-imports"&gt;
&lt;h2&gt;1.1 Optimized Imports&lt;/h2&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_53cec2b26b3746f39a85b5613305ffec-1"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# pypi&lt;/span&gt;
&lt;a name="rest_code_53cec2b26b3746f39a85b5613305ffec-2"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numba&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
&lt;a name="rest_code_53cec2b26b3746f39a85b5613305ffec-3"&gt;&lt;/a&gt;    &lt;span class="n"&gt;jit&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_53cec2b26b3746f39a85b5613305ffec-4"&gt;&lt;/a&gt;    &lt;span class="n"&gt;jitclass&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_53cec2b26b3746f39a85b5613305ffec-5"&gt;&lt;/a&gt;    &lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_53cec2b26b3746f39a85b5613305ffec-6"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numba&lt;/span&gt;
&lt;a name="rest_code_53cec2b26b3746f39a85b5613305ffec-7"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="the-spec"&gt;
&lt;h2&gt;1.2 The Spec&lt;/h2&gt;
&lt;p&gt;This is how you tell numba what attributes the class will have.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_865ae0078fb540e9888ac1ea7ede48e9-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;spec&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
&lt;a name="rest_code_865ae0078fb540e9888ac1ea7ede48e9-2"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"epsilon"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numba&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;a name="rest_code_865ae0078fb540e9888ac1ea7ede48e9-3"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"arms"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numba&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double&lt;/span&gt;&lt;span class="p"&gt;[:]),&lt;/span&gt;
&lt;a name="rest_code_865ae0078fb540e9888ac1ea7ede48e9-4"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"counts"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numba&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double&lt;/span&gt;&lt;span class="p"&gt;[:]),&lt;/span&gt;
&lt;a name="rest_code_865ae0078fb540e9888ac1ea7ede48e9-5"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"rewards"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numba&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double&lt;/span&gt;&lt;span class="p"&gt;[:]),&lt;/span&gt;
&lt;a name="rest_code_865ae0078fb540e9888ac1ea7ede48e9-6"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"total_reward"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numba&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int64&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;a name="rest_code_865ae0078fb540e9888ac1ea7ede48e9-7"&gt;&lt;/a&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="the-constructor"&gt;
&lt;h2&gt;1.3 The Constructor&lt;/h2&gt;
&lt;p&gt;The constructor takes two arguments - &lt;em&gt;epsilon&lt;/em&gt; and &lt;em&gt;arms&lt;/em&gt;. The &lt;em&gt;arms&lt;/em&gt; list should contain the mean payout for each arm.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_b2a3580c86f144c7a666a75c6109bc95-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_b2a3580c86f144c7a666a75c6109bc95-2"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;
&lt;a name="rest_code_b2a3580c86f144c7a666a75c6109bc95-3"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;arms&lt;/span&gt;
&lt;a name="rest_code_b2a3580c86f144c7a666a75c6109bc95-4"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_b2a3580c86f144c7a666a75c6109bc95-5"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_b2a3580c86f144c7a666a75c6109bc95-6"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;a name="rest_code_b2a3580c86f144c7a666a75c6109bc95-7"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="reset"&gt;
&lt;h2&gt;1.4 Reset&lt;/h2&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_0926b1d2c024467eb3277782017d8fc0-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_0926b1d2c024467eb3277782017d8fc0-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""sets the counts, rewards, total_reward to 0s&lt;/span&gt;
&lt;a name="rest_code_0926b1d2c024467eb3277782017d8fc0-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_0926b1d2c024467eb3277782017d8fc0-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    This lets you re-used the EpsilonGreedy&lt;/span&gt;
&lt;a name="rest_code_0926b1d2c024467eb3277782017d8fc0-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_0926b1d2c024467eb3277782017d8fc0-6"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_0926b1d2c024467eb3277782017d8fc0-7"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_0926b1d2c024467eb3277782017d8fc0-8"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;a name="rest_code_0926b1d2c024467eb3277782017d8fc0-9"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="best-arm"&gt;
&lt;h2&gt;1.5 Best Arm&lt;/h2&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;best_arm&lt;/tt&gt; property returns the index of the arm that has the highest average reward so far. It returns the index instead of the arm itself because it's used to get the matching counts and rewards in the &lt;tt class="docutils literal"&gt;update&lt;/tt&gt; method. Since I'm using the &lt;tt class="docutils literal"&gt;jitclass&lt;/tt&gt; decorator I'm going to get rid of &lt;tt class="docutils literal"&gt;first_find&lt;/tt&gt;.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_04febdb970cc4fe5ba593e4d59282922-1"&gt;&lt;/a&gt;&lt;span class="nd"&gt;@property&lt;/span&gt;
&lt;a name="rest_code_04febdb970cc4fe5ba593e4d59282922-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;best_arm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_04febdb970cc4fe5ba593e4d59282922-3"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""Index of the arm with the most reward"""&lt;/span&gt;
&lt;a name="rest_code_04febdb970cc4fe5ba593e4d59282922-4"&gt;&lt;/a&gt;    &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_04febdb970cc4fe5ba593e4d59282922-5"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
&lt;a name="rest_code_04febdb970cc4fe5ba593e4d59282922-6"&gt;&lt;/a&gt;        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
&lt;a name="rest_code_04febdb970cc4fe5ba593e4d59282922-7"&gt;&lt;/a&gt;            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;
&lt;a name="rest_code_04febdb970cc4fe5ba593e4d59282922-8"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="select-arm"&gt;
&lt;h2&gt;1.6 Select Arm&lt;/h2&gt;
&lt;p&gt;This differs from the other Epsilon Greedy code only in that I'm using numpy instead of python for the random function.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_fd1eefc5c3c148d99501b0c43400b4e5-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;select_arm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_fd1eefc5c3c148d99501b0c43400b4e5-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""chooses the next arm to update&lt;/span&gt;
&lt;a name="rest_code_fd1eefc5c3c148d99501b0c43400b4e5-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_fd1eefc5c3c148d99501b0c43400b4e5-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;a name="rest_code_fd1eefc5c3c148d99501b0c43400b4e5-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     int: index of the next arm to pull&lt;/span&gt;
&lt;a name="rest_code_fd1eefc5c3c148d99501b0c43400b4e5-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_fd1eefc5c3c148d99501b0c43400b4e5-7"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_fd1eefc5c3c148d99501b0c43400b4e5-8"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_fd1eefc5c3c148d99501b0c43400b4e5-9"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_arm&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="pull-arm"&gt;
&lt;h2&gt;1.7 Pull Arm&lt;/h2&gt;
&lt;p&gt;Since we can't give user-defined objects as attributes of the class, this version will be both algorithm and bandit. This is what's different from the other Epsilon Greedy algorithms in that we're returning the arm's mean plus a random number from the normal distribution. If numba allowed us to pass in objects maybe we could have just switched out bandits. I need to look into how to make that work.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_3b0039b7e94e4abfad83f6d8bbcc1169-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;pull_arm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_3b0039b7e94e4abfad83f6d8bbcc1169-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""gets the reward&lt;/span&gt;
&lt;a name="rest_code_3b0039b7e94e4abfad83f6d8bbcc1169-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_3b0039b7e94e4abfad83f6d8bbcc1169-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a name="rest_code_3b0039b7e94e4abfad83f6d8bbcc1169-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     arm (int): index for the arm-probability array&lt;/span&gt;
&lt;a name="rest_code_3b0039b7e94e4abfad83f6d8bbcc1169-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;a name="rest_code_3b0039b7e94e4abfad83f6d8bbcc1169-7"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     float: reward&lt;/span&gt;
&lt;a name="rest_code_3b0039b7e94e4abfad83f6d8bbcc1169-8"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_3b0039b7e94e4abfad83f6d8bbcc1169-9"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="update"&gt;
&lt;h2&gt;1.8 Update&lt;/h2&gt;
&lt;p&gt;The update method pulls the arm whose index it is given and then updates the count and reward. Here we're calling the &lt;tt class="docutils literal"&gt;pull_arm&lt;/tt&gt; method instead of using a &lt;tt class="docutils literal"&gt;BernoulliArm&lt;/tt&gt; so we can't re-use the original method.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_19a62851d9f0423ebf43e5e21c8d5be3-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_19a62851d9f0423ebf43e5e21c8d5be3-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""pulls the arm and updates the value&lt;/span&gt;
&lt;a name="rest_code_19a62851d9f0423ebf43e5e21c8d5be3-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_19a62851d9f0423ebf43e5e21c8d5be3-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a name="rest_code_19a62851d9f0423ebf43e5e21c8d5be3-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     arm (int): index of the arm to pull&lt;/span&gt;
&lt;a name="rest_code_19a62851d9f0423ebf43e5e21c8d5be3-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_19a62851d9f0423ebf43e5e21c8d5be3-7"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;a name="rest_code_19a62851d9f0423ebf43e5e21c8d5be3-8"&gt;&lt;/a&gt;    &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_19a62851d9f0423ebf43e5e21c8d5be3-9"&gt;&lt;/a&gt;    &lt;span class="n"&gt;average_reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_19a62851d9f0423ebf43e5e21c8d5be3-10"&gt;&lt;/a&gt;    &lt;span class="n"&gt;reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pull_arm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_19a62851d9f0423ebf43e5e21c8d5be3-11"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;
&lt;a name="rest_code_19a62851d9f0423ebf43e5e21c8d5be3-12"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(((&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;average_reward&lt;/span&gt;
&lt;a name="rest_code_19a62851d9f0423ebf43e5e21c8d5be3-13"&gt;&lt;/a&gt;                        &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;a name="rest_code_19a62851d9f0423ebf43e5e21c8d5be3-14"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</description><category>bandits reinforcementLearning</category><guid>https://necromuralist.github.io/reinforcement_learning/posts/Epsilon-Greedy-with-Normal-Distribution-Payouts/</guid><pubDate>Wed, 02 Aug 2017 01:48:00 GMT</pubDate></item><item><title>Optimistic Initial Values</title><link>https://necromuralist.github.io/reinforcement_learning/posts/Optimistic-Initial-Values/</link><dc:creator>Brunhilde</dc:creator><description>&lt;div&gt;&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;1 Introduction&lt;/h2&gt;
&lt;p&gt;This is one possible to the n-armed bandit problem. It is similar to the &lt;em&gt;Epsilon Greedy&lt;/em&gt; algorithm except that instead of using a conditional to decide whether to explore or exploit, the algorithm sets the estimated (mean) payout for each arm to 1 (the theoretical maximum for our case) and then always exploits. As things proceed, the arms will settle down to their actual payoff-rates and those that haven't been explored will be chosen because they are still too high.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-tangle"&gt;
&lt;h2&gt;2 The Tangle&lt;/h2&gt;
&lt;p&gt;This is the no-web template to build the final file.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_3d43d62f55ec448887d380b3d4ade7eb-1"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;imports&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_3d43d62f55ec448887d380b3d4ade7eb-2"&gt;&lt;/a&gt;
&lt;a name="rest_code_3d43d62f55ec448887d380b3d4ade7eb-3"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;spec&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_3d43d62f55ec448887d380b3d4ade7eb-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_3d43d62f55ec448887d380b3d4ade7eb-5"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;class&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;declaration&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_3d43d62f55ec448887d380b3d4ade7eb-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_3d43d62f55ec448887d380b3d4ade7eb-7"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;constructor&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_3d43d62f55ec448887d380b3d4ade7eb-8"&gt;&lt;/a&gt;
&lt;a name="rest_code_3d43d62f55ec448887d380b3d4ade7eb-9"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;select&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_3d43d62f55ec448887d380b3d4ade7eb-10"&gt;&lt;/a&gt;
&lt;a name="rest_code_3d43d62f55ec448887d380b3d4ade7eb-11"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;pull&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_3d43d62f55ec448887d380b3d4ade7eb-12"&gt;&lt;/a&gt;
&lt;a name="rest_code_3d43d62f55ec448887d380b3d4ade7eb-13"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_3d43d62f55ec448887d380b3d4ade7eb-14"&gt;&lt;/a&gt;
&lt;a name="rest_code_3d43d62f55ec448887d380b3d4ade7eb-15"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="imports"&gt;
&lt;h2&gt;3 Imports&lt;/h2&gt;
&lt;p&gt;These are our external dependencies.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_094fe67a3f324db389a039d8ca5315c0-1"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numba&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;jitclass&lt;/span&gt;
&lt;a name="rest_code_094fe67a3f324db389a039d8ca5315c0-2"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numba&lt;/span&gt;
&lt;a name="rest_code_094fe67a3f324db389a039d8ca5315c0-3"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="the-spec"&gt;
&lt;h2&gt;4 The Spec&lt;/h2&gt;
&lt;p&gt;In order to use numba with the &lt;tt class="docutils literal"&gt;OptimisticInitialValues&lt;/tt&gt; class you have to create a 'spec' that tells numba what the data-types are for each of its fields.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_72676d90280e4990abdece090b95f2f3-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;SPEC&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
&lt;a name="rest_code_72676d90280e4990abdece090b95f2f3-2"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"arms"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numba&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double&lt;/span&gt;&lt;span class="p"&gt;[:]),&lt;/span&gt;
&lt;a name="rest_code_72676d90280e4990abdece090b95f2f3-3"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"counts"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numba&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double&lt;/span&gt;&lt;span class="p"&gt;[:]),&lt;/span&gt;
&lt;a name="rest_code_72676d90280e4990abdece090b95f2f3-4"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"rewards"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numba&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double&lt;/span&gt;&lt;span class="p"&gt;[:]),&lt;/span&gt;
&lt;a name="rest_code_72676d90280e4990abdece090b95f2f3-5"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"total_reward"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numba&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int64&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;a name="rest_code_72676d90280e4990abdece090b95f2f3-6"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"initial_reward"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numba&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;a name="rest_code_72676d90280e4990abdece090b95f2f3-7"&gt;&lt;/a&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="the-class-declaration"&gt;
&lt;h2&gt;5 The Class Declaration&lt;/h2&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_bb240b82d9af4491aaf2bb5eaa737bd6-1"&gt;&lt;/a&gt;&lt;span class="nd"&gt;@jitclass&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;SPEC&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_bb240b82d9af4491aaf2bb5eaa737bd6-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;OptimisticInitialValues&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_bb240b82d9af4491aaf2bb5eaa737bd6-3"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""Optimistic Initial Values greedy algorithm&lt;/span&gt;
&lt;a name="rest_code_bb240b82d9af4491aaf2bb5eaa737bd6-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_bb240b82d9af4491aaf2bb5eaa737bd6-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a name="rest_code_bb240b82d9af4491aaf2bb5eaa737bd6-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     numpy.array[float]: payout-probabilities for each arm&lt;/span&gt;
&lt;a name="rest_code_bb240b82d9af4491aaf2bb5eaa737bd6-7"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="the-constructor"&gt;
&lt;h2&gt;6 The Constructor&lt;/h2&gt;
&lt;p&gt;Here's our first change from the epsilon-greedy algorithm. We no longer have an &lt;tt class="docutils literal"&gt;epsilon&lt;/tt&gt; value and instead of initializing the &lt;tt class="docutils literal"&gt;rewards&lt;/tt&gt; as zeros we initialize them with an 'initial' reward. Also, although you can't see it here, the arms have to be a list of mean payout values (see the &lt;tt class="docutils literal"&gt;pull_arm&lt;/tt&gt; method below).&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_da54e378a7884e4d87bc46dbbc0559de-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;initial_reward&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_da54e378a7884e4d87bc46dbbc0559de-2"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;arms&lt;/span&gt;
&lt;a name="rest_code_da54e378a7884e4d87bc46dbbc0559de-3"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_da54e378a7884e4d87bc46dbbc0559de-4"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;initial_reward&lt;/span&gt;
&lt;a name="rest_code_da54e378a7884e4d87bc46dbbc0559de-5"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;a name="rest_code_da54e378a7884e4d87bc46dbbc0559de-6"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initial_reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;initial_reward&lt;/span&gt;
&lt;a name="rest_code_da54e378a7884e4d87bc46dbbc0559de-7"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="select-arm"&gt;
&lt;h2&gt;7 Select Arm&lt;/h2&gt;
&lt;p&gt;This chooses the next arm. Unlike the epsilon-greedy algorithm it will always pick the 'best' arm, choosing the first if there is a tie. Since the whole class is in the jit I'm also not using the external &lt;tt class="docutils literal"&gt;find_first&lt;/tt&gt; method.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_d34c28e22dfb4b86b528e64f5ba33e24-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;select_arm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_d34c28e22dfb4b86b528e64f5ba33e24-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""Index of the arm with the most reward&lt;/span&gt;
&lt;a name="rest_code_d34c28e22dfb4b86b528e64f5ba33e24-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_d34c28e22dfb4b86b528e64f5ba33e24-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;a name="rest_code_d34c28e22dfb4b86b528e64f5ba33e24-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     integer: index of arm with highest average reward&lt;/span&gt;
&lt;a name="rest_code_d34c28e22dfb4b86b528e64f5ba33e24-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_d34c28e22dfb4b86b528e64f5ba33e24-7"&gt;&lt;/a&gt;    &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_d34c28e22dfb4b86b528e64f5ba33e24-8"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
&lt;a name="rest_code_d34c28e22dfb4b86b528e64f5ba33e24-9"&gt;&lt;/a&gt;        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
&lt;a name="rest_code_d34c28e22dfb4b86b528e64f5ba33e24-10"&gt;&lt;/a&gt;            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="pull-arm"&gt;
&lt;h2&gt;8 Pull Arm&lt;/h2&gt;
&lt;p&gt;This gets the reward for the arm. with a Bernoulli arm, there's a chance that an arm will be set to 0 on its first pull, at which point you will never explore it (since there's no exploration), so even the best arm might get wiped out. To fix this you need a different scheme. This one uses a population mean (selected &lt;tt class="docutils literal"&gt;from self.arms&lt;/tt&gt;) which has noise added by selecting from the standard normal distribution.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_c9539a9322b14bd2ad669f1a6e083052-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;pull_arm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_c9539a9322b14bd2ad669f1a6e083052-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""gets the reward&lt;/span&gt;
&lt;a name="rest_code_c9539a9322b14bd2ad669f1a6e083052-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_c9539a9322b14bd2ad669f1a6e083052-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a name="rest_code_c9539a9322b14bd2ad669f1a6e083052-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     arm (int): index for the arm population-mean array&lt;/span&gt;
&lt;a name="rest_code_c9539a9322b14bd2ad669f1a6e083052-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;a name="rest_code_c9539a9322b14bd2ad669f1a6e083052-7"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     float: payout for the arm&lt;/span&gt;
&lt;a name="rest_code_c9539a9322b14bd2ad669f1a6e083052-8"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_c9539a9322b14bd2ad669f1a6e083052-9"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="update-arm"&gt;
&lt;h2&gt;9 Update Arm&lt;/h2&gt;
&lt;p&gt;This pulls the arm and updates the reward. This works the same as the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;epsilon-greedy&lt;/span&gt;&lt;/tt&gt; version does.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_89188c86760f4da1bf6f0f2ca889f8c9-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_89188c86760f4da1bf6f0f2ca889f8c9-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""pulls the arm and updates the average reward&lt;/span&gt;
&lt;a name="rest_code_89188c86760f4da1bf6f0f2ca889f8c9-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_89188c86760f4da1bf6f0f2ca889f8c9-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    also updates the total_reward the algorithm has earned so far&lt;/span&gt;
&lt;a name="rest_code_89188c86760f4da1bf6f0f2ca889f8c9-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_89188c86760f4da1bf6f0f2ca889f8c9-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a name="rest_code_89188c86760f4da1bf6f0f2ca889f8c9-7"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     arm (int): index of the arm to pull&lt;/span&gt;
&lt;a name="rest_code_89188c86760f4da1bf6f0f2ca889f8c9-8"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_89188c86760f4da1bf6f0f2ca889f8c9-9"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;a name="rest_code_89188c86760f4da1bf6f0f2ca889f8c9-10"&gt;&lt;/a&gt;    &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_89188c86760f4da1bf6f0f2ca889f8c9-11"&gt;&lt;/a&gt;    &lt;span class="n"&gt;average_reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_89188c86760f4da1bf6f0f2ca889f8c9-12"&gt;&lt;/a&gt;    &lt;span class="n"&gt;reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pull_arm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_89188c86760f4da1bf6f0f2ca889f8c9-13"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;
&lt;a name="rest_code_89188c86760f4da1bf6f0f2ca889f8c9-14"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(((&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;average_reward&lt;/span&gt;
&lt;a name="rest_code_89188c86760f4da1bf6f0f2ca889f8c9-15"&gt;&lt;/a&gt;                         &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;a name="rest_code_89188c86760f4da1bf6f0f2ca889f8c9-16"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="reset"&gt;
&lt;h2&gt;10 Reset&lt;/h2&gt;
&lt;p&gt;This resets the values so that you can re-use the algorithm. As with the constructor, it sets the &lt;tt class="docutils literal"&gt;rewards&lt;/tt&gt; to all ones instead of zeros as was the case with the epsilon-greedy algorithm.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_ce5660285bb149e4be8059b15828b78a-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_ce5660285bb149e4be8059b15828b78a-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""sets the counts, rewards, total_reward to 0s&lt;/span&gt;
&lt;a name="rest_code_ce5660285bb149e4be8059b15828b78a-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_ce5660285bb149e4be8059b15828b78a-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    This lets you re-used the EpsilonGreedy&lt;/span&gt;
&lt;a name="rest_code_ce5660285bb149e4be8059b15828b78a-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_ce5660285bb149e4be8059b15828b78a-6"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_ce5660285bb149e4be8059b15828b78a-7"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initial_reward&lt;/span&gt;
&lt;a name="rest_code_ce5660285bb149e4be8059b15828b78a-8"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;a name="rest_code_ce5660285bb149e4be8059b15828b78a-9"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</description><category>bandits reinforcementLearning</category><guid>https://necromuralist.github.io/reinforcement_learning/posts/Optimistic-Initial-Values/</guid><pubDate>Wed, 02 Aug 2017 01:47:00 GMT</pubDate></item><item><title>Assessing the Performance</title><link>https://necromuralist.github.io/reinforcement_learning/posts/assessing-the-performance/</link><dc:creator>hades</dc:creator><description>&lt;div&gt;&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;1 Introduction&lt;/h2&gt;
&lt;p&gt;As with the Epsilon-Greedy algorithm I'm going to use the Cumulative Reward as the metric. In this case we don't really have a parameter to tune.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="imports"&gt;
&lt;h2&gt;2 Imports&lt;/h2&gt;
&lt;p&gt;The dependencies.&lt;/p&gt;
&lt;pre class="code ipython"&gt;&lt;a name="rest_code_470478eeeb404670a80ca4a9aa8abf24-1"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# python standard library&lt;/span&gt;
&lt;a name="rest_code_470478eeeb404670a80ca4a9aa8abf24-2"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;datetime&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;
&lt;a name="rest_code_470478eeeb404670a80ca4a9aa8abf24-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_470478eeeb404670a80ca4a9aa8abf24-4"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# pypi&lt;/span&gt;
&lt;a name="rest_code_470478eeeb404670a80ca4a9aa8abf24-5"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numba&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;jit&lt;/span&gt;
&lt;a name="rest_code_470478eeeb404670a80ca4a9aa8abf24-6"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt;
&lt;a name="rest_code_470478eeeb404670a80ca4a9aa8abf24-7"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt;
&lt;a name="rest_code_470478eeeb404670a80ca4a9aa8abf24-8"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plot&lt;/span&gt;
&lt;a name="rest_code_470478eeeb404670a80ca4a9aa8abf24-9"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;seaborn&lt;/span&gt;
&lt;a name="rest_code_470478eeeb404670a80ca4a9aa8abf24-10"&gt;&lt;/a&gt;
&lt;a name="rest_code_470478eeeb404670a80ca4a9aa8abf24-11"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# this project&lt;/span&gt;
&lt;a name="rest_code_470478eeeb404670a80ca4a9aa8abf24-12"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;optimistic_initial_values&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;OptimisticInitialValues&lt;/span&gt;
&lt;a name="rest_code_470478eeeb404670a80ca4a9aa8abf24-13"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;epsilon_greedy_normal&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;EpsilonGreedyNormal&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="set-up-the-plotting"&gt;
&lt;h2&gt;3 Set-up the Plotting&lt;/h2&gt;
&lt;p&gt;This will enable the plotting and set the style.&lt;/p&gt;
&lt;pre class="code ipython"&gt;&lt;a name="rest_code_36fb7ba1399144aab2f0f2f574dde833-1"&gt;&lt;/a&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="k"&gt;matplotlib&lt;/span&gt; inline
&lt;a name="rest_code_36fb7ba1399144aab2f0f2f574dde833-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;seaborn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_style&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"whitegrid"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="running-the-assessment"&gt;
&lt;h2&gt;4 Running the Assessment&lt;/h2&gt;
&lt;pre class="code ipython"&gt;&lt;a name="rest_code_5088e976d842497498ee0088acf31011-1"&gt;&lt;/a&gt;&lt;span class="nd"&gt;@jit&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;cumulative_reward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trials&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;400&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-3"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""this generates the cumulative reward as the agent pulls the arms&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     agent: implementation that selects and updates the arms&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-7"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     trials (int): number of times to train the agent&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-8"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     times (int): length of time to train the agent&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-9"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-10"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     numpy.array: average cumulative rewards over time&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-11"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-12"&gt;&lt;/a&gt;    &lt;span class="n"&gt;cumulative_rewards&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-13"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;trial&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trials&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-14"&gt;&lt;/a&gt;        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-15"&gt;&lt;/a&gt;            &lt;span class="n"&gt;arm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select_arm&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-16"&gt;&lt;/a&gt;            &lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-17"&gt;&lt;/a&gt;            &lt;span class="n"&gt;cumulative_rewards&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-18"&gt;&lt;/a&gt;        &lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_5088e976d842497498ee0088acf31011-19"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;cumulative_rewards&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;trials&lt;/span&gt;
&lt;/pre&gt;&lt;pre class="code ipython"&gt;&lt;a name="rest_code_e91f155c7ffd4c9b90fc5a183ba36291-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;plot_cumulative&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cumulative&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_e91f155c7ffd4c9b90fc5a183ba36291-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""generates and plots cumulative average&lt;/span&gt;
&lt;a name="rest_code_e91f155c7ffd4c9b90fc5a183ba36291-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_e91f155c7ffd4c9b90fc5a183ba36291-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a name="rest_code_e91f155c7ffd4c9b90fc5a183ba36291-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     cumulative (pandas.DataFrame): data to plot&lt;/span&gt;
&lt;a name="rest_code_e91f155c7ffd4c9b90fc5a183ba36291-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_e91f155c7ffd4c9b90fc5a183ba36291-7"&gt;&lt;/a&gt;    &lt;span class="n"&gt;figure&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_e91f155c7ffd4c9b90fc5a183ba36291-8"&gt;&lt;/a&gt;    &lt;span class="n"&gt;axe&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gca&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_e91f155c7ffd4c9b90fc5a183ba36291-9"&gt;&lt;/a&gt;    &lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Cumulative Reward of the Optimistic Initial Values Algorithm ({} trials)"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TRIALS&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_e91f155c7ffd4c9b90fc5a183ba36291-10"&gt;&lt;/a&gt;    &lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Time (number of pulls on the arm)"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_e91f155c7ffd4c9b90fc5a183ba36291-11"&gt;&lt;/a&gt;    &lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Cumulative Reward"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_e91f155c7ffd4c9b90fc5a183ba36291-12"&gt;&lt;/a&gt;    &lt;span class="n"&gt;cumulative&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_e91f155c7ffd4c9b90fc5a183ba36291-13"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;pre class="code ipython"&gt;&lt;a name="rest_code_fbe49364bad2422a95c0fd932b8b9e62-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;TRIALS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5000&lt;/span&gt;
&lt;a name="rest_code_fbe49364bad2422a95c0fd932b8b9e62-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;TIMES&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;400&lt;/span&gt;
&lt;a name="rest_code_fbe49364bad2422a95c0fd932b8b9e62-3"&gt;&lt;/a&gt;&lt;span class="n"&gt;similar_payout_rates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;6.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_fbe49364bad2422a95c0fd932b8b9e62-4"&gt;&lt;/a&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;similar_payout_rates&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_fbe49364bad2422a95c0fd932b8b9e62-5"&gt;&lt;/a&gt;&lt;span class="n"&gt;one_good_arm_rates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;9.0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;a name="rest_code_fbe49364bad2422a95c0fd932b8b9e62-6"&gt;&lt;/a&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;one_good_arm_rates&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;div class="section" id="similar-arms"&gt;
&lt;h3&gt;4.1 Similar Arms&lt;/h3&gt;
&lt;p&gt;This will create a range where each arm only differs by 0.1&lt;/p&gt;
&lt;pre class="code ipython"&gt;&lt;a name="rest_code_e4202bf9f9db4ba3b03001746dbe1faa-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;optimistic_agent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;OptimisticInitialValues&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;similar_payout_rates&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;10.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_e4202bf9f9db4ba3b03001746dbe1faa-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
&lt;a name="rest_code_e4202bf9f9db4ba3b03001746dbe1faa-3"&gt;&lt;/a&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Optimistic Initial Values"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cumulative_reward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimistic_agent&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trials&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TRIALS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TIMES&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_e4202bf9f9db4ba3b03001746dbe1faa-4"&gt;&lt;/a&gt;&lt;span class="n"&gt;epsilon_agent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;EpsilonGreedyNormal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;similar_payout_rates&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_e4202bf9f9db4ba3b03001746dbe1faa-5"&gt;&lt;/a&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Epsilon Greedy (0.1)"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cumulative_reward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epsilon_agent&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trials&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TRIALS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TIMES&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_e4202bf9f9db4ba3b03001746dbe1faa-6"&gt;&lt;/a&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_e4202bf9f9db4ba3b03001746dbe1faa-7"&gt;&lt;/a&gt;&lt;span class="n"&gt;plot_cumulative&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;img alt="optimistic_similar_cumulative.png" src="https://necromuralist.github.io/reinforcement_learning/posts/assessing-the-performance/optimistic_similar_cumulative.png"&gt;
&lt;p&gt;The Optimistic Initial Values agent does better than the Epsilon Greedy, as you would expect (since it eventually stops exploring). But it looks suspisciously linear.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="one-good-arm"&gt;
&lt;h3&gt;4.2 One Good Arm&lt;/h3&gt;
&lt;p&gt;Lets see how it goes when one arm dominates the payouts.&lt;/p&gt;
&lt;pre class="code ipython"&gt;&lt;a name="rest_code_f700a5d944f94d90bfb9dbdffa071196-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;one_good_arm_rates&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_f700a5d944f94d90bfb9dbdffa071196-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;optimistic_agent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;OptimisticInitialValues&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;one_good_arm_rates&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;10.&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_f700a5d944f94d90bfb9dbdffa071196-3"&gt;&lt;/a&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
&lt;a name="rest_code_f700a5d944f94d90bfb9dbdffa071196-4"&gt;&lt;/a&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Optimistic Initial Values"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cumulative_reward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimistic_agent&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trials&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TRIALS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TIMES&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_f700a5d944f94d90bfb9dbdffa071196-5"&gt;&lt;/a&gt;&lt;span class="n"&gt;epsilon_agent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;EpsilonGreedyNormal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;one_good_arm_rates&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_f700a5d944f94d90bfb9dbdffa071196-6"&gt;&lt;/a&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Epsilon Greedy (0.1)"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cumulative_reward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epsilon_agent&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trials&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TRIALS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TIMES&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_f700a5d944f94d90bfb9dbdffa071196-7"&gt;&lt;/a&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_f700a5d944f94d90bfb9dbdffa071196-8"&gt;&lt;/a&gt;&lt;span class="n"&gt;plot_cumulative&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;img alt="optimistic_cumulative_one_good_arm.png" src="https://necromuralist.github.io/reinforcement_learning/posts/assessing-the-performance/optimistic_cumulative_one_good_arm.png"&gt;
&lt;p&gt;It looks like the optimistic agent does even better with one dominant arm. Likely because it found it quick enough that always exploiting it gives it a huge advantage over the epsilon greedy, which never stops exploring.&lt;/p&gt;
&lt;pre class="code ipython"&gt;&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-1"&gt;&lt;/a&gt;&lt;span class="nd"&gt;@jit&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;average_reward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trials&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-3"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""this generates the average reward for the trials over time&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     trials (int): number of times to train the agent&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-7"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     times (int): length of time to train the agent&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-8"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-9"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     numpy.array: the average reward&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-10"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-11"&gt;&lt;/a&gt;    &lt;span class="n"&gt;average_rewards&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-12"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;trial&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trials&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-13"&gt;&lt;/a&gt;        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-14"&gt;&lt;/a&gt;            &lt;span class="n"&gt;arm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select_arm&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-15"&gt;&lt;/a&gt;            &lt;span class="n"&gt;old_reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-16"&gt;&lt;/a&gt;            &lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-17"&gt;&lt;/a&gt;            &lt;span class="n"&gt;average_rewards&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;old_reward&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-18"&gt;&lt;/a&gt;        &lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_89ce97f0090245438df0b00ca2a05772-19"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;average_rewards&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;trials&lt;/span&gt;
&lt;/pre&gt;&lt;pre class="code ipython"&gt;&lt;a name="rest_code_493ab2bb18e64c8f8dd038db6c1198df-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
&lt;a name="rest_code_493ab2bb18e64c8f8dd038db6c1198df-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Optimistic Initial Values"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;average_reward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimistic_agent&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;TIMES&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;TRIALS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_493ab2bb18e64c8f8dd038db6c1198df-3"&gt;&lt;/a&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Epsilon 0.1"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;average_reward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epsilon_agent&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;TIMES&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;TRIALS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;pre class="code ipython"&gt;&lt;a name="rest_code_0d7788727eae4a418a37b68eaf63dd8a-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;averages&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;pre class="code ipython"&gt;&lt;a name="rest_code_ae6c43cabdb7467fad6f3ec6c15697b0-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;figure&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_ae6c43cabdb7467fad6f3ec6c15697b0-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;axe&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gca&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_ae6c43cabdb7467fad6f3ec6c15697b0-3"&gt;&lt;/a&gt;&lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Average Reward (One Dominant Arm)"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_ae6c43cabdb7467fad6f3ec6c15697b0-4"&gt;&lt;/a&gt;&lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Time (number of pulls on the arm)"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_ae6c43cabdb7467fad6f3ec6c15697b0-5"&gt;&lt;/a&gt;&lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Average Reward"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_ae6c43cabdb7467fad6f3ec6c15697b0-6"&gt;&lt;/a&gt;&lt;span class="n"&gt;averages&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;marker&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'.'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;linestyle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"None"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;img alt="optimistic_averages.png" src="https://necromuralist.github.io/reinforcement_learning/posts/assessing-the-performance/optimistic_averages.png"&gt;
&lt;p&gt;It looks like there was a brief period where the Epsilon Greedy did better, but the Optimistic agent settled in fairly quickly.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>bandits reinforcementLearning</category><guid>https://necromuralist.github.io/reinforcement_learning/posts/assessing-the-performance/</guid><pubDate>Wed, 02 Aug 2017 01:46:00 GMT</pubDate></item><item><title>Finding the Best Epsilon</title><link>https://necromuralist.github.io/reinforcement_learning/posts/finding-the-best-epsilon/</link><dc:creator>hades</dc:creator><description>&lt;div&gt;&lt;div class="contents topic" id="contents"&gt;
&lt;p class="topic-title first"&gt;Contents&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference internal" href="https://necromuralist.github.io/reinforcement_learning/posts/finding-the-best-epsilon/#introduction" id="id1"&gt;1 Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="https://necromuralist.github.io/reinforcement_learning/posts/finding-the-best-epsilon/#imports" id="id2"&gt;2 Imports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="https://necromuralist.github.io/reinforcement_learning/posts/finding-the-best-epsilon/#set-up-the-plotting" id="id3"&gt;3 Set-up the Plotting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="https://necromuralist.github.io/reinforcement_learning/posts/finding-the-best-epsilon/#the-probabilities" id="id4"&gt;4 The Probabilities&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class="reference internal" href="https://necromuralist.github.io/reinforcement_learning/posts/finding-the-best-epsilon/#generate-the-probabilities" id="id5"&gt;4.1 Generate the Probabilities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="https://necromuralist.github.io/reinforcement_learning/posts/finding-the-best-epsilon/#inspecting-the-outcome" id="id6"&gt;4.2 Inspecting the Outcome&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="https://necromuralist.github.io/reinforcement_learning/posts/finding-the-best-epsilon/#average-reward" id="id7"&gt;5 Average Reward&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="https://necromuralist.github.io/reinforcement_learning/posts/finding-the-best-epsilon/#cumulative-reward" id="id8"&gt;6 Cumulative Reward&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class="reference internal" href="https://necromuralist.github.io/reinforcement_learning/posts/finding-the-best-epsilon/#imbalanced-case" id="id9"&gt;6.1 Imbalanced Case&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="https://necromuralist.github.io/reinforcement_learning/posts/finding-the-best-epsilon/#id1"&gt;1 Introduction&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This is one of the ways to characterize the performance of the &lt;em&gt;Epsilon Greedy&lt;/em&gt; agent using our &lt;em&gt;Bernoulli Arm&lt;/em&gt;. We are going to look at three ways to evaluate how well the algorithm does.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;probability of using the best arm&lt;/li&gt;
&lt;li&gt;average reward&lt;/li&gt;
&lt;li&gt;cumulative reward&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="imports"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="https://necromuralist.github.io/reinforcement_learning/posts/finding-the-best-epsilon/#id2"&gt;2 Imports&lt;/a&gt;&lt;/h2&gt;
&lt;pre class="code ipython"&gt;&lt;a name="rest_code_9566dfce869447e6afc25f6f15c724f8-1"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# python standard library&lt;/span&gt;
&lt;a name="rest_code_9566dfce869447e6afc25f6f15c724f8-2"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt;
&lt;a name="rest_code_9566dfce869447e6afc25f6f15c724f8-3"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;datetime&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;
&lt;a name="rest_code_9566dfce869447e6afc25f6f15c724f8-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_9566dfce869447e6afc25f6f15c724f8-5"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# pypi&lt;/span&gt;
&lt;a name="rest_code_9566dfce869447e6afc25f6f15c724f8-6"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numba&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;jit&lt;/span&gt;
&lt;a name="rest_code_9566dfce869447e6afc25f6f15c724f8-7"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt;
&lt;a name="rest_code_9566dfce869447e6afc25f6f15c724f8-8"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt;
&lt;a name="rest_code_9566dfce869447e6afc25f6f15c724f8-9"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plot&lt;/span&gt;
&lt;a name="rest_code_9566dfce869447e6afc25f6f15c724f8-10"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;seaborn&lt;/span&gt;
&lt;a name="rest_code_9566dfce869447e6afc25f6f15c724f8-11"&gt;&lt;/a&gt;
&lt;a name="rest_code_9566dfce869447e6afc25f6f15c724f8-12"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# this project&lt;/span&gt;
&lt;a name="rest_code_9566dfce869447e6afc25f6f15c724f8-13"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;epsilon_greedy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
&lt;a name="rest_code_9566dfce869447e6afc25f6f15c724f8-14"&gt;&lt;/a&gt;    &lt;span class="n"&gt;EpsilonGreedy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_9566dfce869447e6afc25f6f15c724f8-15"&gt;&lt;/a&gt;    &lt;span class="n"&gt;find_first&lt;/span&gt;
&lt;a name="rest_code_9566dfce869447e6afc25f6f15c724f8-16"&gt;&lt;/a&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_9566dfce869447e6afc25f6f15c724f8-17"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;epsilon_greedy_optimized&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;EpsilonGreedyOptimized&lt;/span&gt;
&lt;a name="rest_code_9566dfce869447e6afc25f6f15c724f8-18"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;bernoulli_arm&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;BernoulliArm&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="set-up-the-plotting"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="https://necromuralist.github.io/reinforcement_learning/posts/finding-the-best-epsilon/#id3"&gt;3 Set-up the Plotting&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This will enable the plotting and set the style.&lt;/p&gt;
&lt;pre class="code ipython"&gt;&lt;a name="rest_code_2ec146ec395d4c76bd6bcc9d96af1655-1"&gt;&lt;/a&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="k"&gt;matplotlib&lt;/span&gt; inline
&lt;a name="rest_code_2ec146ec395d4c76bd6bcc9d96af1655-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;seaborn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_style&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"whitegrid"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="the-probabilities"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="https://necromuralist.github.io/reinforcement_learning/posts/finding-the-best-epsilon/#id4"&gt;4 The Probabilities&lt;/a&gt;&lt;/h2&gt;
&lt;div class="section" id="generate-the-probabilities"&gt;
&lt;h3&gt;&lt;a class="toc-backref" href="https://necromuralist.github.io/reinforcement_learning/posts/finding-the-best-epsilon/#id5"&gt;4.1 Generate the Probabilities&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;This code will run generate the probabilities. Although I made it so that using the &lt;tt class="docutils literal"&gt;EpsilonGreedy&lt;/tt&gt; call method would both choose the arm and update the reward, in this case we need to know which arm was selected so I'm going to do the steps individually.&lt;/p&gt;
&lt;pre class="code ipython"&gt;&lt;a name="rest_code_2734695ef4a346879a158aaab60f5d01-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;generate_probabilities&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trials&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_2734695ef4a346879a158aaab60f5d01-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""this generates the probabilites for finding the best arm&lt;/span&gt;
&lt;a name="rest_code_2734695ef4a346879a158aaab60f5d01-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_2734695ef4a346879a158aaab60f5d01-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a name="rest_code_2734695ef4a346879a158aaab60f5d01-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     trials (int): number of times to train the agent&lt;/span&gt;
&lt;a name="rest_code_2734695ef4a346879a158aaab60f5d01-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     times (int): length of time to train the agent&lt;/span&gt;
&lt;a name="rest_code_2734695ef4a346879a158aaab60f5d01-7"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;a name="rest_code_2734695ef4a346879a158aaab60f5d01-8"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     Dict: the probabilites for each epsilon over time&lt;/span&gt;
&lt;a name="rest_code_2734695ef4a346879a158aaab60f5d01-9"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     """&lt;/span&gt;
&lt;a name="rest_code_2734695ef4a346879a158aaab60f5d01-10"&gt;&lt;/a&gt;    &lt;span class="n"&gt;arm_probabilities&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_2734695ef4a346879a158aaab60f5d01-11"&gt;&lt;/a&gt;    &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arm_probabilities&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_2734695ef4a346879a158aaab60f5d01-12"&gt;&lt;/a&gt;    &lt;span class="n"&gt;best_arm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;arm_probabilities&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arm_probabilities&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_2734695ef4a346879a158aaab60f5d01-13"&gt;&lt;/a&gt;    &lt;span class="n"&gt;arms&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;BernoulliArm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probability&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;probability&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;arm_probabilities&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_2734695ef4a346879a158aaab60f5d01-14"&gt;&lt;/a&gt;    &lt;span class="n"&gt;epsilons&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;0.05&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;a name="rest_code_2734695ef4a346879a158aaab60f5d01-15"&gt;&lt;/a&gt;
&lt;a name="rest_code_2734695ef4a346879a158aaab60f5d01-16"&gt;&lt;/a&gt;    &lt;span class="n"&gt;outcomes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
&lt;a name="rest_code_2734695ef4a346879a158aaab60f5d01-17"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;epsilons&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_2734695ef4a346879a158aaab60f5d01-18"&gt;&lt;/a&gt;        &lt;span class="n"&gt;agent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;EpsilonGreedy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_2734695ef4a346879a158aaab60f5d01-19"&gt;&lt;/a&gt;        &lt;span class="n"&gt;probabilities&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_2734695ef4a346879a158aaab60f5d01-20"&gt;&lt;/a&gt;        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;trial&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trials&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_2734695ef4a346879a158aaab60f5d01-21"&gt;&lt;/a&gt;            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_2734695ef4a346879a158aaab60f5d01-22"&gt;&lt;/a&gt;                &lt;span class="n"&gt;arm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select_arm&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_2734695ef4a346879a158aaab60f5d01-23"&gt;&lt;/a&gt;                &lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_2734695ef4a346879a158aaab60f5d01-24"&gt;&lt;/a&gt;                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;arm&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;best_arm&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_2734695ef4a346879a158aaab60f5d01-25"&gt;&lt;/a&gt;                    &lt;span class="n"&gt;probabilities&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;a name="rest_code_2734695ef4a346879a158aaab60f5d01-26"&gt;&lt;/a&gt;            &lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_2734695ef4a346879a158aaab60f5d01-27"&gt;&lt;/a&gt;        &lt;span class="n"&gt;outcomes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Epsilon {:.02f}"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;probabilities&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;times&lt;/span&gt;
&lt;a name="rest_code_2734695ef4a346879a158aaab60f5d01-28"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;outcomes&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="inspecting-the-outcome"&gt;
&lt;h3&gt;&lt;a class="toc-backref" href="https://necromuralist.github.io/reinforcement_learning/posts/finding-the-best-epsilon/#id6"&gt;4.2 Inspecting the Outcome&lt;/a&gt;&lt;/h3&gt;
&lt;pre class="code ipython"&gt;&lt;a name="rest_code_3cd428fbcb7241ed824c26d3f7adbc5b-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;TRIALS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5000&lt;/span&gt;
&lt;a name="rest_code_3cd428fbcb7241ed824c26d3f7adbc5b-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;TIMES&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;400&lt;/span&gt;
&lt;/pre&gt;&lt;pre class="code ipython"&gt;&lt;a name="rest_code_be9a3375be39424088d622fca6913db3-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;start&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_be9a3375be39424088d622fca6913db3-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;probabilities&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;generate_probabilities&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trials&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TRIALS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TIMES&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_be9a3375be39424088d622fca6913db3-3"&gt;&lt;/a&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Run Time: {0}"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_be9a3375be39424088d622fca6913db3-4"&gt;&lt;/a&gt;&lt;span class="n"&gt;probabilities&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probabilities&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_be9a3375be39424088d622fca6913db3-5"&gt;&lt;/a&gt;&lt;span class="n"&gt;probabilities&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;describe&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;pre class="literal-block"&gt;
       Epsilon 0.05  Epsilon 0.10  Epsilon 0.20  Epsilon 0.30  Epsilon 0.40  \
count    400.000000    400.000000    400.000000    400.000000    400.000000
mean       8.784956      9.971200      9.801344      9.084844      8.235194
std        3.064916      2.523568      1.775677      1.319447      0.998305
min        0.120000      0.275000      0.495000      0.757500      1.050000
25%        7.141875      9.740000     10.264375      9.390000      8.409375
50%        9.937500     11.187500     10.456250      9.475000      8.480000
75%       11.225000     11.457500     10.515000      9.530625      8.545000
max       11.720000     11.605000     10.727500      9.665000      8.712500

       Epsilon 0.50
count    400.000000
mean       7.310087
std        0.763259
min        1.190000
25%        7.406875
50%        7.473750
75%        7.535625
max        7.725000
&lt;/pre&gt;
&lt;pre class="code ipython"&gt;&lt;a name="rest_code_1b165519ea9d4b47975fd48a911d3b77-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;probabilities&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"epsilon_greedy_accuracy.csv"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;pre class="code ipython"&gt;&lt;a name="rest_code_36aff4162ef54dc5a516dd663c0e4a07-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;figure&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_36aff4162ef54dc5a516dd663c0e4a07-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;axe&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gca&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_36aff4162ef54dc5a516dd663c0e4a07-3"&gt;&lt;/a&gt;&lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Accuracy of the Epsilon Greedy Algorithm ({} trials)"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TRIALS&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_36aff4162ef54dc5a516dd663c0e4a07-4"&gt;&lt;/a&gt;&lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Time (number of pulls on the arm)"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_36aff4162ef54dc5a516dd663c0e4a07-5"&gt;&lt;/a&gt;&lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Probability of retrieving the best arm"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_36aff4162ef54dc5a516dd663c0e4a07-6"&gt;&lt;/a&gt;&lt;span class="n"&gt;probabilities&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;img alt="epsilon_greedy_probablilities.png" src="https://necromuralist.github.io/reinforcement_learning/posts/finding-the-best-epsilon/epsilon_greedy_probablilities.png"&gt;
&lt;p&gt;Looking at the plots, it appears that the epsilons greater than 0.05 converge faster that 0.05 (their curves are steeper at the beginning), as you would expect, but they also don't do as well in the long run, as you might also expect, since they're doing more exploration. In the long run, the more exploitation, the better the profit, but I suppose it depends on the window you have to work with, if you have a short one, then the more aggresive explorers might be better. Anything less than 350 would do better with 0.1 rather than 0.05, for instance.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="average-reward"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="https://necromuralist.github.io/reinforcement_learning/posts/finding-the-best-epsilon/#id7"&gt;5 Average Reward&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;One of the things to note about the previous trials is that there was one arm that did notably better than all the others. When they are more uniform using the probability of retrieving the best arm might not be as revealing. Instead, using the average reward so far would give us more information.&lt;/p&gt;
&lt;pre class="code ipython"&gt;&lt;a name="rest_code_2ec45af74d5448759c2c7f0ff10d2dc8-1"&gt;&lt;/a&gt;&lt;span class="nd"&gt;@jit&lt;/span&gt;
&lt;a name="rest_code_2ec45af74d5448759c2c7f0ff10d2dc8-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;average_reward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trials&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_2ec45af74d5448759c2c7f0ff10d2dc8-3"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""this generates the probabilites for finding the best arm&lt;/span&gt;
&lt;a name="rest_code_2ec45af74d5448759c2c7f0ff10d2dc8-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_2ec45af74d5448759c2c7f0ff10d2dc8-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a name="rest_code_2ec45af74d5448759c2c7f0ff10d2dc8-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     trials (int): number of times to train the agent&lt;/span&gt;
&lt;a name="rest_code_2ec45af74d5448759c2c7f0ff10d2dc8-7"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     times (int): length of time to train the agent&lt;/span&gt;
&lt;a name="rest_code_2ec45af74d5448759c2c7f0ff10d2dc8-8"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;a name="rest_code_2ec45af74d5448759c2c7f0ff10d2dc8-9"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     Dict: the probabilites for each epsilon over time&lt;/span&gt;
&lt;a name="rest_code_2ec45af74d5448759c2c7f0ff10d2dc8-10"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_2ec45af74d5448759c2c7f0ff10d2dc8-11"&gt;&lt;/a&gt;    &lt;span class="n"&gt;arm_probabilities&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;a name="rest_code_2ec45af74d5448759c2c7f0ff10d2dc8-12"&gt;&lt;/a&gt;    &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arm_probabilities&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_2ec45af74d5448759c2c7f0ff10d2dc8-13"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;# arms = [BernoulliArm(probability) for probability in arm_probabilities]&lt;/span&gt;
&lt;a name="rest_code_2ec45af74d5448759c2c7f0ff10d2dc8-14"&gt;&lt;/a&gt;    &lt;span class="n"&gt;epsilons&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;a name="rest_code_2ec45af74d5448759c2c7f0ff10d2dc8-15"&gt;&lt;/a&gt;
&lt;a name="rest_code_2ec45af74d5448759c2c7f0ff10d2dc8-16"&gt;&lt;/a&gt;    &lt;span class="n"&gt;outcomes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
&lt;a name="rest_code_2ec45af74d5448759c2c7f0ff10d2dc8-17"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;epsilons&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_2ec45af74d5448759c2c7f0ff10d2dc8-18"&gt;&lt;/a&gt;        &lt;span class="n"&gt;agent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;EpsilonGreedyOptimized&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arm_probabilities&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_2ec45af74d5448759c2c7f0ff10d2dc8-19"&gt;&lt;/a&gt;        &lt;span class="n"&gt;average_rewards&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_2ec45af74d5448759c2c7f0ff10d2dc8-20"&gt;&lt;/a&gt;        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;trial&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trials&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_2ec45af74d5448759c2c7f0ff10d2dc8-21"&gt;&lt;/a&gt;            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_2ec45af74d5448759c2c7f0ff10d2dc8-22"&gt;&lt;/a&gt;                &lt;span class="n"&gt;arm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select_arm&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_2ec45af74d5448759c2c7f0ff10d2dc8-23"&gt;&lt;/a&gt;                &lt;span class="n"&gt;old_reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt;
&lt;a name="rest_code_2ec45af74d5448759c2c7f0ff10d2dc8-24"&gt;&lt;/a&gt;                &lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_2ec45af74d5448759c2c7f0ff10d2dc8-25"&gt;&lt;/a&gt;                &lt;span class="n"&gt;average_rewards&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;old_reward&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_2ec45af74d5448759c2c7f0ff10d2dc8-26"&gt;&lt;/a&gt;            &lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_2ec45af74d5448759c2c7f0ff10d2dc8-27"&gt;&lt;/a&gt;        &lt;span class="n"&gt;outcomes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Epsilon {0:.02f}"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;average_rewards&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;trials&lt;/span&gt;
&lt;a name="rest_code_2ec45af74d5448759c2c7f0ff10d2dc8-28"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;outcomes&lt;/span&gt;
&lt;/pre&gt;&lt;pre class="code ipython"&gt;&lt;a name="rest_code_68ba6aa4345445af8299627bb194084b-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;start&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_68ba6aa4345445af8299627bb194084b-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;averages&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;average_reward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TIMES&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;TRIALS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_68ba6aa4345445af8299627bb194084b-3"&gt;&lt;/a&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Run Time: {0}"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_68ba6aa4345445af8299627bb194084b-4"&gt;&lt;/a&gt;&lt;span class="n"&gt;averages&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;averages&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;pre class="literal-block"&gt;
Run Time: 0:01:08.727723
&lt;/pre&gt;
&lt;pre class="code ipython"&gt;&lt;a name="rest_code_dbc65296c3d647c1bd9745e8113d3ca5-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;figure&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_dbc65296c3d647c1bd9745e8113d3ca5-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;axe&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gca&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_dbc65296c3d647c1bd9745e8113d3ca5-3"&gt;&lt;/a&gt;&lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Reward of the Epsilon Greedy Algorithm ({} trials)"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TRIALS&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_dbc65296c3d647c1bd9745e8113d3ca5-4"&gt;&lt;/a&gt;&lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Time (number of pulls on the arm)"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_dbc65296c3d647c1bd9745e8113d3ca5-5"&gt;&lt;/a&gt;&lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Average Reward"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_dbc65296c3d647c1bd9745e8113d3ca5-6"&gt;&lt;/a&gt;&lt;span class="n"&gt;averages&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;marker&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'.'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;linestyle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"None"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;img alt="epsilon_averages.png" src="https://necromuralist.github.io/reinforcement_learning/posts/finding-the-best-epsilon/epsilon_averages.png"&gt;
&lt;p&gt;There's much more variablity and overlap here, as you might expect since I made the probabilities closer. Interestingly, the strongly exploratory agents seem to do worse, even from the beginning, while the more exploitative ones do better.  Although it looks like 0.2 might be doing as well or better than 0.1 once you get over 100.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="cumulative-reward"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="https://necromuralist.github.io/reinforcement_learning/posts/finding-the-best-epsilon/#id8"&gt;6 Cumulative Reward&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The previous two metrics turn out to be useful, but somewhat unfair to the aggresively exploring models, which we know won't ultimately do as well, but do have an advantage in the initial phase. To better qualify the overall effect of exploration versus exploitation, it's better to use a cumulative sum of the rewards.&lt;/p&gt;
&lt;pre class="code ipython"&gt;&lt;a name="rest_code_77d77817a1e24d048694565a4c224b41-1"&gt;&lt;/a&gt;&lt;span class="nd"&gt;@jit&lt;/span&gt;
&lt;a name="rest_code_77d77817a1e24d048694565a4c224b41-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;cumulative_reward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trials&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_77d77817a1e24d048694565a4c224b41-3"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""this generates the cumulative reward as the agent pulls the arms&lt;/span&gt;
&lt;a name="rest_code_77d77817a1e24d048694565a4c224b41-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_77d77817a1e24d048694565a4c224b41-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a name="rest_code_77d77817a1e24d048694565a4c224b41-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     arms (numpy.array): array of probabilities that the arm will pay-off&lt;/span&gt;
&lt;a name="rest_code_77d77817a1e24d048694565a4c224b41-7"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     trials (int): number of times to train the agent&lt;/span&gt;
&lt;a name="rest_code_77d77817a1e24d048694565a4c224b41-8"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     times (int): length of time to train the agent&lt;/span&gt;
&lt;a name="rest_code_77d77817a1e24d048694565a4c224b41-9"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;a name="rest_code_77d77817a1e24d048694565a4c224b41-10"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     Dict: the probabilites for each epsilon over time&lt;/span&gt;
&lt;a name="rest_code_77d77817a1e24d048694565a4c224b41-11"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_77d77817a1e24d048694565a4c224b41-12"&gt;&lt;/a&gt;    &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_77d77817a1e24d048694565a4c224b41-13"&gt;&lt;/a&gt;    &lt;span class="n"&gt;epsilons&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;a name="rest_code_77d77817a1e24d048694565a4c224b41-14"&gt;&lt;/a&gt;
&lt;a name="rest_code_77d77817a1e24d048694565a4c224b41-15"&gt;&lt;/a&gt;    &lt;span class="n"&gt;outcomes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
&lt;a name="rest_code_77d77817a1e24d048694565a4c224b41-16"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;epsilons&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_77d77817a1e24d048694565a4c224b41-17"&gt;&lt;/a&gt;        &lt;span class="n"&gt;agent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;EpsilonGreedyOptimized&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_77d77817a1e24d048694565a4c224b41-18"&gt;&lt;/a&gt;        &lt;span class="n"&gt;cumulative_rewards&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_77d77817a1e24d048694565a4c224b41-19"&gt;&lt;/a&gt;        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;trial&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trials&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_77d77817a1e24d048694565a4c224b41-20"&gt;&lt;/a&gt;            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_77d77817a1e24d048694565a4c224b41-21"&gt;&lt;/a&gt;                &lt;span class="n"&gt;arm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select_arm&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_77d77817a1e24d048694565a4c224b41-22"&gt;&lt;/a&gt;                &lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_77d77817a1e24d048694565a4c224b41-23"&gt;&lt;/a&gt;                &lt;span class="n"&gt;cumulative_rewards&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt;
&lt;a name="rest_code_77d77817a1e24d048694565a4c224b41-24"&gt;&lt;/a&gt;            &lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_77d77817a1e24d048694565a4c224b41-25"&gt;&lt;/a&gt;        &lt;span class="n"&gt;outcomes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Epsilon {:.02f}"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cumulative_rewards&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;trials&lt;/span&gt;
&lt;a name="rest_code_77d77817a1e24d048694565a4c224b41-26"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;outcomes&lt;/span&gt;
&lt;/pre&gt;&lt;pre class="code ipython"&gt;&lt;a name="rest_code_d08b5e62d2474ec19d47be0155ed3359-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;generate_cumulative&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_d08b5e62d2474ec19d47be0155ed3359-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""runs the cumulative output function&lt;/span&gt;
&lt;a name="rest_code_d08b5e62d2474ec19d47be0155ed3359-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_d08b5e62d2474ec19d47be0155ed3359-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a name="rest_code_d08b5e62d2474ec19d47be0155ed3359-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     arms (numpy.array): probabilities that arms will pay out&lt;/span&gt;
&lt;a name="rest_code_d08b5e62d2474ec19d47be0155ed3359-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_d08b5e62d2474ec19d47be0155ed3359-7"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;a name="rest_code_d08b5e62d2474ec19d47be0155ed3359-8"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     pandas.DataFrame: the average cumulative rewards&lt;/span&gt;
&lt;a name="rest_code_d08b5e62d2474ec19d47be0155ed3359-9"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_d08b5e62d2474ec19d47be0155ed3359-10"&gt;&lt;/a&gt;    &lt;span class="n"&gt;start&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_d08b5e62d2474ec19d47be0155ed3359-11"&gt;&lt;/a&gt;    &lt;span class="n"&gt;cumulative&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cumulative_reward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;times&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TIMES&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trials&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TRIALS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_d08b5e62d2474ec19d47be0155ed3359-12"&gt;&lt;/a&gt;    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Run Time: {0}"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_d08b5e62d2474ec19d47be0155ed3359-13"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cumulative&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;pre class="code ipython"&gt;&lt;a name="rest_code_cf79435bf2804356b8f31b413b8eff41-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;plot_cumulative&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cumulative&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_cf79435bf2804356b8f31b413b8eff41-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""generates and plots cumulative average&lt;/span&gt;
&lt;a name="rest_code_cf79435bf2804356b8f31b413b8eff41-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_cf79435bf2804356b8f31b413b8eff41-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a name="rest_code_cf79435bf2804356b8f31b413b8eff41-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     cumulative (pandas.DataFrame): data to plot&lt;/span&gt;
&lt;a name="rest_code_cf79435bf2804356b8f31b413b8eff41-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_cf79435bf2804356b8f31b413b8eff41-7"&gt;&lt;/a&gt;    &lt;span class="n"&gt;figure&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_cf79435bf2804356b8f31b413b8eff41-8"&gt;&lt;/a&gt;    &lt;span class="n"&gt;axe&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gca&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_cf79435bf2804356b8f31b413b8eff41-9"&gt;&lt;/a&gt;    &lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Cumulative Reward of the Epsilon Greedy Algorithm ({} trials)"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TRIALS&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_cf79435bf2804356b8f31b413b8eff41-10"&gt;&lt;/a&gt;    &lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Time (number of pulls on the arm)"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_cf79435bf2804356b8f31b413b8eff41-11"&gt;&lt;/a&gt;    &lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Cumulative Reward"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_cf79435bf2804356b8f31b413b8eff41-12"&gt;&lt;/a&gt;    &lt;span class="n"&gt;cumulative&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_cf79435bf2804356b8f31b413b8eff41-13"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;pre class="code ipython"&gt;&lt;a name="rest_code_e05f0740d0054c6b9b535f386357f840-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;cumulative&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;generate_cumulative&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_e05f0740d0054c6b9b535f386357f840-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;plot_cumulative&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cumulative&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;img alt="epsilon_greedy_cumulative.png" src="https://necromuralist.github.io/reinforcement_learning/posts/finding-the-best-epsilon/epsilon_greedy_cumulative.png"&gt;
&lt;p&gt;Because of the randomness this will change everytime you run it, but we can see that in this case, the average cumulative reward was better for the 0.3 and 0.5 epsilon values than the more conservative values up until around 275, and the second most conservative case (0.2) actually did worse on average than the more exploratory cases did.&lt;/p&gt;
&lt;div class="section" id="imbalanced-case"&gt;
&lt;h3&gt;&lt;a class="toc-backref" href="https://necromuralist.github.io/reinforcement_learning/posts/finding-the-best-epsilon/#id9"&gt;6.1 Imbalanced Case&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;I'll re-run this again with more arms and a only one clear good arm to see if this changes things.&lt;/p&gt;
&lt;pre class="code ipython"&gt;&lt;a name="rest_code_a2f72a36ec724a60b22ac0a9554e9b13-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;plot_cumulative&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;generate_cumulative&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;])))&lt;/span&gt;
&lt;/pre&gt;&lt;img alt="epsilon_cumulative_2.png" src="https://necromuralist.github.io/reinforcement_learning/posts/finding-the-best-epsilon/epsilon_cumulative_2.png"&gt;
&lt;p&gt;In this case, the most exploitive agent did much worse than the other agents. It looks like it didn't find the best arm until around the 240th pull. In this case, when most arms pay off poorly and one arm pays off much better, the exploratory arms accumulate more reward within our time frame. I'm guessing that the 0.10 epsilon would, given enough time, pull ahead, and you can in fact see that the most exploratory agent has already been surpassed by the 0.2 agent, so eventually exploration would probably take a back seat to exploitation, but not in this case. It's important to note, however, that if the most exploitive agent had happened to find the best arm at the start, he would likely have ended up the best, it's just the nature of randomization that you aren't guaranteed that this would be the case.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>algorithm</category><guid>https://necromuralist.github.io/reinforcement_learning/posts/finding-the-best-epsilon/</guid><pubDate>Mon, 31 Jul 2017 01:41:00 GMT</pubDate></item><item><title>A Bernoulli Arm</title><link>https://necromuralist.github.io/reinforcement_learning/posts/A-Bernoulli-Arm/</link><dc:creator>hades</dc:creator><description>&lt;div&gt;&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;1 Introduction&lt;/h2&gt;
&lt;p&gt;This is an implementation of one arm of a &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Multi-armed_bandit"&gt;n-armed bandit&lt;/a&gt; to test the Epsilon Greedy algorithm. It takes a probability that it will return a reward. It also optionally let's you set the penalty and reward values, but defaults to a reward of 1 and a penalty of 0 (so it's really no reward more than a penalty).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="imports"&gt;
&lt;h2&gt;2 Imports&lt;/h2&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_4afa4929c961478aa96fe9044e29aa64-1"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# python standard library&lt;/span&gt;
&lt;a name="rest_code_4afa4929c961478aa96fe9044e29aa64-2"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="bernoulli-arm"&gt;
&lt;h2&gt;3 Bernoulli Arm&lt;/h2&gt;
&lt;p&gt;The Bernoulli Arm will generate a value when its arm is pulled at a payout rate specified by the `probability` value.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_7d2cbd299aab47909ceec8b5336bb20f-1"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;imports&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_7d2cbd299aab47909ceec8b5336bb20f-2"&gt;&lt;/a&gt;
&lt;a name="rest_code_7d2cbd299aab47909ceec8b5336bb20f-3"&gt;&lt;/a&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;BernoulliArm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_7d2cbd299aab47909ceec8b5336bb20f-4"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""A simulation of one arm of a multi-armed bandit&lt;/span&gt;
&lt;a name="rest_code_7d2cbd299aab47909ceec8b5336bb20f-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_7d2cbd299aab47909ceec8b5336bb20f-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a name="rest_code_7d2cbd299aab47909ceec8b5336bb20f-7"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     probability (float): probability of a reward&lt;/span&gt;
&lt;a name="rest_code_7d2cbd299aab47909ceec8b5336bb20f-8"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     reward (float): value to return on a win&lt;/span&gt;
&lt;a name="rest_code_7d2cbd299aab47909ceec8b5336bb20f-9"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     penalty (float): value to return on a loss&lt;/span&gt;
&lt;a name="rest_code_7d2cbd299aab47909ceec8b5336bb20f-10"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_7d2cbd299aab47909ceec8b5336bb20f-11"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;constructor&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_7d2cbd299aab47909ceec8b5336bb20f-12"&gt;&lt;/a&gt;
&lt;a name="rest_code_7d2cbd299aab47909ceec8b5336bb20f-13"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;call&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;div class="section" id="constructor"&gt;
&lt;h3&gt;3.1 Constructor&lt;/h3&gt;
&lt;p&gt;The constructor takes three values:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;probability of winning&lt;/li&gt;
&lt;li&gt;reward on winning&lt;/li&gt;
&lt;li&gt;penalty on losing&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Because of the way the problem is set up, the reward and penalty are already set at 1 and 0, but I didn't want there to be magic numbers so they can be changed if needed.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_b00f751f28aa45aeba6b40027979d1fe-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;probability&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;penalty&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_b00f751f28aa45aeba6b40027979d1fe-2"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;probability&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;probability&lt;/span&gt;
&lt;a name="rest_code_b00f751f28aa45aeba6b40027979d1fe-3"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;
&lt;a name="rest_code_b00f751f28aa45aeba6b40027979d1fe-4"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;penalty&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;penalty&lt;/span&gt;
&lt;a name="rest_code_b00f751f28aa45aeba6b40027979d1fe-5"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="the-call"&gt;
&lt;h3&gt;3.2 The Call&lt;/h3&gt;
&lt;p&gt;This is called &lt;tt class="docutils literal"&gt;pull&lt;/tt&gt; in most cases, but I thought it would be more uniform to put it in a call.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_c3c69341a376401fa843c7f55b194f65-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__call__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_c3c69341a376401fa843c7f55b194f65-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""pulls the arm and returns a reward or penalty&lt;/span&gt;
&lt;a name="rest_code_c3c69341a376401fa843c7f55b194f65-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_c3c69341a376401fa843c7f55b194f65-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;a name="rest_code_c3c69341a376401fa843c7f55b194f65-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     float: value returned on pulling the arm&lt;/span&gt;
&lt;a name="rest_code_c3c69341a376401fa843c7f55b194f65-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_c3c69341a376401fa843c7f55b194f65-7"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;probability&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_c3c69341a376401fa843c7f55b194f65-8"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;penalty&lt;/span&gt;
&lt;a name="rest_code_c3c69341a376401fa843c7f55b194f65-9"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reward&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>algorithm</category><guid>https://necromuralist.github.io/reinforcement_learning/posts/A-Bernoulli-Arm/</guid><pubDate>Mon, 31 Jul 2017 01:37:00 GMT</pubDate></item><item><title>The Epsilon Greedy Algorithm</title><link>https://necromuralist.github.io/reinforcement_learning/posts/The-Epsilon-Greedy-Algorithm/</link><dc:creator>hades</dc:creator><description>&lt;div&gt;&lt;div class="section" id="background"&gt;
&lt;h2&gt;1 Background&lt;/h2&gt;
&lt;p&gt;This is an implementation of the Epsilon Greedy algorithm to find solutions for the multi-arm-bandit problem.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="imports"&gt;
&lt;h2&gt;2 Imports&lt;/h2&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_23ac67c6dfdb4854b223d56d80d9d5ac-1"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# python&lt;/span&gt;
&lt;a name="rest_code_23ac67c6dfdb4854b223d56d80d9d5ac-2"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt;
&lt;a name="rest_code_23ac67c6dfdb4854b223d56d80d9d5ac-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_23ac67c6dfdb4854b223d56d80d9d5ac-4"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# pypi&lt;/span&gt;
&lt;a name="rest_code_23ac67c6dfdb4854b223d56d80d9d5ac-5"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numba&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;jit&lt;/span&gt;
&lt;a name="rest_code_23ac67c6dfdb4854b223d56d80d9d5ac-6"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="find-first"&gt;
&lt;h2&gt;3 Find First&lt;/h2&gt;
&lt;p&gt;This is a helper function to find the first matching item in an array-like collection.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_b617331fb934406da31b63f681d87441-1"&gt;&lt;/a&gt;&lt;span class="nd"&gt;@jit&lt;/span&gt;
&lt;a name="rest_code_b617331fb934406da31b63f681d87441-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;find_first&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vector&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_b617331fb934406da31b63f681d87441-3"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""find the first item in the vector&lt;/span&gt;
&lt;a name="rest_code_b617331fb934406da31b63f681d87441-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_b617331fb934406da31b63f681d87441-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a name="rest_code_b617331fb934406da31b63f681d87441-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     item: thing to match&lt;/span&gt;
&lt;a name="rest_code_b617331fb934406da31b63f681d87441-7"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     vector: thing to search&lt;/span&gt;
&lt;a name="rest_code_b617331fb934406da31b63f681d87441-8"&gt;&lt;/a&gt;
&lt;a name="rest_code_b617331fb934406da31b63f681d87441-9"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;a name="rest_code_b617331fb934406da31b63f681d87441-10"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     value: index of first matching item, -1 if not found&lt;/span&gt;
&lt;a name="rest_code_b617331fb934406da31b63f681d87441-11"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_b617331fb934406da31b63f681d87441-12"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vector&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
&lt;a name="rest_code_b617331fb934406da31b63f681d87441-13"&gt;&lt;/a&gt;        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;vector&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
&lt;a name="rest_code_b617331fb934406da31b63f681d87441-14"&gt;&lt;/a&gt;            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;
&lt;a name="rest_code_b617331fb934406da31b63f681d87441-15"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="epsilon-greedy"&gt;
&lt;h2&gt;4 Epsilon Greedy&lt;/h2&gt;
&lt;p&gt;The &lt;em&gt;epsilon-greedy&lt;/em&gt; algorithm tries to solve the exploitation-exploration dilemna by exploring a fraction of the time (set by &lt;em&gt;epsilon&lt;/em&gt;) and using the best solution found so far the rest of the time. This implementation is based on the one in Bandit Algorithms for Website Optimization &lt;a class="footnote-reference" href="https://necromuralist.github.io/reinforcement_learning/posts/The-Epsilon-Greedy-Algorithm/#id5" id="id1"&gt;[1]&lt;/a&gt; .&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_ee0a9045898c4e999014a90dcea50649-1"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;imports&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_ee0a9045898c4e999014a90dcea50649-2"&gt;&lt;/a&gt;
&lt;a name="rest_code_ee0a9045898c4e999014a90dcea50649-3"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_ee0a9045898c4e999014a90dcea50649-4"&gt;&lt;/a&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;EpsilonGreedy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_ee0a9045898c4e999014a90dcea50649-5"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""The Epsilon Greedy Algorithm&lt;/span&gt;
&lt;a name="rest_code_ee0a9045898c4e999014a90dcea50649-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_ee0a9045898c4e999014a90dcea50649-7"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a name="rest_code_ee0a9045898c4e999014a90dcea50649-8"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     epsilon (float): fraction of the time to explore&lt;/span&gt;
&lt;a name="rest_code_ee0a9045898c4e999014a90dcea50649-9"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     arms (list): collection of bandits to pull&lt;/span&gt;
&lt;a name="rest_code_ee0a9045898c4e999014a90dcea50649-10"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_ee0a9045898c4e999014a90dcea50649-11"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;constructor&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_ee0a9045898c4e999014a90dcea50649-12"&gt;&lt;/a&gt;
&lt;a name="rest_code_ee0a9045898c4e999014a90dcea50649-13"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;best&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_ee0a9045898c4e999014a90dcea50649-14"&gt;&lt;/a&gt;
&lt;a name="rest_code_ee0a9045898c4e999014a90dcea50649-15"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_ee0a9045898c4e999014a90dcea50649-16"&gt;&lt;/a&gt;
&lt;a name="rest_code_ee0a9045898c4e999014a90dcea50649-17"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_ee0a9045898c4e999014a90dcea50649-18"&gt;&lt;/a&gt;
&lt;a name="rest_code_ee0a9045898c4e999014a90dcea50649-19"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;select&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_ee0a9045898c4e999014a90dcea50649-20"&gt;&lt;/a&gt;
&lt;a name="rest_code_ee0a9045898c4e999014a90dcea50649-21"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_ee0a9045898c4e999014a90dcea50649-22"&gt;&lt;/a&gt;
&lt;a name="rest_code_ee0a9045898c4e999014a90dcea50649-23"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_ee0a9045898c4e999014a90dcea50649-24"&gt;&lt;/a&gt;
&lt;a name="rest_code_ee0a9045898c4e999014a90dcea50649-25"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;call&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;div class="section" id="the-constructor"&gt;
&lt;h3&gt;4.1 The Constructor&lt;/h3&gt;
&lt;p&gt;The constructor takes two arguments - &lt;em&gt;epsilon&lt;/em&gt; and &lt;em&gt;arms&lt;/em&gt;. The &lt;em&gt;arms&lt;/em&gt; list should contain bandits that return a reward or penalty when pulled (called).&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_fbd9f908667d42dc8c75999805b8eb38-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_fbd9f908667d42dc8c75999805b8eb38-2"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;
&lt;a name="rest_code_fbd9f908667d42dc8c75999805b8eb38-3"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;arms&lt;/span&gt;
&lt;a name="rest_code_fbd9f908667d42dc8c75999805b8eb38-4"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
&lt;a name="rest_code_fbd9f908667d42dc8c75999805b8eb38-5"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_rewards&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
&lt;a name="rest_code_fbd9f908667d42dc8c75999805b8eb38-6"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
&lt;a name="rest_code_fbd9f908667d42dc8c75999805b8eb38-7"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="best-arm"&gt;
&lt;h3&gt;4.2 Best Arm&lt;/h3&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;best_arm&lt;/tt&gt; property returns the index of the arm that has the highest average reward so far. It returns the index instead of the arm itself because it's used to get the matching counts and rewards in the &lt;tt class="docutils literal"&gt;update&lt;/tt&gt; method.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_458ed2d7ec8547529703d5c5aa7e719f-1"&gt;&lt;/a&gt;&lt;span class="nd"&gt;@property&lt;/span&gt;
&lt;a name="rest_code_458ed2d7ec8547529703d5c5aa7e719f-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;best_arm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_458ed2d7ec8547529703d5c5aa7e719f-3"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""Index of the arm with the most reward"""&lt;/span&gt;
&lt;a name="rest_code_458ed2d7ec8547529703d5c5aa7e719f-4"&gt;&lt;/a&gt;    &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_458ed2d7ec8547529703d5c5aa7e719f-5"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;find_first&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="counts"&gt;
&lt;h3&gt;4.3 Counts&lt;/h3&gt;
&lt;p&gt;The `counts` keeps track of the number of times each arm is pulled.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_a5b9d10334664527a10b1cb29c80b28b-1"&gt;&lt;/a&gt;&lt;span class="nd"&gt;@property&lt;/span&gt;
&lt;a name="rest_code_a5b9d10334664527a10b1cb29c80b28b-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_a5b9d10334664527a10b1cb29c80b28b-3"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""counts of times each arm is pulled&lt;/span&gt;
&lt;a name="rest_code_a5b9d10334664527a10b1cb29c80b28b-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_a5b9d10334664527a10b1cb29c80b28b-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;a name="rest_code_a5b9d10334664527a10b1cb29c80b28b-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     numpy.array: array of counts&lt;/span&gt;
&lt;a name="rest_code_a5b9d10334664527a10b1cb29c80b28b-7"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_a5b9d10334664527a10b1cb29c80b28b-8"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_counts&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_a5b9d10334664527a10b1cb29c80b28b-9"&gt;&lt;/a&gt;        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_a5b9d10334664527a10b1cb29c80b28b-10"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_counts&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="rewards"&gt;
&lt;h3&gt;4.4 Rewards&lt;/h3&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;rewards&lt;/tt&gt; attributes holds the running average reward that each arm has returned.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_a63c0d0e666e490caec6127b66507495-1"&gt;&lt;/a&gt;&lt;span class="nd"&gt;@property&lt;/span&gt;
&lt;a name="rest_code_a63c0d0e666e490caec6127b66507495-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_a63c0d0e666e490caec6127b66507495-3"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""array of running average of rewards for each arms&lt;/span&gt;
&lt;a name="rest_code_a63c0d0e666e490caec6127b66507495-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_a63c0d0e666e490caec6127b66507495-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;a name="rest_code_a63c0d0e666e490caec6127b66507495-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     numpy.array: running averages&lt;/span&gt;
&lt;a name="rest_code_a63c0d0e666e490caec6127b66507495-7"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_a63c0d0e666e490caec6127b66507495-8"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_rewards&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_a63c0d0e666e490caec6127b66507495-9"&gt;&lt;/a&gt;        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_rewards&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_a63c0d0e666e490caec6127b66507495-10"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_rewards&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="reset"&gt;
&lt;h3&gt;4.5 Reset&lt;/h3&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_5a8af36ae7fb4496b99b5fe524616be3-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_5a8af36ae7fb4496b99b5fe524616be3-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""sets the counts and rewards to None&lt;/span&gt;
&lt;a name="rest_code_5a8af36ae7fb4496b99b5fe524616be3-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_5a8af36ae7fb4496b99b5fe524616be3-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    This lets you re-used the EpsilonGreedy without re-constructing&lt;/span&gt;
&lt;a name="rest_code_5a8af36ae7fb4496b99b5fe524616be3-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    the arms&lt;/span&gt;
&lt;a name="rest_code_5a8af36ae7fb4496b99b5fe524616be3-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_5a8af36ae7fb4496b99b5fe524616be3-7"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
&lt;a name="rest_code_5a8af36ae7fb4496b99b5fe524616be3-8"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_rewards&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
&lt;a name="rest_code_5a8af36ae7fb4496b99b5fe524616be3-9"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;a name="rest_code_5a8af36ae7fb4496b99b5fe524616be3-10"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="select-arm"&gt;
&lt;h3&gt;4.6 Select Arm&lt;/h3&gt;
&lt;p&gt;The &lt;em&gt;select_arm&lt;/em&gt; method will choose either the best arm or a random one based on a randomly drawn value and how it compares to epsilon.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_2d724ae6b1ea4cf1bb9af459dfce8620-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;select_arm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_2d724ae6b1ea4cf1bb9af459dfce8620-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""chooses the next arm to update&lt;/span&gt;
&lt;a name="rest_code_2d724ae6b1ea4cf1bb9af459dfce8620-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_2d724ae6b1ea4cf1bb9af459dfce8620-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;a name="rest_code_2d724ae6b1ea4cf1bb9af459dfce8620-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     int: index of the next arm to pull&lt;/span&gt;
&lt;a name="rest_code_2d724ae6b1ea4cf1bb9af459dfce8620-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_2d724ae6b1ea4cf1bb9af459dfce8620-7"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;a name="rest_code_2d724ae6b1ea4cf1bb9af459dfce8620-8"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randrange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_2d724ae6b1ea4cf1bb9af459dfce8620-9"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_arm&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="update"&gt;
&lt;h3&gt;4.7 Update&lt;/h3&gt;
&lt;p&gt;The update method pulls the arm whose index it is given and then updates the count and reward.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_439f7a7e4f8e495d8cdfd6c2cfe06e16-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_439f7a7e4f8e495d8cdfd6c2cfe06e16-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""pulls the arm and updates the value&lt;/span&gt;
&lt;a name="rest_code_439f7a7e4f8e495d8cdfd6c2cfe06e16-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_439f7a7e4f8e495d8cdfd6c2cfe06e16-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a name="rest_code_439f7a7e4f8e495d8cdfd6c2cfe06e16-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     arm (int): index of the arm to pull&lt;/span&gt;
&lt;a name="rest_code_439f7a7e4f8e495d8cdfd6c2cfe06e16-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_439f7a7e4f8e495d8cdfd6c2cfe06e16-7"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;a name="rest_code_439f7a7e4f8e495d8cdfd6c2cfe06e16-8"&gt;&lt;/a&gt;    &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_439f7a7e4f8e495d8cdfd6c2cfe06e16-9"&gt;&lt;/a&gt;    &lt;span class="n"&gt;average_reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_439f7a7e4f8e495d8cdfd6c2cfe06e16-10"&gt;&lt;/a&gt;    &lt;span class="n"&gt;reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]()&lt;/span&gt;
&lt;a name="rest_code_439f7a7e4f8e495d8cdfd6c2cfe06e16-11"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;
&lt;a name="rest_code_439f7a7e4f8e495d8cdfd6c2cfe06e16-12"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(((&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;average_reward&lt;/span&gt;
&lt;a name="rest_code_439f7a7e4f8e495d8cdfd6c2cfe06e16-13"&gt;&lt;/a&gt;                        &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;a name="rest_code_439f7a7e4f8e495d8cdfd6c2cfe06e16-14"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="call"&gt;
&lt;h3&gt;4.8 Call&lt;/h3&gt;
&lt;p&gt;The &lt;em&gt;__call__&lt;/em&gt; method will be the main update method that unifies the naming conventions found in the books.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_b0b6cad0b781434283b5728c379a863f-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__call__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_b0b6cad0b781434283b5728c379a863f-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""chooses an arm and updates the rewards"""&lt;/span&gt;
&lt;a name="rest_code_b0b6cad0b781434283b5728c379a863f-3"&gt;&lt;/a&gt;    &lt;span class="n"&gt;arm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select_arm&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_b0b6cad0b781434283b5728c379a863f-4"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_b0b6cad0b781434283b5728c379a863f-5"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="epsilon-greedy-optimized"&gt;
&lt;h2&gt;5 Epsilon Greedy Optimized&lt;/h2&gt;
&lt;p&gt;It turns out that while the implementation above works correctly, it can be rather slow, given that we need to train it thousands of times to get meaningful results. This is a numba-compatible version that drops the testing time from around 11 minutes to a minute or less. One of the restrictions of using classes in numba is that you have to declare the types of all the attributes of the class (this happens in the &lt;cite&gt;spec&lt;/cite&gt; passed to the &lt;cite&gt;jitclass&lt;/cite&gt; decorator). This means that I can't pass in &lt;cite&gt;BernoulliArm&lt;/cite&gt; objects to the constructor, because &lt;cite&gt;numba&lt;/cite&gt; has no idea what they are, so this solution is a hybrid greedy algorithm and bandit arm mashed together.&lt;/p&gt;
&lt;p&gt;The documentation for &lt;cite&gt;numba&lt;/cite&gt; states that you have to initialize the attributes in the &lt;cite&gt;__init__&lt;/cite&gt; method so I'm getting rid of the properties that build the numpy arrays and moving their creation to the constructor. In addition, the code that no longer expects the =BernoulliArm= objects will have to be re-implemented. In the tangle code anything with the &lt;cite&gt;optimized-&lt;/cite&gt; prefix is re-implemented (other than the &lt;cite&gt;spec&lt;/cite&gt;), otherwise the code is being pulled in from the original &lt;cite&gt;EpsilonGreedy&lt;/cite&gt; implementation.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_d6e222b28b39496abedc2ff3f8a2035c-1"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;optimized&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;imports&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_d6e222b28b39496abedc2ff3f8a2035c-2"&gt;&lt;/a&gt;
&lt;a name="rest_code_d6e222b28b39496abedc2ff3f8a2035c-3"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;spec&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_d6e222b28b39496abedc2ff3f8a2035c-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_d6e222b28b39496abedc2ff3f8a2035c-5"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_d6e222b28b39496abedc2ff3f8a2035c-6"&gt;&lt;/a&gt;&lt;span class="nd"&gt;@jitclass&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;spec&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_d6e222b28b39496abedc2ff3f8a2035c-7"&gt;&lt;/a&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;EpsilonGreedyOptimized&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_d6e222b28b39496abedc2ff3f8a2035c-8"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""The Epsilon Greedy Algorithm&lt;/span&gt;
&lt;a name="rest_code_d6e222b28b39496abedc2ff3f8a2035c-9"&gt;&lt;/a&gt;
&lt;a name="rest_code_d6e222b28b39496abedc2ff3f8a2035c-10"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a name="rest_code_d6e222b28b39496abedc2ff3f8a2035c-11"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     epsilon (float): fraction of the time to explore&lt;/span&gt;
&lt;a name="rest_code_d6e222b28b39496abedc2ff3f8a2035c-12"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     arms (list): collection of probabilities for bandit arm&lt;/span&gt;
&lt;a name="rest_code_d6e222b28b39496abedc2ff3f8a2035c-13"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_d6e222b28b39496abedc2ff3f8a2035c-14"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;optimized&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;constructor&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_d6e222b28b39496abedc2ff3f8a2035c-15"&gt;&lt;/a&gt;
&lt;a name="rest_code_d6e222b28b39496abedc2ff3f8a2035c-16"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;best&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_d6e222b28b39496abedc2ff3f8a2035c-17"&gt;&lt;/a&gt;
&lt;a name="rest_code_d6e222b28b39496abedc2ff3f8a2035c-18"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;select&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_d6e222b28b39496abedc2ff3f8a2035c-19"&gt;&lt;/a&gt;
&lt;a name="rest_code_d6e222b28b39496abedc2ff3f8a2035c-20"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;optimized&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;pull&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_d6e222b28b39496abedc2ff3f8a2035c-21"&gt;&lt;/a&gt;
&lt;a name="rest_code_d6e222b28b39496abedc2ff3f8a2035c-22"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;optimized&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_d6e222b28b39496abedc2ff3f8a2035c-23"&gt;&lt;/a&gt;
&lt;a name="rest_code_d6e222b28b39496abedc2ff3f8a2035c-24"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;optimized&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_d6e222b28b39496abedc2ff3f8a2035c-25"&gt;&lt;/a&gt;
&lt;a name="rest_code_d6e222b28b39496abedc2ff3f8a2035c-26"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;call&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;div class="section" id="optimized-imports"&gt;
&lt;h3&gt;5.1 Optimized Imports&lt;/h3&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_635a8bb3b1de46f2853f27c04771ca55-1"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# python&lt;/span&gt;
&lt;a name="rest_code_635a8bb3b1de46f2853f27c04771ca55-2"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt;
&lt;a name="rest_code_635a8bb3b1de46f2853f27c04771ca55-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_635a8bb3b1de46f2853f27c04771ca55-4"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# pypi&lt;/span&gt;
&lt;a name="rest_code_635a8bb3b1de46f2853f27c04771ca55-5"&gt;&lt;/a&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numba&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
&lt;a name="rest_code_635a8bb3b1de46f2853f27c04771ca55-6"&gt;&lt;/a&gt;    &lt;span class="n"&gt;jit&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_635a8bb3b1de46f2853f27c04771ca55-7"&gt;&lt;/a&gt;    &lt;span class="n"&gt;jitclass&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_635a8bb3b1de46f2853f27c04771ca55-8"&gt;&lt;/a&gt;    &lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_635a8bb3b1de46f2853f27c04771ca55-9"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numba&lt;/span&gt;
&lt;a name="rest_code_635a8bb3b1de46f2853f27c04771ca55-10"&gt;&lt;/a&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="the-spec"&gt;
&lt;h3&gt;5.2 The Spec&lt;/h3&gt;
&lt;p&gt;This is how you tell numba what attributes the class will have. This is where most of the errors were when I first tried this. The error-messages aren't particularly helpful. Just be aware that this is the first place you should look if things crash.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_2222291ed1c34e9e9851c1b8296ca79b-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;spec&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
&lt;a name="rest_code_2222291ed1c34e9e9851c1b8296ca79b-2"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"epsilon"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numba&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;a name="rest_code_2222291ed1c34e9e9851c1b8296ca79b-3"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"arms"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numba&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double&lt;/span&gt;&lt;span class="p"&gt;[:]),&lt;/span&gt;
&lt;a name="rest_code_2222291ed1c34e9e9851c1b8296ca79b-4"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"counts"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numba&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double&lt;/span&gt;&lt;span class="p"&gt;[:]),&lt;/span&gt;
&lt;a name="rest_code_2222291ed1c34e9e9851c1b8296ca79b-5"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"rewards"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numba&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double&lt;/span&gt;&lt;span class="p"&gt;[:]),&lt;/span&gt;
&lt;a name="rest_code_2222291ed1c34e9e9851c1b8296ca79b-6"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"total_reward"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numba&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int64&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;a name="rest_code_2222291ed1c34e9e9851c1b8296ca79b-7"&gt;&lt;/a&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="id2"&gt;
&lt;h3&gt;5.3 The Constructor&lt;/h3&gt;
&lt;p&gt;The constructor takes two arguments - &lt;em&gt;epsilon&lt;/em&gt; and &lt;em&gt;arms&lt;/em&gt;. The &lt;em&gt;arms&lt;/em&gt; list should contain probabilities that a reward or penalty will be returned when pulled.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_3480462edc804ebbac168c996942ee28-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_3480462edc804ebbac168c996942ee28-2"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;
&lt;a name="rest_code_3480462edc804ebbac168c996942ee28-3"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;arms&lt;/span&gt;
&lt;a name="rest_code_3480462edc804ebbac168c996942ee28-4"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_3480462edc804ebbac168c996942ee28-5"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_3480462edc804ebbac168c996942ee28-6"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;a name="rest_code_3480462edc804ebbac168c996942ee28-7"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="id3"&gt;
&lt;h3&gt;5.4 Reset&lt;/h3&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_c59ad6082ae04030a5b4627bf2b4179e-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_c59ad6082ae04030a5b4627bf2b4179e-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""sets the counts, rewards, total_reward to 0s&lt;/span&gt;
&lt;a name="rest_code_c59ad6082ae04030a5b4627bf2b4179e-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_c59ad6082ae04030a5b4627bf2b4179e-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    This lets you re-used the EpsilonGreedy&lt;/span&gt;
&lt;a name="rest_code_c59ad6082ae04030a5b4627bf2b4179e-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_c59ad6082ae04030a5b4627bf2b4179e-6"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_c59ad6082ae04030a5b4627bf2b4179e-7"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_c59ad6082ae04030a5b4627bf2b4179e-8"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;a name="rest_code_c59ad6082ae04030a5b4627bf2b4179e-9"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="pull-arm"&gt;
&lt;h3&gt;5.5 Pull Arm&lt;/h3&gt;
&lt;p&gt;Since we can't give user-defined objects as attributes of the class, this version will be both algorithm and bandit.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_eb2d902dbb5e4daebc8b4bc2086f2489-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;pull_arm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_eb2d902dbb5e4daebc8b4bc2086f2489-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""gets the reward&lt;/span&gt;
&lt;a name="rest_code_eb2d902dbb5e4daebc8b4bc2086f2489-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_eb2d902dbb5e4daebc8b4bc2086f2489-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a name="rest_code_eb2d902dbb5e4daebc8b4bc2086f2489-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     arm (int): index for the arm-probability array&lt;/span&gt;
&lt;a name="rest_code_eb2d902dbb5e4daebc8b4bc2086f2489-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;a name="rest_code_eb2d902dbb5e4daebc8b4bc2086f2489-7"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     int: reward or no reward&lt;/span&gt;
&lt;a name="rest_code_eb2d902dbb5e4daebc8b4bc2086f2489-8"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_eb2d902dbb5e4daebc8b4bc2086f2489-9"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arms&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
&lt;a name="rest_code_eb2d902dbb5e4daebc8b4bc2086f2489-10"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;a name="rest_code_eb2d902dbb5e4daebc8b4bc2086f2489-11"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="id4"&gt;
&lt;h3&gt;5.6 Update&lt;/h3&gt;
&lt;p&gt;The update method pulls the arm whose index it is given and then updates the count and reward. Here we're calling the &lt;tt class="docutils literal"&gt;pull_arm&lt;/tt&gt; method instead of using a &lt;tt class="docutils literal"&gt;BernoulliArm&lt;/tt&gt; so we can't re-use the original method.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_c6ab5cff46c44626a5fe274ef28fd0b2-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;a name="rest_code_c6ab5cff46c44626a5fe274ef28fd0b2-2"&gt;&lt;/a&gt;    &lt;span class="sd"&gt;"""pulls the arm and updates the value&lt;/span&gt;
&lt;a name="rest_code_c6ab5cff46c44626a5fe274ef28fd0b2-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_c6ab5cff46c44626a5fe274ef28fd0b2-4"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;a name="rest_code_c6ab5cff46c44626a5fe274ef28fd0b2-5"&gt;&lt;/a&gt;&lt;span class="sd"&gt;     arm (int): index of the arm to pull&lt;/span&gt;
&lt;a name="rest_code_c6ab5cff46c44626a5fe274ef28fd0b2-6"&gt;&lt;/a&gt;&lt;span class="sd"&gt;    """&lt;/span&gt;
&lt;a name="rest_code_c6ab5cff46c44626a5fe274ef28fd0b2-7"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;a name="rest_code_c6ab5cff46c44626a5fe274ef28fd0b2-8"&gt;&lt;/a&gt;    &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_c6ab5cff46c44626a5fe274ef28fd0b2-9"&gt;&lt;/a&gt;    &lt;span class="n"&gt;average_reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_c6ab5cff46c44626a5fe274ef28fd0b2-10"&gt;&lt;/a&gt;    &lt;span class="n"&gt;reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pull_arm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_c6ab5cff46c44626a5fe274ef28fd0b2-11"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;total_reward&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;
&lt;a name="rest_code_c6ab5cff46c44626a5fe274ef28fd0b2-12"&gt;&lt;/a&gt;    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arm&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(((&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;average_reward&lt;/span&gt;
&lt;a name="rest_code_c6ab5cff46c44626a5fe274ef28fd0b2-13"&gt;&lt;/a&gt;                        &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;a name="rest_code_c6ab5cff46c44626a5fe274ef28fd0b2-14"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="references"&gt;
&lt;h2&gt;6 References&lt;/h2&gt;
&lt;table class="docutils footnote" frame="void" id="id5" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label"&gt;&lt;col&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="https://necromuralist.github.io/reinforcement_learning/posts/The-Epsilon-Greedy-Algorithm/#id1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Bandit Algorithms for Website Optimization by John Myles White. Copyright 2013 John Myles White, 978-1-449-34133-6&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>algorithm reinforcementlearning</category><guid>https://necromuralist.github.io/reinforcement_learning/posts/The-Epsilon-Greedy-Algorithm/</guid><pubDate>Mon, 31 Jul 2017 01:22:00 GMT</pubDate></item></channel></rss>